{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["uyNbGpi1hS5H","gdEGTRICkido"],"toc_visible":true,"authorship_tag":"ABX9TyNfaWd2v1bbRFlI//MFm4VS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"f7adb7cde6b64cc481dcdd9cb00c1914":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bc05b9a3cf5744a5b3f5b764db5ef0a0","IPY_MODEL_4582089aa4164165a3e520daa269dd17","IPY_MODEL_afa1d93317cc48f0b79621152a67ee33"],"layout":"IPY_MODEL_54a966ef488149948356b36bad65296d"}},"bc05b9a3cf5744a5b3f5b764db5ef0a0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cd399e78693b40b8bb64f19ea32581f5","placeholder":"​","style":"IPY_MODEL_8c5919884a1245b7a08038e963a598a6","value":"100%"}},"4582089aa4164165a3e520daa269dd17":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e1989434a0d47da9dd6be1aab30d93d","max":12,"min":0,"orientation":"horizontal","style":"IPY_MODEL_57ae36671a6b426f94d8c47443a8515a","value":12}},"afa1d93317cc48f0b79621152a67ee33":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_77ed573aeed34fe7bce0ece6aca258cf","placeholder":"​","style":"IPY_MODEL_21c6d78354474e73a3b0f26cd92f130b","value":" 12/12 [00:30&lt;00:00,  3.29s/it]"}},"54a966ef488149948356b36bad65296d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd399e78693b40b8bb64f19ea32581f5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c5919884a1245b7a08038e963a598a6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1e1989434a0d47da9dd6be1aab30d93d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"57ae36671a6b426f94d8c47443a8515a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"77ed573aeed34fe7bce0ece6aca258cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"21c6d78354474e73a3b0f26cd92f130b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e5cf33567d834d6dbffb5d006fff2877":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e7c25061ed5c4db88a0915c2b498dd32","IPY_MODEL_eeae5d1164de4f25a83ebf9dc67d6f28","IPY_MODEL_167e0df749854b1683f4d240f59de96c"],"layout":"IPY_MODEL_c7d352bbae624870b21ac9ef26836554"}},"e7c25061ed5c4db88a0915c2b498dd32":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_36be4090c4e64d75bcbcc359befc25e7","placeholder":"​","style":"IPY_MODEL_892d4a86931c44e8b0146979f39f562f","value":"100%"}},"eeae5d1164de4f25a83ebf9dc67d6f28":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_78aba7ca3b5a4fe3875b73c5b50f642b","max":348,"min":0,"orientation":"horizontal","style":"IPY_MODEL_95866bd430f442c8a440f2aa9475725b","value":348}},"167e0df749854b1683f4d240f59de96c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_74c62fc8e1b14f7abe1ac61f5abdcb98","placeholder":"​","style":"IPY_MODEL_080d49a2208c4ac492427f79e1123d21","value":" 348/348 [00:00&lt;00:00, 7349.91it/s]"}},"c7d352bbae624870b21ac9ef26836554":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"36be4090c4e64d75bcbcc359befc25e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"892d4a86931c44e8b0146979f39f562f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"78aba7ca3b5a4fe3875b73c5b50f642b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95866bd430f442c8a440f2aa9475725b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"74c62fc8e1b14f7abe1ac61f5abdcb98":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"080d49a2208c4ac492427f79e1123d21":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# imports"],"metadata":{"id":"OK8UiP6DrqX_"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"xHkY3CpQfM7Y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702160180845,"user_tz":300,"elapsed":19165,"user":{"displayName":"Quentin Auster","userId":"15884522498219677551"}},"outputId":"d73edd21-4271-4cdf-cad0-c2099a725857"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.8/252.8 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install wandb torchsummaryX mne transformers -q"]},{"cell_type":"code","source":["### Torch\n","import  torch\n","import  torch.nn as nn\n","import  torch.nn.functional as F\n","from    torch.optim import lr_scheduler\n","from    torchsummaryX import summary\n","from    torch.utils.data import Dataset, DataLoader, random_split\n","import  torchaudio\n","import  torchaudio.transforms as tat\n","from    torch.nn.utils.rnn import (pad_sequence, pack_padded_sequence,\n","                                   pad_packed_sequence)\n","\n","### General\n","import  random\n","import  numpy as np\n","import  pandas as pd\n","import  pickle\n","import  scipy\n","import  gc\n","import  re\n","from    tqdm.auto import tqdm\n","import  os\n","import  datetime\n","import  time\n","import  wandb\n","import  matplotlib.pyplot as plt\n","import  seaborn as sns\n","\n","# wav2vec2 and EEG processing\n","from    transformers import (AutoProcessor, AutoModelForPreTraining,\n","                             CLIPProcessor, CLIPModel)\n","import  mne\n","\n","# Device\n","DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(\"Device: \", DEVICE)"],"metadata":{"id":"L80P0Fy3gSnQ","executionInfo":{"status":"ok","timestamp":1702160691922,"user_tz":300,"elapsed":182,"user":{"displayName":"Quentin Auster","userId":"15884522498219677551"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"0f41569c-fc7d-405d-eeaa-deac0229406e"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Device:  cpu\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dQolBsAOjmyd","executionInfo":{"status":"ok","timestamp":1702160242842,"user_tz":300,"elapsed":39406,"user":{"displayName":"Quentin Auster","userId":"15884522498219677551"}},"outputId":"c95d2327-b913-4b20-c68a-ef95b3844e14"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["DATA_PATH  = '/content/gdrive/MyDrive/11785-IDLf23/Final project/0_Data/'\n","\n","BRAIN_PATH = os.path.join(DATA_PATH, 'brennan_and_hale_v2')\n","AUDIO_PATH = os.path.join(DATA_PATH, 'brennan_and_hale_v2/audio')\n","PPROC_PATH = os.path.join(DATA_PATH, 'brennan_and_hale_v2/proc/timelock-preprocessing')\n","\n","EEG_PATH   = os.path.join(DATA_PATH, 'eeg-segments')\n","EMBED_PATH = os.path.join(DATA_PATH, 'brennan_wav2vec2_embeddings')\n","\n","PRETRAINED_PATH = '/content/gdrive/MyDrive/11785-IDLf23/Final project/pretrained-models'"],"metadata":{"id":"2HtQm-rYRF5N","executionInfo":{"status":"ok","timestamp":1702166759782,"user_tz":300,"elapsed":198,"user":{"displayName":"Quentin Auster","userId":"15884522498219677551"}}},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":["# data [QA]"],"metadata":{"id":"YlPfcrpLgvxH"}},{"cell_type":"markdown","source":["## dataset definition"],"metadata":{"id":"gMGTtnzfhdjZ"}},{"cell_type":"code","source":["class BrainAudioDataset(torch.utils.data.Dataset):\n","    \"\"\"\n","    Hybrid Memory Efficient Dataset.\n","    Loads all audio embeddings in __init__(). ~1-2min.\n","    Loads eeg in __getitem__().\n","    Loading everything in __init__() crashes.\n","    Loading everything in __getitem__() is super slow.\n","\n","    Load eeg\n","    Pad (add AUD of zeros) to be 62 chan\n","    Mask the bad chans\n","    \"\"\"\n","\n","    def __init__(self, eeg_path=EEG_PATH, embed_path=EMBED_PATH, transforms=None):\n","        super().__init__()\n","\n","        self.eeg_root = eeg_path\n","        self.embed_root = embed_path\n","        self.transforms = transforms\n","\n","        eeg_fnames = sorted(os.listdir(self.eeg_root))\n","        embed_fnames = sorted(os.listdir(self.embed_root))\n","\n","        gc.collect()\n","\n","        self.audio_embeddings = {}\n","        for segment_idx, fname in enumerate(tqdm(embed_fnames), start=1):\n","            audio_fpath = os.path.join(self.embed_root, fname)\n","            audio = torch.load(audio_fpath)\n","            audio_embed = audio.hidden_states[-1]\n","            audio_embed = audio_embed.squeeze(0)\n","            audio_embed = audio_embed.to(DEVICE)\n","            self.audio_embeddings[segment_idx] = audio_embed\n","            del audio, audio_embed\n","            gc.collect()\n","\n","        self.eegs = {}\n","        for idx, fname in enumerate(tqdm(eeg_fnames)):\n","            # Extract subject id and segment (to be returned in __getitem__())\n","            subject_idx, segment_idx = self.extract_info(fname)\n","\n","            # Load eeg filepath\n","            eeg_fpath = os.path.join(self.eeg_root, fname)\n","            self.eegs[idx] = (subject_idx, int(segment_idx), eeg_fpath)\n","\n","        self.length = len(self.eegs)\n","\n","        # montage = mne.channels.make_standard_montage(\"easycap-M10\")\n","\n","    def __len__(self):\n","\n","        return self.length\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        - eeg           : (1, T_eeg, C_eeg)\n","        - audio_embed   : (1, T_audio, 1024)\n","        \"\"\"\n","        # Get eeg\n","        _, segment_idx, eeg_fpath = self.eegs[idx]\n","        eeg = np.load(eeg_fpath)\n","        eeg = torch.tensor(eeg.transpose(), device=DEVICE)\n","\n","        # If nchan < 62, zero pad for the audio channel (should be done in preproc)\n","\n","        # Remove bad chan (should be done in prepoc)\n","\n","        # Retrieve pre-loaded audio embedding\n","        audio_embed = self.audio_embeddings[segment_idx]\n","        return eeg, audio_embed, len(eeg), len(audio_embed)\n","\n","    def collate_fn(self, batch):\n","\n","        eeg, audio_embed, len_eeg, len_audio_embed = zip(*batch)\n","\n","        # pack the mfccs and transcripts using the pad_sequence function from pytorch\n","        batch_eeg_pad = pad_sequence(eeg, batch_first=True)\n","        batch_audio_embed_pad = pad_sequence(audio_embed, batch_first=True)\n","        del eeg, audio_embed\n","\n","        # Apply transformations\n","        if self.transforms is not None:\n","            batch_eeg_pad = self.transforms(batch_eeg_pad)\n","\n","        return (batch_eeg_pad,\n","                batch_audio_embed_pad,\n","                torch.tensor(len_eeg, dtype=torch.int64),\n","                torch.tensor(len_audio_embed, dtype=torch.int64))\n","\n","    def read_sfp(self, file_path):\n","        \"\"\"\n","        Reads a BESA SFP (Surface Point) file (locations of sensors)\n","        \"\"\"\n","\n","        electrodes = {}\n","        with open(file_path, 'r') as file:\n","            for line in file:\n","                parts = line.strip().split()  # Split by whitespace\n","                if len(parts) == 4:\n","                    # Parse the electrode name and coordinates\n","                    name = parts[0]\n","                    x, y, z = map(float, parts[1:])  # Convert strings to floats\n","                    electrodes[name] = (x, y, z)\n","\n","        return electrodes\n","\n","    def extract_info(self, fname):\n","        # This pattern looks for any text (non-digits) followed by digits, a hyphen, and more digits\n","        match = re.match(r'([A-Za-z]+[0-9]+)-([0-9]+).npy', fname)\n","        if match:\n","            return match.group(1), match.group(2)\n","        else:\n","            return None\n"],"metadata":{"id":"_qxzB4c-aAkg","executionInfo":{"status":"ok","timestamp":1702160243054,"user_tz":300,"elapsed":256,"user":{"displayName":"Quentin Auster","userId":"15884522498219677551"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["## dataloader definition"],"metadata":{"id":"ZyxB-LPwhmJs"}},{"cell_type":"markdown","source":["## train/test split"],"metadata":{"id":"aKLE3docr8gM"}},{"cell_type":"code","source":["full_dataset = BrainAudioDataset()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["f7adb7cde6b64cc481dcdd9cb00c1914","bc05b9a3cf5744a5b3f5b764db5ef0a0","4582089aa4164165a3e520daa269dd17","afa1d93317cc48f0b79621152a67ee33","54a966ef488149948356b36bad65296d","cd399e78693b40b8bb64f19ea32581f5","8c5919884a1245b7a08038e963a598a6","1e1989434a0d47da9dd6be1aab30d93d","57ae36671a6b426f94d8c47443a8515a","77ed573aeed34fe7bce0ece6aca258cf","21c6d78354474e73a3b0f26cd92f130b","e5cf33567d834d6dbffb5d006fff2877","e7c25061ed5c4db88a0915c2b498dd32","eeae5d1164de4f25a83ebf9dc67d6f28","167e0df749854b1683f4d240f59de96c","c7d352bbae624870b21ac9ef26836554","36be4090c4e64d75bcbcc359befc25e7","892d4a86931c44e8b0146979f39f562f","78aba7ca3b5a4fe3875b73c5b50f642b","95866bd430f442c8a440f2aa9475725b","74c62fc8e1b14f7abe1ac61f5abdcb98","080d49a2208c4ac492427f79e1123d21"]},"id":"XOwY0sFhtiPx","executionInfo":{"status":"ok","timestamp":1702161195908,"user_tz":300,"elapsed":31132,"user":{"displayName":"Quentin Auster","userId":"15884522498219677551"}},"outputId":"eefa22a7-8920-44d2-9c83-3eb2d37f4e8d"},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/12 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7adb7cde6b64cc481dcdd9cb00c1914"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/348 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5cf33567d834d6dbffb5d006fff2877"}},"metadata":{}}]},{"cell_type":"code","source":["TRAIN_PORTION = 0.8\n","VAL_PORTION   = 0.1\n","TEST_PORTION  = 0.1\n","\n","# Splitting the dataset\n","torch.manual_seed(1)\n","train_data, val_data, test_data = random_split(full_dataset, [TRAIN_PORTION, VAL_PORTION, TEST_PORTION])\n","\n","train_loader = torch.utils.data.DataLoader(\n","    dataset      = train_data,\n","    batch_size   = 8,\n","    shuffle      = True,\n","    drop_last    = False,\n","    num_workers  = 2,\n","    pin_memory   = True,\n","    collate_fn   = full_dataset.collate_fn\n",")\n","\n","val_loader = torch.utils.data.DataLoader(\n","    dataset      = val_data,\n","    batch_size   = 8,\n","    shuffle      = False,\n","    drop_last    = False,\n","    num_workers  = 2,\n","    pin_memory   = True,\n","    collate_fn   = full_dataset.collate_fn\n",")\n","\n","test_loader = torch.utils.data.DataLoader(\n","    dataset      = test_data,\n","    batch_size   = 8,\n","    shuffle      = False,\n","    drop_last    = False,\n","    num_workers  = 2,\n","    pin_memory   = True,\n","    collate_fn   = full_dataset.collate_fn\n",")"],"metadata":{"id":"1Fugny6Lrmr0","executionInfo":{"status":"ok","timestamp":1702161408251,"user_tz":300,"elapsed":155,"user":{"displayName":"Quentin Auster","userId":"15884522498219677551"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["print('-'*80)\n","print(f'Len full data:      {len(full_dataset)}')\n","print(f'Len train data:     {len(full_dataset)}')\n","print(f'Len val data:       {len(full_dataset)}')\n","print(f'Len test data:      {len(full_dataset)}')\n","print('-'*80)\n","print(f'Len Train Loader:   {train_loader.__len__()}')\n","print(f'Len Val Loader:     {val_loader.__len__()}')\n","print(f'Len Test Loader:    {test_loader.__len__()}')\n","\n","gc.collect()\n","\n","for batch in train_loader:\n","    eeg, audio_embedding, l_eeg, l_audio = batch\n","    print(eeg.shape, audio_embedding.shape, l_eeg.shape, l_audio.shape)\n","    print(eeg.dtype, audio_embedding.dtype, l_eeg.dtype, l_audio.dtype)\n","    del eeg, audio_embedding, l_eeg, l_audio\n","    gc.collect()\n","    break\n","\n","for batch in val_loader:\n","    eeg, audio_embedding, l_eeg, l_audio = batch\n","    print(eeg.shape, audio_embedding.shape, l_eeg.shape, l_audio.shape)\n","    print(eeg.dtype, audio_embedding.dtype, l_eeg.dtype, l_audio.dtype)\n","    del eeg, audio_embedding, l_eeg, l_audio\n","    gc.collect()\n","    break\n","\n","for batch in test_loader:\n","    eeg, audio_embedding, l_eeg, l_audio = batch\n","    print(eeg.shape, audio_embedding.shape, l_eeg.shape, l_audio.shape)\n","    print(eeg.dtype, audio_embedding.dtype, l_eeg.dtype, l_audio.dtype)\n","    del eeg, audio_embedding, l_eeg, l_audio\n","    gc.collect()\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QnuiZJ93ttqU","executionInfo":{"status":"ok","timestamp":1702161543844,"user_tz":300,"elapsed":13377,"user":{"displayName":"Quentin Auster","userId":"15884522498219677551"}},"outputId":"e61e163d-da5d-4a5c-b5cc-6499138d2b91"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["--------------------------------------------------------------------------------\n","Len full data:      348\n","Len train data:     348\n","Len val data:       348\n","Len test data:      348\n","--------------------------------------------------------------------------------\n","Len Train Loader:   35\n","Len Val Loader:     5\n","Len Test Loader:    5\n","torch.Size([8, 35022, 62]) torch.Size([8, 3499, 1024]) torch.Size([8]) torch.Size([8])\n","torch.float64 torch.float32 torch.int64 torch.int64\n","torch.Size([8, 33172, 62]) torch.Size([8, 3313, 1024]) torch.Size([8]) torch.Size([8])\n","torch.float64 torch.float32 torch.int64 torch.int64\n","torch.Size([8, 31915, 62]) torch.Size([8, 3499, 1024]) torch.Size([8]) torch.Size([8])\n","torch.float64 torch.float32 torch.int64 torch.int64\n"]}]},{"cell_type":"markdown","source":["# models [KS]"],"metadata":{"id":"g-bEfzz6ho3g"}},{"cell_type":"code","source":["class SubjectLayers(nn.Module): # learn how different are the subjects and based on that normalize the EEG\n","    def __init__(self, in_channels, out_channels, n_subjects):\n","        super().__init__()\n","        self.weights = nn.Parameter(torch.randn(n_subjects, in_channels, out_channels)) # initialize weights for each subject\n","        self.weights.data *= 1 / in_channels**0.5 # normalize weights\n","\n","    def forward(self, x, n_subjects):\n","        _, C, D = self.weights.shape\n","        subject_weights = self.weights.gather(0, n_subjects.view(-1, 1, 1).expand(-1, C, D)) # select the appropriate weights for each subject in the batch\n","        transformed_eeg = torch.einsum(\"bct,bcd->bdt\", x, subject_weights) # apply the subject-specific transformations\n","\n","        return transformed_eeg\n","\n","class ScaledEmbedding(nn.Module): # assign a unique vector to each subject (similar to positional embedding)\n","\n","    def __init__(self, n_subjects, embedding_dim, scale):\n","\n","        super().__init__()\n","        self.embedding = nn.Embedding(n_subjects, embedding_dim)\n","        self.embedding.weight.data /= scale\n","        self.scale = scale\n","\n","    def forward(self, x):\n","        scaled_embedding = self.embedding(x) * self.scale\n","        return scaled_embedding\n","\n","class LayerScale(nn.Module):\n","    def __init__(self, channels, init = 0.1, boost = 5.):\n","        super().__init__()\n","        self.scale = nn.Parameter(torch.zeros(channels, requires_grad=True))\n","        self.scale.data[:] = init / boost\n","        self.boost = boost\n","\n","    def forward(self, x):\n","        return (self.boost * self.scale[:, None]) * x\n","\n","class ConvSequence(nn.Module):\n","\n","    def __init__(self,\n","                 channels = [16, 32, 64, 128],\n","                 kernel = 4, dilation_growth = 1, dilation_period = None,\n","                 stride = 2, dropout = 0.0, leakiness = 0.0,\n","                 groups = 1, decode = False, batch_norm = False,\n","                 dropout_input = 0, skip = False, scale = None,\n","                 rewrite = False, activation_on_last = True,\n","                 post_skip = False, glu = 0, glu_context = 0,\n","                 glu_glu = True, activation = None):\n","\n","        super().__init__()\n","        dilation = 1\n","        channels = tuple(channels)\n","        self.skip = skip\n","        self.sequence = nn.ModuleList()\n","        self.glus = nn.ModuleList()\n","        if activation is None:\n","            activation = partial(nn.LeakyReLU, leakiness)\n","        Conv = nn.Conv1d if not decode else nn.ConvTranspose1d\n","        # build layers\n","        for k, (chin, chout) in enumerate(zip(channels[:-1], channels[1:])):\n","            layers: tp.List[nn.Module] = []\n","            is_last = k == len(channels) - 2\n","\n","            # Set dropout for the input of the conv sequence if defined\n","            if k == 0 and dropout_input:\n","                assert 0 < dropout_input < 1\n","                layers.append(nn.Dropout(dropout_input))\n","\n","            # conv layer\n","            if dilation_growth > 1:\n","                assert kernel % 2 != 0 # supports only odd kernel with dilation\n","            if dilation_period and (k % dilation_period) == 0:\n","                dilation = 1\n","            pad = kernel // 2 * dilation\n","            layers.append(Conv(chin, chout, kernel, stride, pad,\n","                               dilation=dilation, groups=groups if k > 0 else 1))\n","            dilation *= dilation_growth\n","            # non-linearity\n","            if activation_on_last or not is_last:\n","                if batch_norm:\n","                    layers.append(nn.BatchNorm1d(num_features=chout))\n","                layers.append(activation())\n","                if dropout:\n","                    layers.append(nn.Dropout(dropout))\n","                if rewrite:\n","                    layers += [nn.Conv1d(chout, chout, 1), nn.LeakyReLU(leakiness)]\n","            if chin == chout and skip:\n","                if scale is not None:\n","                    layers.append(LayerScale(chout, scale))\n","                if post_skip:\n","                    layers.append(Conv(chout, chout, 1, groups=chout, bias=False))\n","\n","            self.sequence.append(nn.Sequential(*layers))\n","            if glu and (k + 1) % glu == 0:\n","                ch = 2 * chout if glu_glu else chout\n","                act = nn.GLU(dim=1) if glu_glu else activation()\n","                self.glus.append(\n","                    nn.Sequential(\n","                        nn.Conv1d(chout, ch, 1 + 2 * glu_context, padding=glu_context), act))\n","            else:\n","                self.glus.append(None)\n","\n","    def forward(self, x):\n","        for module_idx, module in enumerate(self.sequence):\n","            old_x = x\n","            x = module(x)\n","            if self.skip and x.shape == old_x.shape:\n","                x = x + old_x\n","            glu = self.glus[module_idx]\n","            if glu is not None:\n","                x = glu(x)\n","        return x\n","\n","class Attention(nn.Module): # scaled dot product with relative position encoding and local attention\n","    def __init__(self, channels, radius = 50, heads = 4):\n","        super().__init__()\n","        assert channels % heads == 0\n","        self.content = nn.Conv1d(channels, channels, 1)\n","        self.query = nn.Conv1d(channels, channels, 1)\n","        self.key = nn.Conv1d(channels, channels, 1)\n","        self.embedding = nn.Embedding(radius * 2 + 1, channels // heads)\n","        weight = self.embedding.weight.data\n","        weight[:] = weight.cumsum(0) / torch.arange(1, len(weight) + 1).float().view(-1, 1).sqrt()\n","        self.heads = heads\n","        self.radius = radius\n","        self.bn = nn.BatchNorm1d(channels)\n","        self.fc = nn.Conv1d(channels, channels, 1)\n","        self.scale = nn.Parameter(torch.full([channels], 0.1))\n","\n","    def forward(self, x):\n","\n","        def _split(y):\n","            return y.view(y.shape[0], self.heads, -1, y.shape[2])\n","\n","        content = _split(self.content(x))\n","        query = _split(self.query(x))\n","        key = _split(self.key(x))\n","\n","        batch_size, _, dim, length = content.shape\n","\n","        dots = torch.einsum(\"bhct,bhcs->bhts\", query, key) # first index `t` is query, second index `s` is key.\n","\n","        steps = torch.arange(length, device=x.device)\n","        relative = (steps[:, None] - steps[None, :])\n","        embs = self.embedding.weight.gather(0, self.radius + relative.clamp_(-self.radius, self.radius).view(-1, 1).expand(-1, dim))\n","        embs = embs.view(length, length, -1)\n","        dots += 0.3 * torch.einsum(\"bhct,tsc->bhts\", query, embs)\n","        dots = torch.where(\n","            relative.abs() <= self.radius, dots, torch.tensor(-float('inf')).to(embs))\n","\n","        weights = torch.softmax(dots, dim=-1)\n","        out = torch.einsum(\"bhts,bhcs->bhct\", weights, content)\n","        out += 0.3 * torch.einsum(\"bhts,tsc->bhct\", weights, embs)\n","        out = out.reshape(batch_size, -1, length)\n","        out = F.relu(self.bn(self.fc(out))) * self.scale.view(1, -1, 1)\n","        return out\n","\n","\"\"\"\n","# TEST 1\n","def test_subject_layers():\n","    batch_size = 2\n","    in_channels = 3\n","    out_channels = 2\n","    n_subjects = 1\n","    time_steps = 2\n","    eeg_data = torch.randn(batch_size, in_channels, time_steps)\n","    print(eeg_data)\n","    subjects = torch.randint(0, n_subjects, (batch_size,))\n","    print(subjects)\n","    subject_layers = SubjectLayers(in_channels, out_channels, n_subjects)\n","    output = subject_layers(eeg_data, subjects)\n","    expected_shape = (batch_size, out_channels, time_steps)\n","    assert output.shape == expected_shape, f\"Output shape mismatch: expected {expected_shape}, got {output.shape}\"\n","test_subject_layers()\n","\n","# TEST 2\n","def scaled_embedding():\n","  n_subjects = 10\n","  embedding_dim = 5\n","  scale = 10\n","  scaled_embedding = ScaledEmbedding(n_subjects, embedding_dim, scale)\n","  subject_indices = torch.tensor([1, 2])\n","  embeddings = scaled_embedding(subject_indices)\n","  print(embeddings)\n","scaled_embedding()\n","\n","# TEST 3\n","conv_sequence = ConvSequence(channels=[16, 32, 64, 128])\n","input_tensor = torch.randn(1, 16, 50) # (Batch Size, Channels, Length)\n","output = conv_sequence(input_tensor)\n","output.shape\n","#summary(conv_sequence, input_size=(16, 50))\n","\"\"\""],"metadata":{"id":"2JRp5y9zZEJl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class EEG_Encoder(nn.Module):\n","\n","    def __init__(self,\n","\n","                 in_channels = 64, n_subjects = 30,\n","\n","                 # subject layers\n","                 out_channels = 128,\n","\n","                 # conv\n","                 conv_chanels = [16, 32, 64, 128],\n","                 kernel = 4, stride = 2, conv_dropout = 0,\n","                 batch_norm = False, dropout_input = 0,\n","                 leakiness = 0,\n","\n","                 # lstm\n","                 hidden_size = 128, lstm_layers = 4, lstm_dropout = 0.1,\n","\n","                 # attention\n","                 attention_heads = 4, subject_dim = 64, embedding_scale = 1.0):\n","\n","        super().__init__()\n","\n","        # subject layers (normalization across subjects)\n","        self.subject_layers = SubjectLayers(in_channels, out_channels, n_subjects)\n","\n","        # scaled embedding (optional)\n","        # self.subject_embedding = ScaledEmbedding(n_subjects, subject_dim, embedding_scale)\n","\n","        # convsequence\n","        self.convs = ConvSequence(channels=conv_chanels, kernel = kernel_size,\n","                                  stride = stride, dropout = conv_dropout,\n","                                  batch_norm = batch_norm, leakiness = leakiness,\n","                                  dropout_input = dropout_input)\n","\n","        # lstm\n","        self.lstm = nn.LSTM(input_size    = out_channels,\n","                            hidden_size   = hidden_size//2,\n","                            num_layers    = lstm_layers,\n","                            dropout       = lstm_dropout,\n","                            bidirectional = True,\n","                            batch_first    = True)\n","\n","        # attention\n","        self.attention = Attention(hidden_size, heads=attention_heads)\n","\n","        # final linear layer\n","        self.finalconv1 = nn.Conv1d(hidden_size, out_channels, kernel_size = 1)\n","\n","    def forward(self, eeg_inputs): # to pass as an additional paramater n_subjects=10\n","\n","        print('eeg_inputs before convs', eeg_inputs.shape)\n","        out = self.convs(eeg_inputs)\n","        print('eeg_inputs after convs', out.shape)\n","        print(\"After convs:\", torch.isnan(out).any())\n","\n","        # lstm\n","        out = out.permute(2, 0, 1)\n","        out, _ = self.lstm(out)\n","        out = out.permute(1, 2, 0)\n","        print(\"After LSTM:\", torch.isnan(out).any())\n","\n","        # attention\n","        out = out + self.attention(out)\n","        print(\"After Attention:\", torch.isnan(out).any())\n","\n","        # final\n","        out = self.finalconv1(out)\n","        print(\"Final Output:\", torch.isnan(out).any())\n","\n","        return out\n","\n","# TEST\n","in_channels = 64\n","n_subjects = 30\n","out_channels = 128\n","conv_channels = [64, 32, 64, 128]\n","kernel_size = 4\n","stride = 2\n","conv_dropout = 0\n","batch_norm = False\n","dropout_input = 0\n","leakiness = 0\n","hidden_size = 128\n","lstm_layers = 4\n","lstm_dropout = 0.1\n","attention_heads = 4\n","subject_dim = 64\n","embedding_scale = 1.0\n","\n","eeg_encoder = EEG_Encoder(in_channels, n_subjects, out_channels, conv_channels, kernel_size, stride,\n","                          conv_dropout, batch_norm, dropout_input, leakiness, hidden_size, lstm_layers,\n","                          lstm_dropout, attention_heads, subject_dim, embedding_scale)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","eeg_encoder.to(device)\n","batch_size = 2\n","in_channels = 64\n","sequence_length = 128\n","x_sample = torch.rand(batch_size, in_channels, sequence_length)\n","summary(eeg_encoder, x_sample.to(device))"],"metadata":{"id":"SUfHS4daZHHy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## decoder\n","\n","audio -> wav2vec2 -> embedding space <- our model <- eeg\n","\n","once in common embedding space, use CLIP\n","\n","From CLIP --> decode to words"],"metadata":{"id":"Oxf0pcOFpYjI"}},{"cell_type":"markdown","source":["# losses and metrics"],"metadata":{"id":"N4TkGFvZjt2D"}},{"cell_type":"markdown","source":["## clip [DS]"],"metadata":{"id":"VY5hMIVkj_oj"}},{"cell_type":"code","source":[],"metadata":{"id":"spuoOZ9Gju3y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## wer [CM]"],"metadata":{"id":"uSY_MAO6j8hI"}},{"cell_type":"markdown","source":["### general"],"metadata":{"id":"fR6YV-9TrENJ"}},{"cell_type":"code","source":[],"metadata":{"id":"h-9qo2zfkAp1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### vocab-specific"],"metadata":{"id":"wVIA4rXSrFt3"}},{"cell_type":"code","source":[],"metadata":{"id":"aGMPNPaSrHAI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# trainer\n"],"metadata":{"id":"82KWAuB6jPQo"}},{"cell_type":"code","source":["class Trainer:\n","    def __init__(self, model, loader, optimizer, criterion, scheduler, max_epochs= 1, run_id= 'exp'):\n","\n","        self.model      = model\n","        self.loader     = loader\n","        self.optimizer  = optimizer\n","        self.criterion  = criterion\n","        self.scheduler = scheduler\n","\n","        self.train_losses           = []\n","        self.val_losses             = []\n","        self.prediction_probs       = []\n","        self.prediction_probs_test  = []\n","        self.generated_texts_test   = []\n","        self.epochs                 = 0\n","        self.max_epochs             = max_epochs\n","        self.run_id                 = run_id\n","\n","\n","    def calculate_loss(self, out, target):\n","        # output: (B, T, Vocab_size) - probability distributions\n","        # target: (B, T)\n","        # Read the documentation of CrossEntropyLoss and try to understand how it takes inputs\n","\n","        # Tip: If your target is of shape (B, T) it means that you have B batches with T words.\n","        # Tip: What is the total number of words in this batch?\n","        # Tip: Crossentropy calculates the loss between a label and its probability distribution.\n","\n","        out     = out.view(-1, out.size(-1)) # TODO\n","        targets = target.view(-1)  # TODO\n","        loss    = self.criterion(out, targets)\n","\n","        return loss\n","\n","\n","    def train(self):\n","\n","        self.model.train() # set to training mode\n","        self.model.to(DEVICE)\n","        epoch_loss  = 0\n","        num_batches = 0\n","\n","        for batch_num, (inputs, targets) in enumerate(tqdm(self.loader)):\n","\n","            # TODO: Complete the loop. You should be able to complete this without any helper comments after 3 HWs\n","            # Tip: Mixed precision training\n","            # For loss calculation, use the calculate_loss function. You need to complete it before using.\n","\n","            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n","\n","            self.optimizer.zero_grad()\n","\n","            output, _ = self.model(inputs)\n","            loss = self.calculate_loss(output, targets)\n","            loss.backward()\n","            self.optimizer.step()\n","\n","            loss_val = loss.item()\n","            epoch_loss += loss_val\n","            num_batches += 1\n","\n","        #epoch_loss = epoch_loss / (batch_num + 1)\n","        epoch_loss = epoch_loss / num_batches\n","        self.epochs += 1\n","        print('[TRAIN] \\tEpoch [%d/%d] \\tLoss: %.4f \\tLr: %.6f'\n","                      % (self.epochs, self.max_epochs, epoch_loss, self.optimizer.param_groups[0]['lr']))\n","        self.train_losses.append(epoch_loss)\n","\n","\n","    def test(self): # Don't change this function\n","\n","        self.model.eval() # set to eval mode\n","        prediction_probs     = self.model.predict(fixtures_pred['inp']).detach().cpu().numpy() # get predictions\n","        self.prediction_probs.append(prediction_probs)\n","\n","        generated_indexes_test   = self.model.generate(fixtures_gen_test, 10).detach().cpu().numpy() # generated predictions for 10 words\n","\n","        nll                   = get_prediction_nll(prediction_probs, fixtures_pred['out'])\n","        generated_texts_test  = make_generation_text(fixtures_gen_test, generated_indexes_test, VOCAB)\n","        self.val_losses.append(nll)\n","\n","        self.generated_texts_test.append(generated_texts_test)\n","\n","        # generate predictions for test data\n","        prediction_probs_test = self.model.predict(fixtures_pred_test['inp']).detach().cpu().numpy() # get predictions\n","        self.prediction_probs_test.append(prediction_probs_test)\n","\n","        print('[VAL] \\tEpoch [%d/%d] \\tLoss: %.4f'\n","                      % (self.epochs, self.max_epochs, nll))\n","        return nll\n","\n","\n","    def save(self): # Don't change this function\n","\n","        model_path = os.path.join('hw4/experiments', self.run_id, 'model-{}.pkl'.format(self.epochs))\n","        torch.save({'state_dict': self.model.state_dict()}, model_path)\n","        np.save(os.path.join('hw4/experiments', self.run_id, 'prediction-probs-{}.npy'.format(self.epochs)), self.prediction_probs[-1])\n","        np.save(os.path.join('hw4/experiments', self.run_id, 'prediction-probs-test-{}.npy'.format(self.epochs)), self.prediction_probs_test[-1])\n","\n","        with open(os.path.join('hw4/experiments', self.run_id, 'generated-texts-{}-test.txt'.format(self.epochs)), 'w') as fw:\n","            fw.write(self.generated_texts_test[-1])"],"metadata":{"id":"_SIEgYN4jS97"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# experiments"],"metadata":{"id":"N4UE5_2vjUtQ"}},{"cell_type":"code","source":[],"metadata":{"id":"rVoKYgt-j0RE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# evaluation"],"metadata":{"id":"dv73E9hkjYnz"}},{"cell_type":"markdown","source":["## viz"],"metadata":{"id":"EXoEy94Dj3kt"}},{"cell_type":"code","source":[],"metadata":{"id":"q_cyKW1Sj0ol"},"execution_count":null,"outputs":[]}]}