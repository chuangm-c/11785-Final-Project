{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["VGGW29McBsXA","VpwsQQRCWtoz","7v6qIWPExjji","07Ezn5GK4KIq","c0TqyTlnM7uJ","MyunViVM5aSZ","C1UH6pcWh-0G","N4TkGFvZjt2D","82KWAuB6jPQo","N4UE5_2vjUtQ"],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# imports"],"metadata":{"id":"VGGW29McBsXA"}},{"cell_type":"code","source":["!pip install wandb torchsummaryX mne transformers -q"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pOgws5nEhBsx","executionInfo":{"status":"ok","timestamp":1702138758253,"user_tz":300,"elapsed":15263,"user":{"displayName":"Kateryna Shapovalenko","userId":"16393911302329786226"}},"outputId":"de7d5c61-3206-4686-ae87-a5a4c29cec0b"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.8/252.8 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":["### Torch\n","import  torch\n","import  torch.nn as nn\n","import  torch.nn.functional as F\n","from    torch.optim import lr_scheduler\n","from    torchsummaryX import summary\n","from    torch.utils.data import Dataset, DataLoader\n","import  torchaudio\n","import  torchaudio.transforms as tat\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","\n","### General\n","import  random\n","import  numpy as np\n","import  pandas as pd\n","import  scipy\n","import  gc\n","from    tqdm.auto import tqdm\n","import  os\n","import  datetime\n","import  time\n","import  wandb\n","import  matplotlib.pyplot as plt\n","import  seaborn as sns\n","\n","### Other\n","\n","from functools import partial\n","import logging\n","import math\n","import typing as tp\n","\n","# wav2vec2 and EEG processing\n","from    transformers import (AutoProcessor, AutoModelForPreTraining,\n","                             CLIPProcessor, CLIPModel)\n","import  mne\n","from transformers import Wav2Vec2Model"],"metadata":{"id":"L80P0Fy3gSnQ","executionInfo":{"status":"ok","timestamp":1702138984099,"user_tz":300,"elapsed":111,"user":{"displayName":"Kateryna Shapovalenko","userId":"16393911302329786226"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["# working directory"],"metadata":{"id":"VpwsQQRCWtoz"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"doTTk4H_WxMu","executionInfo":{"status":"ok","timestamp":1702138790748,"user_tz":300,"elapsed":18967,"user":{"displayName":"Kateryna Shapovalenko","userId":"16393911302329786226"}},"outputId":"c7ce88cc-37ed-48b6-ecb9-8d4c760ee9d7"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["DATA_PATH  = '/content/gdrive/MyDrive/11785-IDLf23/Final_project/0_Data/'\n","\n","BRAIN_PATH = os.path.join(DATA_PATH, 'brennan_and_hale_v2')\n","AUDIO_PATH = os.path.join(DATA_PATH, 'brennan_and_hale_v2/audio')\n","PPROC_PATH = os.path.join(DATA_PATH, 'brennan_and_hale_v2/proc/timelock-preprocessing')\n","\n","EEG_PATH   = os.path.join(DATA_PATH, 'eeg-segments')\n","EMBED_PATH = os.path.join(DATA_PATH, 'brennan_wav2vec2_embeddings')"],"metadata":{"id":"OElDc-hLWrbu","executionInfo":{"status":"ok","timestamp":1702138790748,"user_tz":300,"elapsed":2,"user":{"displayName":"Kateryna Shapovalenko","userId":"16393911302329786226"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# data [QA]"],"metadata":{"id":"YlPfcrpLgvxH"}},{"cell_type":"markdown","source":["## dataset definition"],"metadata":{"id":"gMGTtnzfhdjZ"}},{"cell_type":"code","source":["print(os.listdir(EEG_PATH))\n","\n","# for fname in os.listdir(EEG_PATH):\n","#     print(fname[:3])\n","\n","import re\n","\n","def extract_info(s):\n","    # This pattern looks for any text (non-digits) followed by digits, a hyphen, and more digits\n","    match = re.match(r'([A-Za-z]+[0-9]+)-([0-9]+).pkl', s)\n","    if match:\n","        return match.group(1), match.group(2)\n","    else:\n","        return None\n","\n","# for fname in os.listdir(EEG_PATH):\n","#     print(extract_info(fname))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y8jhO_QOXgfF","executionInfo":{"status":"ok","timestamp":1702138803453,"user_tz":300,"elapsed":3178,"user":{"displayName":"Kateryna Shapovalenko","userId":"16393911302329786226"}},"outputId":"a87d7e14-0992-4fa0-a30b-1566a78eebfb"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["['S01-1.npy', 'S01-2.npy', 'S01-3.npy', 'S01-4.npy', 'S01-5.npy', 'S01-6.npy', 'S01-7.npy', 'S01-8.npy', 'S01-9.npy', 'S01-10.npy', 'S01-11.npy', 'S01-12.npy', 'S03-1.npy', 'S03-2.npy', 'S03-3.npy', 'S03-4.npy', 'S03-5.npy', 'S03-6.npy', 'S03-7.npy', 'S03-8.npy', 'S03-9.npy', 'S03-10.npy', 'S03-11.npy', 'S03-12.npy', 'S04-1.npy', 'S04-2.npy', 'S04-3.npy', 'S04-4.npy', 'S04-5.npy', 'S04-6.npy', 'S04-7.npy', 'S04-8.npy', 'S04-9.npy', 'S04-10.npy', 'S04-11.npy', 'S04-12.npy', 'S05-1.npy', 'S05-2.npy', 'S05-3.npy', 'S05-4.npy', 'S05-5.npy', 'S05-6.npy', 'S05-7.npy', 'S05-8.npy', 'S05-9.npy', 'S05-10.npy', 'S05-11.npy', 'S05-12.npy', 'S06-1.npy', 'S06-2.npy', 'S06-3.npy', 'S06-4.npy', 'S06-5.npy', 'S06-6.npy', 'S06-7.npy', 'S06-8.npy', 'S06-9.npy', 'S06-10.npy', 'S06-11.npy', 'S06-12.npy', 'S08-1.npy', 'S08-2.npy', 'S08-3.npy', 'S08-4.npy', 'S08-5.npy', 'S08-6.npy', 'S08-7.npy', 'S08-8.npy', 'S08-9.npy', 'S08-10.npy', 'S08-11.npy', 'S08-12.npy', 'S10-1.npy', 'S10-2.npy', 'S10-3.npy', 'S10-4.npy', 'S10-5.npy', 'S10-6.npy', 'S10-7.npy', 'S10-8.npy', 'S10-9.npy', 'S10-10.npy', 'S10-11.npy', 'S10-12.npy', 'S11-1.npy', 'S11-2.npy', 'S11-3.npy', 'S11-4.npy', 'S11-5.npy', 'S11-6.npy', 'S11-7.npy', 'S11-8.npy', 'S11-9.npy', 'S11-10.npy', 'S11-11.npy', 'S11-12.npy', 'S12-1.npy', 'S12-2.npy', 'S12-3.npy', 'S12-4.npy', 'S12-5.npy', 'S12-6.npy', 'S12-7.npy', 'S12-8.npy', 'S12-9.npy', 'S12-10.npy', 'S12-11.npy', 'S12-12.npy', 'S13-1.npy', 'S13-2.npy', 'S13-3.npy', 'S13-4.npy', 'S13-5.npy', 'S13-6.npy', 'S13-7.npy', 'S13-8.npy', 'S13-9.npy', 'S13-10.npy', 'S13-11.npy', 'S13-12.npy', 'S14-1.npy', 'S14-2.npy', 'S14-3.npy', 'S14-4.npy', 'S14-5.npy', 'S14-6.npy', 'S14-7.npy', 'S14-8.npy', 'S14-9.npy', 'S14-10.npy', 'S14-11.npy', 'S14-12.npy', 'S15-1.npy', 'S15-2.npy', 'S15-3.npy', 'S15-4.npy', 'S15-5.npy', 'S15-6.npy', 'S15-7.npy', 'S15-8.npy', 'S15-9.npy', 'S15-10.npy', 'S15-11.npy', 'S15-12.npy', 'S16-1.npy', 'S16-2.npy', 'S16-3.npy', 'S16-4.npy', 'S16-5.npy', 'S16-6.npy', 'S16-7.npy', 'S16-8.npy', 'S16-9.npy', 'S16-10.npy', 'S16-11.npy', 'S16-12.npy', 'S17-1.npy', 'S17-2.npy', 'S17-3.npy', 'S17-4.npy', 'S17-5.npy', 'S17-6.npy', 'S17-7.npy', 'S17-8.npy', 'S17-9.npy', 'S17-10.npy', 'S17-11.npy', 'S17-12.npy', 'S18-1.npy', 'S18-2.npy', 'S18-3.npy', 'S18-4.npy', 'S18-5.npy', 'S18-6.npy', 'S18-7.npy', 'S18-8.npy', 'S18-9.npy', 'S18-10.npy', 'S18-11.npy', 'S18-12.npy', 'S19-1.npy', 'S19-2.npy', 'S19-3.npy', 'S19-4.npy', 'S19-5.npy', 'S19-6.npy', 'S19-7.npy', 'S19-8.npy', 'S19-9.npy', 'S19-10.npy', 'S19-11.npy', 'S19-12.npy', 'S20-1.npy', 'S20-2.npy', 'S20-3.npy', 'S20-4.npy', 'S20-5.npy', 'S20-6.npy', 'S20-7.npy', 'S20-8.npy', 'S20-9.npy', 'S20-10.npy', 'S20-11.npy', 'S20-12.npy', 'S21-1.npy', 'S21-2.npy', 'S21-3.npy', 'S21-4.npy', 'S21-5.npy', 'S21-6.npy', 'S21-7.npy', 'S21-8.npy', 'S21-9.npy', 'S21-10.npy', 'S21-11.npy', 'S21-12.npy', 'S22-1.npy', 'S22-2.npy', 'S22-3.npy', 'S22-4.npy', 'S22-5.npy', 'S22-6.npy', 'S22-7.npy', 'S22-8.npy', 'S22-9.npy', 'S22-10.npy', 'S22-11.npy', 'S22-12.npy', 'S25-1.npy', 'S25-2.npy', 'S25-3.npy', 'S25-4.npy', 'S25-5.npy', 'S25-6.npy', 'S25-7.npy', 'S25-8.npy', 'S25-9.npy', 'S25-10.npy', 'S25-11.npy', 'S25-12.npy', 'S37-1.npy', 'S37-2.npy', 'S37-3.npy', 'S37-4.npy', 'S37-5.npy', 'S37-6.npy', 'S37-7.npy', 'S37-8.npy', 'S37-9.npy', 'S37-10.npy', 'S37-11.npy', 'S37-12.npy', 'S38-1.npy', 'S38-2.npy', 'S38-3.npy', 'S38-4.npy', 'S38-5.npy', 'S38-6.npy', 'S38-7.npy', 'S38-8.npy', 'S38-9.npy', 'S38-10.npy', 'S38-11.npy', 'S38-12.npy', 'S39-1.npy', 'S39-2.npy', 'S39-3.npy', 'S39-4.npy', 'S39-5.npy', 'S39-6.npy', 'S39-7.npy', 'S39-8.npy', 'S39-9.npy', 'S39-10.npy', 'S39-11.npy', 'S39-12.npy', 'S40-1.npy', 'S40-2.npy', 'S40-3.npy', 'S40-4.npy', 'S40-5.npy', 'S40-6.npy', 'S40-7.npy', 'S40-8.npy', 'S40-9.npy', 'S40-10.npy', 'S40-11.npy', 'S40-12.npy', 'S41-1.npy', 'S41-2.npy', 'S41-3.npy', 'S41-4.npy', 'S41-5.npy', 'S41-6.npy', 'S41-7.npy', 'S41-8.npy', 'S41-9.npy', 'S41-10.npy', 'S41-11.npy', 'S41-12.npy', 'S42-1.npy', 'S42-2.npy', 'S42-3.npy', 'S42-4.npy', 'S42-5.npy', 'S42-6.npy', 'S42-7.npy', 'S42-8.npy', 'S42-9.npy', 'S42-10.npy', 'S42-11.npy', 'S42-12.npy', 'S44-1.npy', 'S44-2.npy', 'S44-3.npy', 'S44-4.npy', 'S44-5.npy', 'S44-6.npy', 'S44-7.npy', 'S44-8.npy', 'S44-9.npy', 'S44-10.npy', 'S44-11.npy', 'S44-12.npy', 'S45-1.npy', 'S45-2.npy', 'S45-3.npy', 'S45-4.npy', 'S45-5.npy', 'S45-6.npy', 'S45-7.npy', 'S45-8.npy', 'S45-9.npy', 'S45-10.npy', 'S45-11.npy', 'S45-12.npy', 'S48-1.npy', 'S48-2.npy', 'S48-3.npy', 'S48-4.npy', 'S48-5.npy', 'S48-6.npy', 'S48-7.npy', 'S48-8.npy', 'S48-9.npy', 'S48-10.npy', 'S48-11.npy', 'S48-12.npy']\n"]}]},{"cell_type":"code","source":["class BrainAudioDataset(torch.utils.data.Dataset):\n","\n","    def __init__(self, eeg_path=EEG_PATH, embed_path=EMBED_PATH, transforms=None):\n","        super().__init__()\n","\n","        self.eeg_root = eeg_path\n","        self.embed_root = embed_path\n","        self.transforms = transforms\n","\n","        eeg_fnames = sorted(os.listdir(self.eeg_root))\n","        print(eeg_fnames)\n","\n","        self.data = {}\n","        for idx, fname in enumerate(eeg_fnames):\n","            subject_id, segment = self.extract_info(fname)\n","            eeg_fpath = os.path.join(self.eeg_root, fname)\n","            audio_fpath = os.path.join(self.embed_root, f'audio-{segment}.pkl')\n","            self.data[idx] = (eeg_fpath, audio_fpath)\n","\n","        self.length = len(self.data)\n","\n","    def __len__(self):\n","\n","        return self.length\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        - eeg               : (1, T_eeg, C_eeg)\n","        - audio_embedding   : (1, T_audio, 1024)\n","        - audio_logits      : (1, T_audio, 33)\n","        \"\"\"\n","        eeg_path, audio_path = self.data[idx]\n","\n","        eeg = np.load(eeg_path)\n","        eeg = torch.tensor(eeg.transpose())\n","\n","        audio = torch.load(audio_path)\n","\n","        audio_embed = audio.hidden_states[-1]\n","\n","        return eeg, audio_embed.squeeze(0), len(eeg), len(audio_embed)\n","\n","    def collate_fn(self, batch):\n","\n","        eeg, audio_embed, len_eeg, len_audio_embed = zip(*batch)\n","\n","        # pack the mfccs and transcripts using the pad_sequence function from pytorch\n","        batch_eeg_pad = torch.nn.utils.rnn.pad_sequence(eeg, batch_first=True)\n","        batch_audio_embed_pad = torch.nn.utils.rnn.pad_sequence(audio_embed, batch_first=True)\n","        del eeg, audio_embed\n","\n","        # Apply transformations\n","        if self.transforms is not None:\n","            batch_eeg_pad = self.transforms(batch_eeg_pad)\n","\n","        return (batch_eeg_pad,\n","                batch_audio_embed_pad,\n","                torch.tensor(len_eeg, dtype=torch.int64),\n","                torch.tensor(len_audio_embed, dtype=torch.int64))\n","\n","    def read_sfp(self, file_path):\n","        \"\"\"\n","        Reads a BESA SFP (Surface Point) file (locations of sensors)\n","        \"\"\"\n","\n","        electrodes = {}\n","        with open(file_path, 'r') as file:\n","            for line in file:\n","                parts = line.strip().split()  # Split by whitespace\n","                if len(parts) == 4:\n","                    # Parse the electrode name and coordinates\n","                    name = parts[0]\n","                    x, y, z = map(float, parts[1:])  # Convert strings to floats\n","                    electrodes[name] = (x, y, z)\n","\n","        return electrodes\n","\n","    def extract_info(self, fname):\n","        # This pattern looks for any text (non-digits) followed by digits, a hyphen, and more digits\n","        match = re.match(r'([A-Za-z]+[0-9]+)-([0-9]+).npy', fname)\n","        if match:\n","            return match.group(1), match.group(2)\n","        else:\n","            return None"],"metadata":{"id":"S7b0v62CXl8D","executionInfo":{"status":"ok","timestamp":1702139043049,"user_tz":300,"elapsed":1,"user":{"displayName":"Kateryna Shapovalenko","userId":"16393911302329786226"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["dataset = BrainAudioDataset()\n","\n","for i in range(3):\n","    eeg, audio_embedding, l_eeg, l_audio = dataset[i]\n","    print(eeg.shape, audio_embedding.shape, l_eeg, l_audio)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vM9De5ihjMHO","executionInfo":{"status":"ok","timestamp":1702139048943,"user_tz":300,"elapsed":1552,"user":{"displayName":"Kateryna Shapovalenko","userId":"16393911302329786226"}},"outputId":"b3bf5345-1d46-4ad9-c7a1-7d2760e7d2ca"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["['S01-1.npy', 'S01-10.npy', 'S01-11.npy', 'S01-12.npy', 'S01-2.npy', 'S01-3.npy', 'S01-4.npy', 'S01-5.npy', 'S01-6.npy', 'S01-7.npy', 'S01-8.npy', 'S01-9.npy', 'S03-1.npy', 'S03-10.npy', 'S03-11.npy', 'S03-12.npy', 'S03-2.npy', 'S03-3.npy', 'S03-4.npy', 'S03-5.npy', 'S03-6.npy', 'S03-7.npy', 'S03-8.npy', 'S03-9.npy', 'S04-1.npy', 'S04-10.npy', 'S04-11.npy', 'S04-12.npy', 'S04-2.npy', 'S04-3.npy', 'S04-4.npy', 'S04-5.npy', 'S04-6.npy', 'S04-7.npy', 'S04-8.npy', 'S04-9.npy', 'S05-1.npy', 'S05-10.npy', 'S05-11.npy', 'S05-12.npy', 'S05-2.npy', 'S05-3.npy', 'S05-4.npy', 'S05-5.npy', 'S05-6.npy', 'S05-7.npy', 'S05-8.npy', 'S05-9.npy', 'S06-1.npy', 'S06-10.npy', 'S06-11.npy', 'S06-12.npy', 'S06-2.npy', 'S06-3.npy', 'S06-4.npy', 'S06-5.npy', 'S06-6.npy', 'S06-7.npy', 'S06-8.npy', 'S06-9.npy', 'S08-1.npy', 'S08-10.npy', 'S08-11.npy', 'S08-12.npy', 'S08-2.npy', 'S08-3.npy', 'S08-4.npy', 'S08-5.npy', 'S08-6.npy', 'S08-7.npy', 'S08-8.npy', 'S08-9.npy', 'S10-1.npy', 'S10-10.npy', 'S10-11.npy', 'S10-12.npy', 'S10-2.npy', 'S10-3.npy', 'S10-4.npy', 'S10-5.npy', 'S10-6.npy', 'S10-7.npy', 'S10-8.npy', 'S10-9.npy', 'S11-1.npy', 'S11-10.npy', 'S11-11.npy', 'S11-12.npy', 'S11-2.npy', 'S11-3.npy', 'S11-4.npy', 'S11-5.npy', 'S11-6.npy', 'S11-7.npy', 'S11-8.npy', 'S11-9.npy', 'S12-1.npy', 'S12-10.npy', 'S12-11.npy', 'S12-12.npy', 'S12-2.npy', 'S12-3.npy', 'S12-4.npy', 'S12-5.npy', 'S12-6.npy', 'S12-7.npy', 'S12-8.npy', 'S12-9.npy', 'S13-1.npy', 'S13-10.npy', 'S13-11.npy', 'S13-12.npy', 'S13-2.npy', 'S13-3.npy', 'S13-4.npy', 'S13-5.npy', 'S13-6.npy', 'S13-7.npy', 'S13-8.npy', 'S13-9.npy', 'S14-1.npy', 'S14-10.npy', 'S14-11.npy', 'S14-12.npy', 'S14-2.npy', 'S14-3.npy', 'S14-4.npy', 'S14-5.npy', 'S14-6.npy', 'S14-7.npy', 'S14-8.npy', 'S14-9.npy', 'S15-1.npy', 'S15-10.npy', 'S15-11.npy', 'S15-12.npy', 'S15-2.npy', 'S15-3.npy', 'S15-4.npy', 'S15-5.npy', 'S15-6.npy', 'S15-7.npy', 'S15-8.npy', 'S15-9.npy', 'S16-1.npy', 'S16-10.npy', 'S16-11.npy', 'S16-12.npy', 'S16-2.npy', 'S16-3.npy', 'S16-4.npy', 'S16-5.npy', 'S16-6.npy', 'S16-7.npy', 'S16-8.npy', 'S16-9.npy', 'S17-1.npy', 'S17-10.npy', 'S17-11.npy', 'S17-12.npy', 'S17-2.npy', 'S17-3.npy', 'S17-4.npy', 'S17-5.npy', 'S17-6.npy', 'S17-7.npy', 'S17-8.npy', 'S17-9.npy', 'S18-1.npy', 'S18-10.npy', 'S18-11.npy', 'S18-12.npy', 'S18-2.npy', 'S18-3.npy', 'S18-4.npy', 'S18-5.npy', 'S18-6.npy', 'S18-7.npy', 'S18-8.npy', 'S18-9.npy', 'S19-1.npy', 'S19-10.npy', 'S19-11.npy', 'S19-12.npy', 'S19-2.npy', 'S19-3.npy', 'S19-4.npy', 'S19-5.npy', 'S19-6.npy', 'S19-7.npy', 'S19-8.npy', 'S19-9.npy', 'S20-1.npy', 'S20-10.npy', 'S20-11.npy', 'S20-12.npy', 'S20-2.npy', 'S20-3.npy', 'S20-4.npy', 'S20-5.npy', 'S20-6.npy', 'S20-7.npy', 'S20-8.npy', 'S20-9.npy', 'S21-1.npy', 'S21-10.npy', 'S21-11.npy', 'S21-12.npy', 'S21-2.npy', 'S21-3.npy', 'S21-4.npy', 'S21-5.npy', 'S21-6.npy', 'S21-7.npy', 'S21-8.npy', 'S21-9.npy', 'S22-1.npy', 'S22-10.npy', 'S22-11.npy', 'S22-12.npy', 'S22-2.npy', 'S22-3.npy', 'S22-4.npy', 'S22-5.npy', 'S22-6.npy', 'S22-7.npy', 'S22-8.npy', 'S22-9.npy', 'S25-1.npy', 'S25-10.npy', 'S25-11.npy', 'S25-12.npy', 'S25-2.npy', 'S25-3.npy', 'S25-4.npy', 'S25-5.npy', 'S25-6.npy', 'S25-7.npy', 'S25-8.npy', 'S25-9.npy', 'S37-1.npy', 'S37-10.npy', 'S37-11.npy', 'S37-12.npy', 'S37-2.npy', 'S37-3.npy', 'S37-4.npy', 'S37-5.npy', 'S37-6.npy', 'S37-7.npy', 'S37-8.npy', 'S37-9.npy', 'S38-1.npy', 'S38-10.npy', 'S38-11.npy', 'S38-12.npy', 'S38-2.npy', 'S38-3.npy', 'S38-4.npy', 'S38-5.npy', 'S38-6.npy', 'S38-7.npy', 'S38-8.npy', 'S38-9.npy', 'S39-1.npy', 'S39-10.npy', 'S39-11.npy', 'S39-12.npy', 'S39-2.npy', 'S39-3.npy', 'S39-4.npy', 'S39-5.npy', 'S39-6.npy', 'S39-7.npy', 'S39-8.npy', 'S39-9.npy', 'S40-1.npy', 'S40-10.npy', 'S40-11.npy', 'S40-12.npy', 'S40-2.npy', 'S40-3.npy', 'S40-4.npy', 'S40-5.npy', 'S40-6.npy', 'S40-7.npy', 'S40-8.npy', 'S40-9.npy', 'S41-1.npy', 'S41-10.npy', 'S41-11.npy', 'S41-12.npy', 'S41-2.npy', 'S41-3.npy', 'S41-4.npy', 'S41-5.npy', 'S41-6.npy', 'S41-7.npy', 'S41-8.npy', 'S41-9.npy', 'S42-1.npy', 'S42-10.npy', 'S42-11.npy', 'S42-12.npy', 'S42-2.npy', 'S42-3.npy', 'S42-4.npy', 'S42-5.npy', 'S42-6.npy', 'S42-7.npy', 'S42-8.npy', 'S42-9.npy', 'S44-1.npy', 'S44-10.npy', 'S44-11.npy', 'S44-12.npy', 'S44-2.npy', 'S44-3.npy', 'S44-4.npy', 'S44-5.npy', 'S44-6.npy', 'S44-7.npy', 'S44-8.npy', 'S44-9.npy', 'S45-1.npy', 'S45-10.npy', 'S45-11.npy', 'S45-12.npy', 'S45-2.npy', 'S45-3.npy', 'S45-4.npy', 'S45-5.npy', 'S45-6.npy', 'S45-7.npy', 'S45-8.npy', 'S45-9.npy', 'S48-1.npy', 'S48-10.npy', 'S48-11.npy', 'S48-12.npy', 'S48-2.npy', 'S48-3.npy', 'S48-4.npy', 'S48-5.npy', 'S48-6.npy', 'S48-7.npy', 'S48-8.npy', 'S48-9.npy']\n","torch.Size([28814, 62]) torch.Size([2876, 1024]) 28814 1\n","torch.Size([30655, 62]) torch.Size([3063, 1024]) 30655 1\n","torch.Size([28106, 62]) torch.Size([2808, 1024]) 28106 1\n"]}]},{"cell_type":"markdown","source":["## dataloader definition"],"metadata":{"id":"ZyxB-LPwhmJs"}},{"cell_type":"code","source":["train_loader = torch.utils.data.DataLoader(\n","    dataset      = dataset,\n","    batch_size   = 2,\n","    shuffle      = True,\n","    drop_last    = False,\n","    num_workers  = 8,\n","    pin_memory   = True,\n","    collate_fn   = dataset.collate_fn)\n","\n","print(len(dataset))\n","print(train_loader.__len__())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DUQHYlw_XsKV","executionInfo":{"status":"ok","timestamp":1702139049864,"user_tz":300,"elapsed":114,"user":{"displayName":"Kateryna Shapovalenko","userId":"16393911302329786226"}},"outputId":"ef04a87c-6e9f-4fc5-b538-172157f3d7f4"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["348\n","174\n"]}]},{"cell_type":"code","source":["for batch in train_loader:\n","    x, y, x_len, y_len = batch\n","    print(x.shape, y.shape, x_len.shape, y_len.shape)\n","    print(x.dtype, y.dtype, x_len.dtype, y_len.dtype)\n","    del x, y, x_len, y_len\n","    break\n","\n","# x: raw eeg [batch_size, sequence length, number of features]\n","# y: audio embedding [batch_size, sequence length, number of features]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v4LVdfkOsuZO","executionInfo":{"status":"ok","timestamp":1702139133757,"user_tz":300,"elapsed":82646,"user":{"displayName":"Kateryna Shapovalenko","userId":"16393911302329786226"}},"outputId":"b84cf02e-37aa-4d80-c22c-8e787a856012"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 31914, 62]) torch.Size([2, 3188, 1024]) torch.Size([2]) torch.Size([2])\n","torch.float64 torch.float32 torch.int64 torch.int64\n"]}]},{"cell_type":"markdown","source":["# models [KS]"],"metadata":{"id":"g-bEfzz6ho3g"}},{"cell_type":"markdown","source":["Relevant Meta folders: [Features folder](https://github.com/facebookresearch/brainmagick/tree/main/bm/features) and [Model folder](https://github.com/facebookresearch/brainmagick/tree/main/bm/models).\n","\n","\n","**Shared model components and models:**\n","\n","1. **[common.py](https://github.com/facebookresearch/brainmagick/blob/main/bm/models/common.py)** - collection of components used by other models\n","\n","  - ScaledEmbedding\n","\n","  - SubjectLayers\n","\n","  - LayerScale\n","\n","  - ConvSequence\n","\n","  - DualPathRNN\n","\n","  - PositionGetter\n","\n","  - FourierEmb\n","\n","  - ChannelDropout\n","\n","  - ChannelMerger\n","\n","2. **[convrnn.py](https://github.com/facebookresearch/brainmagick/blob/main/bm/models/convrnn.py)** - a model used both as an a encoder and a decoder.\n","\n","  - LSTM\n","\n","  - Attention\n","\n","  - ConvRNN\n","  \n","    - SubjectLayers (subject layer)\n","\n","    - ScaledEmbedding (subject embedding)\n","    \n","    - LSTM (bidirectional)\n","\n","    - Attention (multi-head dot product)\n","\n","    - ConvSequence (decoder)\n","  \n","    - Conv1d or Conv1d + ReLU + Conv1d (final)\n","\n","3. **[simpleconv.py](https://github.com/facebookresearch/brainmagick/blob/main/bm/models/simpleconv.py)**\n","\n","  - SimpleConv\n","\n","    - takes a sample of channels (subsampled_meg_channels)\n","    \n","    - ChannelDropout\n","\n","    - ChannelMerger\n","\n","    - Conv1d + activations (initial layer)\n","\n","    - SubjectLayers (subject layer)\n","\n","    - ta.transforms.Spectrogram (short-time fourier transform)\n","\n","    - ScaledEmbedding (subject embedding)\n","\n","    - ConvSequence (encoder)\n","\n","    - DualPathRNN\n","  \n","    - Conv1d or Conv1d + ReLU + Conv1d (final)\n","\n","4. **[features.py](https://github.com/facebookresearch/brainmagick/blob/main/bm/models/features.py)** - model to extract features\n","\n","  - DeepMel(ConvSequence)\n","\n","**Combined encoders:**\n","\n","1. **[deep_mel.yaml](https://github.com/facebookresearch/brainmagick/blob/main/bm/conf/feature_model/deep_mel.yaml)** - calls **features.py** - feature model\n","\n","2. **[convrnn.yaml](https://github.com/facebookresearch/brainmagick/blob/main/bm/conf/model/convrnn.yaml)** - calls **convrnn.py**\n","\n","**Combined decoders:**\n","\n","1. **[clip_conv.yaml](https://github.com/facebookresearch/brainmagick/blob/main/bm/conf/model/clip_conv.yaml/)** - calls **simpleconv.py** - default model\n","\n","2. **[decoder_convrnn.yaml](https://github.com/facebookresearch/brainmagick/blob/main/bm/conf/model/decoder_convrnn.yaml)** - calls **convrnn.py**\n","\n","\n","Other: All the imports https://github.com/facebookresearch/brainmagick/blob/main/requirements.txt"],"metadata":{"id":"MioCdwZACrZ1"}},{"cell_type":"markdown","source":["## toy data"],"metadata":{"id":"7v6qIWPExjji"}},{"cell_type":"code","source":["batch_size = 2\n","eeg_num_channels = 4\n","eeg_time_steps = 5\n","audio_time_steps = 6\n","\n","eeg_data = torch.randn(batch_size, eeg_time_steps, eeg_num_channels)\n","audio_data = torch.randn(batch_size, audio_time_steps)\n","vocabulary = [\"word1\", \"word2\", \"word3\", \"word4\", \"word5\"]\n","\n","print('eeg_data.shape', eeg_data.shape)\n","print('eeg_data\\n', eeg_data)\n","print('\\naudio_data.shape', audio_data.shape)\n","print('audio_data\\n', audio_data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"muriv1hLxmzT","executionInfo":{"status":"ok","timestamp":1702015947668,"user_tz":300,"elapsed":155,"user":{"displayName":"Kateryna Shapovalenko","userId":"16393911302329786226"}},"outputId":"e24ce561-493d-4bec-89d0-45d8dd8951c8"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["eeg_data.shape torch.Size([2, 5, 4])\n","eeg_data\n"," tensor([[[-1.1552,  0.5698, -0.0762, -0.2001],\n","         [-0.7471, -0.7212, -0.3877,  1.8738],\n","         [-0.6147,  1.2159,  0.3479,  1.5585],\n","         [-1.1880, -0.4776, -2.1453,  0.4058],\n","         [-2.0641,  0.4924,  1.5100,  0.7131]],\n","\n","        [[ 0.2494, -1.3765,  0.1368,  0.4436],\n","         [ 0.0899,  2.5809,  0.1485, -0.2769],\n","         [-0.7275,  0.1031, -1.8410,  0.4143],\n","         [ 0.5914,  0.3040,  0.0953,  1.6982],\n","         [-0.9059,  0.0356, -0.3355, -1.1054]]])\n","\n","audio_data.shape torch.Size([2, 6])\n","audio_data\n"," tensor([[-0.8494, -0.0652,  0.0875, -0.6615,  1.1431,  1.2393],\n","        [-1.0426, -1.0902,  0.7864, -0.8260, -0.1956,  1.2193]])\n"]}]},{"cell_type":"code","source":["\"\"\"\n","eeg_num_channels = 2\n","eeg_time_steps = 3\n","audio_time_steps = 4\n","\n","eeg_data_subject_0_segment_0 = torch.randn(eeg_time_steps, eeg_num_channels)\n","eeg_data_subject_0_segment_1 = torch.randn(eeg_time_steps, eeg_num_channels)\n","audio_data_subject_0_segment_0 = torch.randn(audio_time_steps)\n","audio_data_subject_0_segment_1 = torch.randn(audio_time_steps)\n","\n","eeg_data_subject_1_segment_0 = torch.randn(eeg_time_steps, eeg_num_channels)\n","audio_data_subject_1_segment_0 = torch.randn(audio_time_steps)\n","\n","data = {\n","    (0, 0): (eeg_data_subject_0_segment_0, audio_data_subject_0_segment_0),\n","    (0, 1): (eeg_data_subject_0_segment_1, audio_data_subject_0_segment_0),\n","\n","    (1, 0): (eeg_data_subject_1_segment_0, audio_data_subject_1_segment_0)}\n","\n","\n","for key, (eeg, audio) in data.items():\n","    print(f\"Subject ID: {key[0]}, Segment ID: {key[1]}\")\n","    print(\"EEG Data Shape:\", eeg.shape)\n","    print(eeg)\n","    print(\"Audio Data Shape:\", audio.shape)\n","    print(audio)\n","    print()\n","\n","batch_size = 2\n","data = {}\n","for subject_id in range(batch_size):\n","    for segment_id in range(batch_size):\n","        eeg_data = torch.randn(eeg_time_steps, eeg_num_channels)\n","        audio_data = torch.randn(audio_time_steps)\n","        data[(subject_id, segment_id)] = (eeg_data, audio_data)\n","data\n","\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":139},"id":"wXSMKi9IW-qg","executionInfo":{"status":"ok","timestamp":1702015949269,"user_tz":300,"elapsed":205,"user":{"displayName":"Kateryna Shapovalenko","userId":"16393911302329786226"}},"outputId":"d44c72b9-5b29-4de4-be70-9b8e4859b18c"},"execution_count":50,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\neeg_num_channels = 2\\neeg_time_steps = 3\\naudio_time_steps = 4\\n\\neeg_data_subject_0_segment_0 = torch.randn(eeg_time_steps, eeg_num_channels)\\neeg_data_subject_0_segment_1 = torch.randn(eeg_time_steps, eeg_num_channels)\\naudio_data_subject_0_segment_0 = torch.randn(audio_time_steps)\\naudio_data_subject_0_segment_1 = torch.randn(audio_time_steps)\\n\\neeg_data_subject_1_segment_0 = torch.randn(eeg_time_steps, eeg_num_channels)\\naudio_data_subject_1_segment_0 = torch.randn(audio_time_steps)\\n\\ndata = {\\n    (0, 0): (eeg_data_subject_0_segment_0, audio_data_subject_0_segment_0),\\n    (0, 1): (eeg_data_subject_0_segment_1, audio_data_subject_0_segment_0), \\n\\n    (1, 0): (eeg_data_subject_1_segment_0, audio_data_subject_1_segment_0)}\\n\\n\\nfor key, (eeg, audio) in data.items():\\n    print(f\"Subject ID: {key[0]}, Segment ID: {key[1]}\")\\n    print(\"EEG Data Shape:\", eeg.shape)\\n    print(eeg)\\n    print(\"Audio Data Shape:\", audio.shape)\\n    print(audio)\\n    print()\\n\\nbatch_size = 2\\ndata = {}\\nfor subject_id in range(batch_size):\\n    for segment_id in range(batch_size):\\n        eeg_data = torch.randn(eeg_time_steps, eeg_num_channels)\\n        audio_data = torch.randn(audio_time_steps)\\n        data[(subject_id, segment_id)] = (eeg_data, audio_data)\\ndata\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":50}]},{"cell_type":"markdown","source":["## brain encoder 1"],"metadata":{"id":"07Ezn5GK4KIq"}},{"cell_type":"code","source":["class SubjectLayers(nn.Module): # learn how different are the subjects and based on that normalize the EEG\n","    def __init__(self, in_channels, out_channels, n_subjects):\n","        super().__init__()\n","        self.weights = nn.Parameter(torch.randn(n_subjects, in_channels, out_channels)) # initialize weights for each subject\n","        self.weights.data *= 1 / in_channels**0.5 # normalize weights\n","\n","    def forward(self, x, n_subjects):\n","        _, C, D = self.weights.shape\n","        subject_weights = self.weights.gather(0, n_subjects.view(-1, 1, 1).expand(-1, C, D)) # select the appropriate weights for each subject in the batch\n","        transformed_eeg = torch.einsum(\"bct,bcd->bdt\", x, subject_weights) # apply the subject-specific transformations\n","\n","        return transformed_eeg\n","\n","class ScaledEmbedding(nn.Module): # assign a unique vector to each subject (similar to positional embedding)\n","\n","    def __init__(self, n_subjects, embedding_dim, scale):\n","\n","        super().__init__()\n","        self.embedding = nn.Embedding(n_subjects, embedding_dim)\n","        self.embedding.weight.data /= scale\n","        self.scale = scale\n","\n","    def forward(self, x):\n","        scaled_embedding = self.embedding(x) * self.scale\n","        return scaled_embedding\n","\n","class LayerScale(nn.Module):\n","    def __init__(self, channels, init = 0.1, boost = 5.):\n","        super().__init__()\n","        self.scale = nn.Parameter(torch.zeros(channels, requires_grad=True))\n","        self.scale.data[:] = init / boost\n","        self.boost = boost\n","\n","    def forward(self, x):\n","        return (self.boost * self.scale[:, None]) * x\n","\n","class ConvSequence(nn.Module):\n","\n","    def __init__(self,\n","                 channels = [16, 32, 64, 128],\n","                 kernel = 4, dilation_growth = 1, dilation_period = None,\n","                 stride = 2, dropout = 0.0, leakiness = 0.0,\n","                 groups = 1, decode = False, batch_norm = False,\n","                 dropout_input = 0, skip = False, scale = None,\n","                 rewrite = False, activation_on_last = True,\n","                 post_skip = False, glu = 0, glu_context = 0,\n","                 glu_glu = True, activation = None):\n","\n","        super().__init__()\n","        dilation = 1\n","        channels = tuple(channels)\n","        self.skip = skip\n","        self.sequence = nn.ModuleList()\n","        self.glus = nn.ModuleList()\n","        if activation is None:\n","            activation = partial(nn.LeakyReLU, leakiness)\n","        Conv = nn.Conv1d if not decode else nn.ConvTranspose1d\n","        # build layers\n","        for k, (chin, chout) in enumerate(zip(channels[:-1], channels[1:])):\n","            layers: tp.List[nn.Module] = []\n","            is_last = k == len(channels) - 2\n","\n","            # Set dropout for the input of the conv sequence if defined\n","            if k == 0 and dropout_input:\n","                assert 0 < dropout_input < 1\n","                layers.append(nn.Dropout(dropout_input))\n","\n","            # conv layer\n","            if dilation_growth > 1:\n","                assert kernel % 2 != 0 # supports only odd kernel with dilation\n","            if dilation_period and (k % dilation_period) == 0:\n","                dilation = 1\n","            pad = kernel // 2 * dilation\n","            layers.append(Conv(chin, chout, kernel, stride, pad,\n","                               dilation=dilation, groups=groups if k > 0 else 1))\n","            dilation *= dilation_growth\n","            # non-linearity\n","            if activation_on_last or not is_last:\n","                if batch_norm:\n","                    layers.append(nn.BatchNorm1d(num_features=chout))\n","                layers.append(activation())\n","                if dropout:\n","                    layers.append(nn.Dropout(dropout))\n","                if rewrite:\n","                    layers += [nn.Conv1d(chout, chout, 1), nn.LeakyReLU(leakiness)]\n","            if chin == chout and skip:\n","                if scale is not None:\n","                    layers.append(LayerScale(chout, scale))\n","                if post_skip:\n","                    layers.append(Conv(chout, chout, 1, groups=chout, bias=False))\n","\n","            self.sequence.append(nn.Sequential(*layers))\n","            if glu and (k + 1) % glu == 0:\n","                ch = 2 * chout if glu_glu else chout\n","                act = nn.GLU(dim=1) if glu_glu else activation()\n","                self.glus.append(\n","                    nn.Sequential(\n","                        nn.Conv1d(chout, ch, 1 + 2 * glu_context, padding=glu_context), act))\n","            else:\n","                self.glus.append(None)\n","\n","    def forward(self, x):\n","        for module_idx, module in enumerate(self.sequence):\n","            old_x = x\n","            x = module(x)\n","            if self.skip and x.shape == old_x.shape:\n","                x = x + old_x\n","            glu = self.glus[module_idx]\n","            if glu is not None:\n","                x = glu(x)\n","        return x\n","\n","class Attention(nn.Module): # scaled dot product with relative position encoding and local attention\n","    def __init__(self, channels, radius = 50, heads = 4):\n","        super().__init__()\n","        assert channels % heads == 0\n","        self.content = nn.Conv1d(channels, channels, 1)\n","        self.query = nn.Conv1d(channels, channels, 1)\n","        self.key = nn.Conv1d(channels, channels, 1)\n","        self.embedding = nn.Embedding(radius * 2 + 1, channels // heads)\n","        weight = self.embedding.weight.data\n","        weight[:] = weight.cumsum(0) / torch.arange(1, len(weight) + 1).float().view(-1, 1).sqrt()\n","        self.heads = heads\n","        self.radius = radius\n","        self.bn = nn.BatchNorm1d(channels)\n","        self.fc = nn.Conv1d(channels, channels, 1)\n","        self.scale = nn.Parameter(torch.full([channels], 0.1))\n","\n","    def forward(self, x):\n","\n","        def _split(y):\n","            return y.view(y.shape[0], self.heads, -1, y.shape[2])\n","\n","        content = _split(self.content(x))\n","        query = _split(self.query(x))\n","        key = _split(self.key(x))\n","\n","        batch_size, _, dim, length = content.shape\n","\n","        dots = torch.einsum(\"bhct,bhcs->bhts\", query, key) # first index `t` is query, second index `s` is key.\n","\n","        steps = torch.arange(length, device=x.device)\n","        relative = (steps[:, None] - steps[None, :])\n","        embs = self.embedding.weight.gather(0, self.radius + relative.clamp_(-self.radius, self.radius).view(-1, 1).expand(-1, dim))\n","        embs = embs.view(length, length, -1)\n","        dots += 0.3 * torch.einsum(\"bhct,tsc->bhts\", query, embs)\n","        dots = torch.where(\n","            relative.abs() <= self.radius, dots, torch.tensor(-float('inf')).to(embs))\n","\n","        weights = torch.softmax(dots, dim=-1)\n","        out = torch.einsum(\"bhts,bhcs->bhct\", weights, content)\n","        out += 0.3 * torch.einsum(\"bhts,tsc->bhct\", weights, embs)\n","        out = out.reshape(batch_size, -1, length)\n","        out = F.relu(self.bn(self.fc(out))) * self.scale.view(1, -1, 1)\n","        return out\n","\n","\"\"\"\n","# TEST 1\n","def test_subject_layers():\n","    batch_size = 2\n","    in_channels = 3\n","    out_channels = 2\n","    n_subjects = 1\n","    time_steps = 2\n","    eeg_data = torch.randn(batch_size, in_channels, time_steps)\n","    print(eeg_data)\n","    subjects = torch.randint(0, n_subjects, (batch_size,))\n","    print(subjects)\n","    subject_layers = SubjectLayers(in_channels, out_channels, n_subjects)\n","    output = subject_layers(eeg_data, subjects)\n","    expected_shape = (batch_size, out_channels, time_steps)\n","    assert output.shape == expected_shape, f\"Output shape mismatch: expected {expected_shape}, got {output.shape}\"\n","test_subject_layers()\n","\n","# TEST 2\n","def scaled_embedding():\n","  n_subjects = 10\n","  embedding_dim = 5\n","  scale = 10\n","  scaled_embedding = ScaledEmbedding(n_subjects, embedding_dim, scale)\n","  subject_indices = torch.tensor([1, 2])\n","  embeddings = scaled_embedding(subject_indices)\n","  print(embeddings)\n","scaled_embedding()\n","\n","# TEST 3\n","conv_sequence = ConvSequence(channels=[16, 32, 64, 128])\n","input_tensor = torch.randn(1, 16, 50) # (Batch Size, Channels, Length)\n","output = conv_sequence(input_tensor)\n","output.shape\n","#summary(conv_sequence, input_size=(16, 50))\n","\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":139},"id":"2xvcGOY133DY","executionInfo":{"status":"ok","timestamp":1702067439382,"user_tz":300,"elapsed":110,"user":{"displayName":"Kateryna Shapovalenko","userId":"16393911302329786226"}},"outputId":"ea5d75a9-dbe1-4a5b-c9af-ccece92fd670"},"execution_count":73,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n# TEST 1\\ndef test_subject_layers():\\n    batch_size = 2\\n    in_channels = 3\\n    out_channels = 2\\n    n_subjects = 1\\n    time_steps = 2\\n    eeg_data = torch.randn(batch_size, in_channels, time_steps) \\n    print(eeg_data)\\n    subjects = torch.randint(0, n_subjects, (batch_size,))\\n    print(subjects)\\n    subject_layers = SubjectLayers(in_channels, out_channels, n_subjects)\\n    output = subject_layers(eeg_data, subjects)\\n    expected_shape = (batch_size, out_channels, time_steps)\\n    assert output.shape == expected_shape, f\"Output shape mismatch: expected {expected_shape}, got {output.shape}\"\\ntest_subject_layers()\\n\\n# TEST 2\\ndef scaled_embedding():\\n  n_subjects = 10\\n  embedding_dim = 5    \\n  scale = 10      \\n  scaled_embedding = ScaledEmbedding(n_subjects, embedding_dim, scale)\\n  subject_indices = torch.tensor([1, 2]) \\n  embeddings = scaled_embedding(subject_indices)\\n  print(embeddings)\\nscaled_embedding()\\n\\n# TEST 3\\nconv_sequence = ConvSequence(channels=[16, 32, 64, 128])\\ninput_tensor = torch.randn(1, 16, 50) # (Batch Size, Channels, Length)\\noutput = conv_sequence(input_tensor)\\noutput.shape\\n#summary(conv_sequence, input_size=(16, 50))\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":73}]},{"cell_type":"code","source":["class EEG_Encoder(nn.Module):\n","\n","    def __init__(self,\n","\n","                 in_channels = 64, n_subjects = 30,\n","\n","                 # subject layers\n","                 out_channels = 128,\n","\n","                 # conv\n","                 conv_chanels = [16, 32, 64, 128],\n","                 kernel = 4, stride = 2, conv_dropout = 0,\n","                 batch_norm = False, dropout_input = 0,\n","                 leakiness = 0,\n","\n","                 # lstm\n","                 hidden_size = 128, lstm_layers = 4, lstm_dropout = 0.1,\n","\n","                 # attention\n","                 attention_heads = 4, subject_dim = 64, embedding_scale = 1.0):\n","\n","        super().__init__()\n","\n","        # subject layers (normalization across subjects)\n","        self.subject_layers = SubjectLayers(in_channels, out_channels, n_subjects)\n","\n","        # scaled embedding (optional)\n","        # self.subject_embedding = ScaledEmbedding(n_subjects, subject_dim, embedding_scale)\n","\n","        # convsequence\n","        self.convs = ConvSequence(channels=conv_chanels, kernel = kernel_size,\n","                                  stride = stride, dropout = conv_dropout,\n","                                  batch_norm = batch_norm, leakiness = leakiness,\n","                                  dropout_input = dropout_input)\n","\n","        # lstm\n","        self.lstm = nn.LSTM(input_size    = out_channels,\n","                            hidden_size   = hidden_size//2,\n","                            num_layers    = lstm_layers,\n","                            dropout       = lstm_dropout,\n","                            bidirectional = True,\n","                            batch_first    = True)\n","\n","        # attention\n","        self.attention = Attention(hidden_size, heads=attention_heads)\n","\n","        # final linear layer\n","        self.finalconv1 = nn.Conv1d(hidden_size, out_channels, kernel_size = 1)\n","\n","    def forward(self, eeg_inputs): # to pass as an additional paramater n_subjects=10\n","\n","        # subject layers\n","        # subject_indices = torch.arange(n_subjects).to(eeg_inputs.device)\n","        # normalized_eeg_inputs = self.subject_layers(eeg_inputs, subject_indices)\n","\n","        # scaled embedding // scaled_embedding = self.subject_embedding(subject_indices)\n","\n","        # convsequence\n","        print('eeg_inputs before convs', eeg_inputs.shape)\n","        out = self.convs(eeg_inputs)\n","        print('eeg_inputs after convs', out.shape)\n","        print(\"After convs:\", torch.isnan(out).any())\n","\n","        # lstm\n","        out = out.permute(2, 0, 1)\n","        out, _ = self.lstm(out)\n","        out = out.permute(1, 2, 0)\n","        print(\"After LSTM:\", torch.isnan(out).any())\n","\n","        # attention\n","        out = out + self.attention(out)\n","        print(\"After Attention:\", torch.isnan(out).any())\n","\n","        # decoder\n","        # out = self.decoder(out)\n","\n","        # final\n","        out = self.finalconv1(out)\n","        print(\"Final Output:\", torch.isnan(out).any())\n","\n","        return out\n","\n","# TEST\n","in_channels = 64\n","n_subjects = 30\n","out_channels = 128\n","conv_channels = [64, 32, 64, 128]\n","kernel_size = 4\n","stride = 2\n","conv_dropout = 0\n","batch_norm = False\n","dropout_input = 0\n","leakiness = 0\n","hidden_size = 128\n","lstm_layers = 4\n","lstm_dropout = 0.1\n","attention_heads = 4\n","subject_dim = 64\n","embedding_scale = 1.0\n","\n","eeg_encoder = EEG_Encoder(in_channels, n_subjects, out_channels, conv_channels, kernel_size, stride,\n","                          conv_dropout, batch_norm, dropout_input, leakiness, hidden_size, lstm_layers,\n","                          lstm_dropout, attention_heads, subject_dim, embedding_scale)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","eeg_encoder.to(device)\n","batch_size = 2\n","in_channels = 64\n","sequence_length = 128\n","x_sample = torch.rand(batch_size, in_channels, sequence_length)\n","summary(eeg_encoder, x_sample.to(device))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"1Saet-MZ1qKW","executionInfo":{"status":"ok","timestamp":1702067441217,"user_tz":300,"elapsed":251,"user":{"displayName":"Kateryna Shapovalenko","userId":"16393911302329786226"}},"outputId":"73430680-9d8e-49ab-c211-1c18cff3cc90"},"execution_count":74,"outputs":[{"output_type":"stream","name":"stdout","text":["eeg_inputs before convs torch.Size([2, 64, 128])\n","eeg_inputs after convs torch.Size([2, 128, 17])\n","After convs: tensor(False)\n","After LSTM: tensor(False)\n","After Attention: tensor(False)\n","Final Output: tensor(False)\n","================================================================================\n","                                 Kernel Shape  Output Shape    Params  \\\n","Layer                                                                   \n","0_convs.sequence.0.Conv1d_0       [64, 32, 4]   [2, 32, 65]    8.224k   \n","1_convs.sequence.0.LeakyReLU_1              -   [2, 32, 65]         -   \n","2_convs.sequence.1.Conv1d_0       [32, 64, 4]   [2, 64, 33]    8.256k   \n","3_convs.sequence.1.LeakyReLU_1              -   [2, 64, 33]         -   \n","4_convs.sequence.2.Conv1d_0      [64, 128, 4]  [2, 128, 17]   32.896k   \n","5_convs.sequence.2.LeakyReLU_1              -  [2, 128, 17]         -   \n","6_lstm                                      -  [17, 2, 128]  397.312k   \n","7_attention.Conv1d_content      [128, 128, 1]  [2, 128, 17]   16.512k   \n","8_attention.Conv1d_query        [128, 128, 1]  [2, 128, 17]   16.512k   \n","9_attention.Conv1d_key          [128, 128, 1]  [2, 128, 17]   16.512k   \n","10_attention.Conv1d_fc          [128, 128, 1]  [2, 128, 17]   16.512k   \n","11_attention.BatchNorm1d_bn             [128]  [2, 128, 17]     256.0   \n","12_finalconv1                   [128, 128, 1]  [2, 128, 17]   16.512k   \n","\n","                               Mult-Adds  \n","Layer                                     \n","0_convs.sequence.0.Conv1d_0      532.48k  \n","1_convs.sequence.0.LeakyReLU_1         -  \n","2_convs.sequence.1.Conv1d_0     270.336k  \n","3_convs.sequence.1.LeakyReLU_1         -  \n","4_convs.sequence.2.Conv1d_0     557.056k  \n","5_convs.sequence.2.LeakyReLU_1         -  \n","6_lstm                          393.216k  \n","7_attention.Conv1d_content      278.528k  \n","8_attention.Conv1d_query        278.528k  \n","9_attention.Conv1d_key          278.528k  \n","10_attention.Conv1d_fc          278.528k  \n","11_attention.BatchNorm1d_bn        128.0  \n","12_finalconv1                   278.528k  \n","--------------------------------------------------------------------------------\n","                         Totals\n","Total params           529.504k\n","Trainable params       529.504k\n","Non-trainable params        0.0\n","Mult-Adds             3.145856M\n","================================================================================\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchsummaryX/torchsummaryX.py:101: FutureWarning: The default value of numeric_only in DataFrame.sum is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n","  df_sum = df.sum()\n"]},{"output_type":"execute_result","data":{"text/plain":["                                 Kernel Shape  Output Shape    Params  \\\n","Layer                                                                   \n","0_convs.sequence.0.Conv1d_0       [64, 32, 4]   [2, 32, 65]    8224.0   \n","1_convs.sequence.0.LeakyReLU_1              -   [2, 32, 65]       NaN   \n","2_convs.sequence.1.Conv1d_0       [32, 64, 4]   [2, 64, 33]    8256.0   \n","3_convs.sequence.1.LeakyReLU_1              -   [2, 64, 33]       NaN   \n","4_convs.sequence.2.Conv1d_0      [64, 128, 4]  [2, 128, 17]   32896.0   \n","5_convs.sequence.2.LeakyReLU_1              -  [2, 128, 17]       NaN   \n","6_lstm                                      -  [17, 2, 128]  397312.0   \n","7_attention.Conv1d_content      [128, 128, 1]  [2, 128, 17]   16512.0   \n","8_attention.Conv1d_query        [128, 128, 1]  [2, 128, 17]   16512.0   \n","9_attention.Conv1d_key          [128, 128, 1]  [2, 128, 17]   16512.0   \n","10_attention.Conv1d_fc          [128, 128, 1]  [2, 128, 17]   16512.0   \n","11_attention.BatchNorm1d_bn             [128]  [2, 128, 17]     256.0   \n","12_finalconv1                   [128, 128, 1]  [2, 128, 17]   16512.0   \n","\n","                                Mult-Adds  \n","Layer                                      \n","0_convs.sequence.0.Conv1d_0      532480.0  \n","1_convs.sequence.0.LeakyReLU_1        NaN  \n","2_convs.sequence.1.Conv1d_0      270336.0  \n","3_convs.sequence.1.LeakyReLU_1        NaN  \n","4_convs.sequence.2.Conv1d_0      557056.0  \n","5_convs.sequence.2.LeakyReLU_1        NaN  \n","6_lstm                           393216.0  \n","7_attention.Conv1d_content       278528.0  \n","8_attention.Conv1d_query         278528.0  \n","9_attention.Conv1d_key           278528.0  \n","10_attention.Conv1d_fc           278528.0  \n","11_attention.BatchNorm1d_bn         128.0  \n","12_finalconv1                    278528.0  "],"text/html":["\n","  <div id=\"df-ee881caf-4e35-4964-bc90-9dff920db6bb\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Kernel Shape</th>\n","      <th>Output Shape</th>\n","      <th>Params</th>\n","      <th>Mult-Adds</th>\n","    </tr>\n","    <tr>\n","      <th>Layer</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0_convs.sequence.0.Conv1d_0</th>\n","      <td>[64, 32, 4]</td>\n","      <td>[2, 32, 65]</td>\n","      <td>8224.0</td>\n","      <td>532480.0</td>\n","    </tr>\n","    <tr>\n","      <th>1_convs.sequence.0.LeakyReLU_1</th>\n","      <td>-</td>\n","      <td>[2, 32, 65]</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2_convs.sequence.1.Conv1d_0</th>\n","      <td>[32, 64, 4]</td>\n","      <td>[2, 64, 33]</td>\n","      <td>8256.0</td>\n","      <td>270336.0</td>\n","    </tr>\n","    <tr>\n","      <th>3_convs.sequence.1.LeakyReLU_1</th>\n","      <td>-</td>\n","      <td>[2, 64, 33]</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4_convs.sequence.2.Conv1d_0</th>\n","      <td>[64, 128, 4]</td>\n","      <td>[2, 128, 17]</td>\n","      <td>32896.0</td>\n","      <td>557056.0</td>\n","    </tr>\n","    <tr>\n","      <th>5_convs.sequence.2.LeakyReLU_1</th>\n","      <td>-</td>\n","      <td>[2, 128, 17]</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>6_lstm</th>\n","      <td>-</td>\n","      <td>[17, 2, 128]</td>\n","      <td>397312.0</td>\n","      <td>393216.0</td>\n","    </tr>\n","    <tr>\n","      <th>7_attention.Conv1d_content</th>\n","      <td>[128, 128, 1]</td>\n","      <td>[2, 128, 17]</td>\n","      <td>16512.0</td>\n","      <td>278528.0</td>\n","    </tr>\n","    <tr>\n","      <th>8_attention.Conv1d_query</th>\n","      <td>[128, 128, 1]</td>\n","      <td>[2, 128, 17]</td>\n","      <td>16512.0</td>\n","      <td>278528.0</td>\n","    </tr>\n","    <tr>\n","      <th>9_attention.Conv1d_key</th>\n","      <td>[128, 128, 1]</td>\n","      <td>[2, 128, 17]</td>\n","      <td>16512.0</td>\n","      <td>278528.0</td>\n","    </tr>\n","    <tr>\n","      <th>10_attention.Conv1d_fc</th>\n","      <td>[128, 128, 1]</td>\n","      <td>[2, 128, 17]</td>\n","      <td>16512.0</td>\n","      <td>278528.0</td>\n","    </tr>\n","    <tr>\n","      <th>11_attention.BatchNorm1d_bn</th>\n","      <td>[128]</td>\n","      <td>[2, 128, 17]</td>\n","      <td>256.0</td>\n","      <td>128.0</td>\n","    </tr>\n","    <tr>\n","      <th>12_finalconv1</th>\n","      <td>[128, 128, 1]</td>\n","      <td>[2, 128, 17]</td>\n","      <td>16512.0</td>\n","      <td>278528.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ee881caf-4e35-4964-bc90-9dff920db6bb')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-ee881caf-4e35-4964-bc90-9dff920db6bb button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-ee881caf-4e35-4964-bc90-9dff920db6bb');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-5b819eb1-924b-4660-8020-962d87050f61\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5b819eb1-924b-4660-8020-962d87050f61')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-5b819eb1-924b-4660-8020-962d87050f61 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":74}]},{"cell_type":"markdown","source":["## brain encoder 2"],"metadata":{"id":"c0TqyTlnM7uJ"}},{"cell_type":"code","source":[],"metadata":{"id":"yAAXV3e-OA4E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## audio encoder"],"metadata":{"id":"D2i3EX7rh5z5"}},{"cell_type":"code","source":["# Speech model\n","\n","class LayerScale(nn.Module):\n","    def __init__(self, channels: int, init: float = 0.1, boost: float = 5.):\n","        super().__init__()\n","        self.scale = nn.Parameter(torch.zeros(channels, requires_grad=True))\n","        self.scale.data[:] = init / boost\n","        self.boost = boost\n","\n","    def forward(self, x):\n","        return (self.boost * self.scale[:, None]) * x\n","\n","class ConvSequence(nn.Module):\n","\n","    def __init__(self,\n","                 channels: tp.Sequence[int],\n","                 kernel: int = 4,\n","                 dilation_growth: int = 1,\n","                 dilation_period: tp.Optional[int] = None,\n","                 stride: int = 2,\n","                 dropout: float = 0.0,\n","                 leakiness: float = 0.0,\n","                 groups: int = 1,\n","                 decode: bool = False,\n","                 batch_norm: bool = False,\n","                 dropout_input: float = 0,\n","                 skip: bool = False,\n","                 scale: tp.Optional[float] = None,\n","                 rewrite: bool = False,\n","                 activation_on_last: bool = True,\n","                 post_skip: bool = False,\n","                 glu: int = 0,\n","                 glu_context: int = 0,\n","                 glu_glu: bool = True,\n","                 activation: tp.Any = None) -> None:\n","        super().__init__()\n","        dilation = 1\n","        channels = tuple(channels)\n","        self.skip = skip\n","        self.sequence = nn.ModuleList()\n","        self.glus = nn.ModuleList()\n","        if activation is None:\n","            activation = partial(nn.LeakyReLU, leakiness)\n","        Conv = nn.Conv1d if not decode else nn.ConvTranspose1d\n","        # build layers\n","        for k, (chin, chout) in enumerate(zip(channels[:-1], channels[1:])):\n","            layers: tp.List[nn.Module] = []\n","            is_last = k == len(channels) - 2\n","\n","            # Set dropout for the input of the conv sequence if defined\n","            if k == 0 and dropout_input:\n","                assert 0 < dropout_input < 1\n","                layers.append(nn.Dropout(dropout_input))\n","\n","            # conv layer\n","            if dilation_growth > 1:\n","                assert kernel % 2 != 0, \"Supports only odd kernel with dilation for now\"\n","            if dilation_period and (k % dilation_period) == 0:\n","                dilation = 1\n","            pad = kernel // 2 * dilation\n","            layers.append(Conv(chin, chout, kernel, stride, pad,\n","                               dilation=dilation, groups=groups if k > 0 else 1))\n","            dilation *= dilation_growth\n","            # non-linearity\n","            if activation_on_last or not is_last:\n","                if batch_norm:\n","                    layers.append(nn.BatchNorm1d(num_features=chout))\n","                layers.append(activation())\n","                if dropout:\n","                    layers.append(nn.Dropout(dropout))\n","                if rewrite:\n","                    layers += [nn.Conv1d(chout, chout, 1), nn.LeakyReLU(leakiness)]\n","                    # layers += [nn.Conv1d(chout, 2 * chout, 1), nn.GLU(dim=1)]\n","            if chin == chout and skip:\n","                if scale is not None:\n","                    layers.append(LayerScale(chout, scale))\n","                if post_skip:\n","                    layers.append(Conv(chout, chout, 1, groups=chout, bias=False))\n","\n","            self.sequence.append(nn.Sequential(*layers))\n","            if glu and (k + 1) % glu == 0:\n","                ch = 2 * chout if glu_glu else chout\n","                act = nn.GLU(dim=1) if glu_glu else activation()\n","                self.glus.append(\n","                    nn.Sequential(\n","                        nn.Conv1d(chout, ch, 1 + 2 * glu_context, padding=glu_context), act))\n","            else:\n","                self.glus.append(None)\n","\n","    def forward(self, x: tp.Any) -> tp.Any:\n","        for module_idx, module in enumerate(self.sequence):\n","            old_x = x\n","            x = module(x)\n","            if self.skip and x.shape == old_x.shape:\n","                x = x + old_x\n","            glu = self.glus[module_idx]\n","            if glu is not None:\n","                x = glu(x)\n","        return x\n","\n","class DeepMel(ConvSequence):\n","    \"\"\"DeepMel model that extracts features from the Mel spectrogram.\n","\n","    Parameters\n","    ----------\n","    n_in_channels :\n","        Number of input channels.\n","    n_hidden_channels :\n","        Number of channels in hidden layers.\n","    n_hidden_layers :\n","        Number of hidden layers.\n","    n_out_channels :\n","        Number of output channels.\n","    kwargs:\n","        Additional keyword arguments to pass to ConvSequence.\n","    \"\"\"\n","    def __init__(self,\n","                 n_in_channels: int,\n","                 n_hidden_channels: int,\n","                 n_hidden_layers: int,\n","                 n_out_channels: int, **kwargs):\n","\n","        channels = \\\n","            [n_in_channels] + [n_hidden_channels] * (n_hidden_layers - 1) + [n_out_channels]\n","\n","        super().__init__(channels, **kwargs)\n","\n","# Test the model\n","n_in_channels = 16\n","n_hidden_channels = 32\n","n_hidden_layers = 3\n","n_out_channels = 64\n","model = DeepMel(n_in_channels, n_hidden_channels, n_hidden_layers, n_out_channels)\n","input_size = (n_in_channels, 128)  # Example input size (channels, sequence length)\n","summary(model, input_size=input_size, device=\"cpu\")"],"metadata":{"id":"4p29Sypr4wjV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## clip loss"],"metadata":{"id":"MyunViVM5aSZ"}},{"cell_type":"code","source":["class ClipLoss(torch.nn.Module): # CLIP (See Open AI CLIP) constrastive loss\n","\n","    def __init__(self, linear=None, twin=True, pool=False, tmin=None, tmax=None,\n","                 tmin_train=None, tmax_train=None, dset_args=None, center=False):\n","        super().__init__()\n","        self.linear = None\n","        self.pool = pool\n","        self.center = center\n","        if linear is not None:\n","            self.linear_est = torch.nn.LazyLinear(linear)\n","            if twin:\n","                self.linear_gt = self.linear_est\n","            else:\n","                self.linear_gt = torch.nn.LazyLinear(linear)\n","        self.tmin = tmin\n","        self.tmax = tmax\n","        self.tmin_train = tmin_train\n","        self.tmax_train = tmax_train\n","        self.dset_args = dset_args\n","\n","    def trim_samples(self, estimates, candidates):\n","        \"\"\"Given estimates that is [B1, C, T] and candidates\n","        which is [B2, C, T], return estimates_trim of size [B1, C, T']\n","        and candidates_trim of size [B2, C, T'], such that T'\n","        corresponds to the samples between [self.tmin, self.tmax]\n","        \"\"\"\n","        if self.training and (self.tmin_train is not None or self.tmax_train is not None):\n","            tmin, tmax = self.tmin_train, self.tmax_train\n","        else:\n","            tmin, tmax = self.tmin, self.tmax\n","        if (tmin is not None) or (tmax is not None):\n","            assert self.dset_args is not None\n","            assert self.dset_args.tmin is not None\n","            dset_tmin = self.dset_args.tmin\n","        if tmin is None:\n","            trim_min = 0\n","        else:\n","            assert tmin >= dset_tmin, 'clip.tmin should be above dset.tmin'\n","            trim_min = int((-dset_tmin + tmin) * self.dset_args.sample_rate)\n","        if tmax is None:\n","            trim_max = estimates.shape[-1]\n","        else:\n","            trim_max = int((-dset_tmin + tmax) * self.dset_args.sample_rate)\n","        estimates_trim = estimates[..., trim_min:trim_max]\n","        candidates_trim = candidates[..., trim_min:trim_max]\n","        return estimates_trim, candidates_trim\n","\n","    def get_scores(self, estimates: torch.Tensor, candidates: torch.Tensor):\n","        \"\"\"Given estimates that is [B, C, T] and candidates\n","        which is [B', C, T], return a [B, B'] matrix of scores of matching.\n","        \"\"\"\n","        estimates, candidates = self.trim_samples(estimates, candidates)\n","        if self.linear:\n","            estimates = self.linear_est(estimates)\n","            candidates = self.linear_gt(candidates)\n","        if self.pool:\n","            estimates = estimates.mean(dim=2, keepdim=True)\n","            candidates = candidates.mean(dim=2, keepdim=True)\n","        if self.center:\n","            estimates = estimates - estimates.mean(dim=(1, 2), keepdim=True)\n","            candidates = candidates - candidates.mean(dim=(1, 2), keepdim=True)\n","        inv_norms = 1 / (1e-8 + candidates.norm(dim=(1, 2), p=2))\n","        scores = torch.einsum(\"bct,oct,o->bo\", estimates, candidates, inv_norms)\n","        return scores\n","\n","    def get_probabilities(self, estimates, candidates):\n","        \"\"\"Given estimates that is [B, C, T] and candidates\n","        which is [B', C, T], return a [B, B'] matrix of probabilities of matching.\n","        \"\"\"\n","        scores = self.get_scores(estimates, candidates)\n","        probabilities = F.softmax(scores, dim=1)\n","        return probabilities\n","\n","    def forward(self, estimate, candidate, mask=None):\n","        \"\"\"Warning: estimate and candidate are not symmetrical.\n","        If estimate of shape [B, C, T] and candidate of size [B', C, T]\n","        with B'>=B, the first B samples of candidate are targets, while\n","        the remaining B'-B samples of candidate are only used as negatives.\n","        \"\"\"\n","        assert estimate.size(0) <= candidate.size(0), \"need at least as many targets as estimates\"\n","        scores = self.get_scores(estimate, candidate)\n","        target = torch.arange(len(scores), device=estimate.device)\n","        loss = F.cross_entropy(scores, target)\n","        return loss\n","\n","# TEST\n","batch_size = 3\n","num_channels = 4\n","time_steps = 5\n","estimates = torch.randn(batch_size, num_channels, time_steps)\n","candidates = torch.randn(batch_size, num_channels, time_steps)\n","clip_loss = ClipLoss()\n","\n","probabilities = clip_loss.get_probabilities(estimates, candidates) # --> output in the model's forward function\n","loss = clip_loss(estimates, candidates) # --> use a loss\n","\n","print(probabilities)\n","print(loss)\n","\n","### Each row in the probability matrix corresponds to a set of EEG data,\n","#   and each column corresponds to a set of audio data.\n","#   The values in the matrix are probabilities that indicate\n","#   how likely it is that a given set of EEG data matches a given set of audio data."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702067443760,"user_tz":300,"elapsed":102,"user":{"displayName":"Kateryna Shapovalenko","userId":"16393911302329786226"}},"outputId":"7c9b27e2-2b9d-4734-b547-4d8c18b42085","id":"nD8j-vr-5aSk"},"execution_count":75,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.3387, 0.1995, 0.4617],\n","        [0.7710, 0.1033, 0.1257],\n","        [0.1687, 0.6365, 0.1948]])\n","tensor(1.6630)\n"]}]},{"cell_type":"code","source":["# estimates = egg[0], eeg[1], eeg[2]\n","# candidates = audio[0], audio[1], audio[2]\n","\n","# egg[0] vs audio[0] - positive pair\n","# egg[0] vs audio[1] - negative pair\n","# egg[0] vs audio[2] - negative pair\n","\n","# egg[0] vs audio[1] - positive pair\n","# egg[0] vs audio[0] - negative pair\n","# egg[0] vs audio[2] - negative pair"],"metadata":{"id":"tQY0aKzgNhnG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## brain-to-audio model"],"metadata":{"id":"C1UH6pcWh-0G"}},{"cell_type":"code","source":["class BrainAudioModel(torch.nn.Module):\n","\n","  def __init__(# brain encoding\n","               self, in_channels, model_chout\n","\n","               # audio encoding\n","\n","               # brain to audio probabilities\n","\n","               ):\n","\n","    super().__init__()\n","\n","    # brain encoding\n","    self.brain_encoding = EEG_Encoder(in_channels=in_channels,\n","                                      n_subjects=30,\n","                                      out_channels=model_chout,\n","                                      conv_chanels=[64, 32, 64, 128],\n","                                      kernel=4,\n","                                      stride=2,\n","                                      conv_dropout=0,\n","                                      batch_norm=False,\n","                                      dropout_input=0,\n","                                      leakiness=0,\n","                                      hidden_size=128,\n","                                      lstm_layers=4,\n","                                      lstm_dropout=0.1,\n","                                      attention_heads=4,\n","                                      subject_dim=64,\n","                                      embedding_scale=1.0)\n","\n","    # audio encoding\n","    #self.audio_encoding = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\") # https://huggingface.co/facebook/wav2vec2-base-960h\n","\n","    # brain to audio probabilities\n","    self.clip = ClipLoss(linear=None, twin=True, pool=False, tmin=None, tmax=None,\n","                         tmin_train=None, tmax_train=None, dset_args=None, center=False)\n","\n","    # linear layers to match the dimensions\n","    self.linear1 = nn.Linear(128, model_chout)\n","\n","  \"\"\"\n","  def freeze_audio_embedding(self): # we need to call it after initializing the model model.freeze_audio_embedding()\n","    for param in self.audio_embedding.parameters():\n","      param.requires_grad = False\n","  \"\"\"\n","\n","  def forward(self, raw_eeg_inputs, audio_hidden_states):  # raw_audio_inputs\n","\n","    # brain encoding\n","    brain_encoder_outputs = self.brain_encoding(raw_eeg_inputs)\n","    print('brain_encoder_outputs', brain_encoder_outputs.shape)\n","\n","    print('audio_hidden_states', audio_hidden_states.shape)\n","\n","    # audio encoding (optional, could be passed as a parameter to forward)\n","    #audio_encoder_outputs = self.audio_encoding(raw_audio_inputs).last_hidden_state\n","    #audio_encoder_outputs = self.linear(audio_encoder_outputs)\n","    audio_hidden_states_processed = self.linear1(audio_hidden_states)\n","    print('audio_hidden_states_processed', audio_hidden_states_processed.shape)\n","\n","    # brain to audio probabilities\n","    brain_encoder_outputs_reshaped = brain_encoder_outputs.permute(0, 2, 1) # reshaping to [B, C, T]\n","    audio_hidden_states_reshaped = audio_hidden_states_processed.permute(0, 2, 1) # reshaping to [B, C, T]\n","    probabilities = self.clip.get_probabilities(brain_encoder_outputs_reshaped, audio_hidden_states_reshaped)\n","    print('probabilities', probabilities)\n","    return probabilities\n","\n","# TEST\n","in_channels = 64  # EEG input channels\n","model_chout = 17  # output channels\n","model = BrainAudioModel(in_channels, model_chout)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","eeg_input_size = (2, in_channels, 128)  # Batch size, channels, sequence length\n","audio_hidden_state_size = (2, model_chout, 128)  # Assuming the same sequence length and batch size\n","eeg_dummy_input = torch.randn(eeg_input_size).to(device)\n","audio_hidden_state_dummy = torch.randn(audio_hidden_state_size).to(device)\n","summary(model, eeg_dummy_input, audio_hidden_state_dummy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"_mcZ_-G6pcmb","executionInfo":{"status":"ok","timestamp":1702067513553,"user_tz":300,"elapsed":286,"user":{"displayName":"Kateryna Shapovalenko","userId":"16393911302329786226"}},"outputId":"bab64d71-d31f-4818-8af3-203df2965033"},"execution_count":77,"outputs":[{"output_type":"stream","name":"stdout","text":["eeg_inputs before convs torch.Size([2, 64, 128])\n","eeg_inputs after convs torch.Size([2, 128, 17])\n","After convs: tensor(False)\n","After LSTM: tensor(True)\n","After Attention: tensor(True)\n","Final Output: tensor(True)\n","brain_encoder_outputs torch.Size([2, 17, 17])\n","audio_hidden_states torch.Size([2, 17, 128])\n","audio_hidden_states_processed torch.Size([2, 17, 17])\n","probabilities tensor([[nan, nan],\n","        [nan, nan]])\n","===============================================================================================\n","                                                Kernel Shape  Output Shape  \\\n","Layer                                                                        \n","0_brain_encoding.convs.sequence.0.Conv1d_0       [64, 32, 4]   [2, 32, 65]   \n","1_brain_encoding.convs.sequence.0.LeakyReLU_1              -   [2, 32, 65]   \n","2_brain_encoding.convs.sequence.1.Conv1d_0       [32, 64, 4]   [2, 64, 33]   \n","3_brain_encoding.convs.sequence.1.LeakyReLU_1              -   [2, 64, 33]   \n","4_brain_encoding.convs.sequence.2.Conv1d_0      [64, 128, 4]  [2, 128, 17]   \n","5_brain_encoding.convs.sequence.2.LeakyReLU_1              -  [2, 128, 17]   \n","6_brain_encoding.LSTM_lstm                                 -  [17, 2, 128]   \n","7_brain_encoding.attention.Conv1d_content      [128, 128, 1]  [2, 128, 17]   \n","8_brain_encoding.attention.Conv1d_query        [128, 128, 1]  [2, 128, 17]   \n","9_brain_encoding.attention.Conv1d_key          [128, 128, 1]  [2, 128, 17]   \n","10_brain_encoding.attention.Conv1d_fc          [128, 128, 1]  [2, 128, 17]   \n","11_brain_encoding.attention.BatchNorm1d_bn             [128]  [2, 128, 17]   \n","12_brain_encoding.Conv1d_finalconv1             [128, 17, 1]   [2, 17, 17]   \n","13_linear1                                         [128, 17]   [2, 17, 17]   \n","\n","                                                Params Mult-Adds  \n","Layer                                                             \n","0_brain_encoding.convs.sequence.0.Conv1d_0      8.224k   532.48k  \n","1_brain_encoding.convs.sequence.0.LeakyReLU_1        -         -  \n","2_brain_encoding.convs.sequence.1.Conv1d_0      8.256k  270.336k  \n","3_brain_encoding.convs.sequence.1.LeakyReLU_1        -         -  \n","4_brain_encoding.convs.sequence.2.Conv1d_0     32.896k  557.056k  \n","5_brain_encoding.convs.sequence.2.LeakyReLU_1        -         -  \n","6_brain_encoding.LSTM_lstm                     340.48k  336.384k  \n","7_brain_encoding.attention.Conv1d_content      16.512k  278.528k  \n","8_brain_encoding.attention.Conv1d_query        16.512k  278.528k  \n","9_brain_encoding.attention.Conv1d_key          16.512k  278.528k  \n","10_brain_encoding.attention.Conv1d_fc          16.512k  278.528k  \n","11_brain_encoding.attention.BatchNorm1d_bn       256.0     128.0  \n","12_brain_encoding.Conv1d_finalconv1             2.193k   36.992k  \n","13_linear1                                      2.193k    2.176k  \n","-----------------------------------------------------------------------------------------------\n","                         Totals\n","Total params           460.546k\n","Trainable params       460.546k\n","Non-trainable params        0.0\n","Mult-Adds             2.849664M\n","===============================================================================================\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchsummaryX/torchsummaryX.py:101: FutureWarning: The default value of numeric_only in DataFrame.sum is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n","  df_sum = df.sum()\n"]},{"output_type":"execute_result","data":{"text/plain":["                                                Kernel Shape  Output Shape  \\\n","Layer                                                                        \n","0_brain_encoding.convs.sequence.0.Conv1d_0       [64, 32, 4]   [2, 32, 65]   \n","1_brain_encoding.convs.sequence.0.LeakyReLU_1              -   [2, 32, 65]   \n","2_brain_encoding.convs.sequence.1.Conv1d_0       [32, 64, 4]   [2, 64, 33]   \n","3_brain_encoding.convs.sequence.1.LeakyReLU_1              -   [2, 64, 33]   \n","4_brain_encoding.convs.sequence.2.Conv1d_0      [64, 128, 4]  [2, 128, 17]   \n","5_brain_encoding.convs.sequence.2.LeakyReLU_1              -  [2, 128, 17]   \n","6_brain_encoding.LSTM_lstm                                 -  [17, 2, 128]   \n","7_brain_encoding.attention.Conv1d_content      [128, 128, 1]  [2, 128, 17]   \n","8_brain_encoding.attention.Conv1d_query        [128, 128, 1]  [2, 128, 17]   \n","9_brain_encoding.attention.Conv1d_key          [128, 128, 1]  [2, 128, 17]   \n","10_brain_encoding.attention.Conv1d_fc          [128, 128, 1]  [2, 128, 17]   \n","11_brain_encoding.attention.BatchNorm1d_bn             [128]  [2, 128, 17]   \n","12_brain_encoding.Conv1d_finalconv1             [128, 17, 1]   [2, 17, 17]   \n","13_linear1                                         [128, 17]   [2, 17, 17]   \n","\n","                                                 Params  Mult-Adds  \n","Layer                                                               \n","0_brain_encoding.convs.sequence.0.Conv1d_0       8224.0   532480.0  \n","1_brain_encoding.convs.sequence.0.LeakyReLU_1       NaN        NaN  \n","2_brain_encoding.convs.sequence.1.Conv1d_0       8256.0   270336.0  \n","3_brain_encoding.convs.sequence.1.LeakyReLU_1       NaN        NaN  \n","4_brain_encoding.convs.sequence.2.Conv1d_0      32896.0   557056.0  \n","5_brain_encoding.convs.sequence.2.LeakyReLU_1       NaN        NaN  \n","6_brain_encoding.LSTM_lstm                     340480.0   336384.0  \n","7_brain_encoding.attention.Conv1d_content       16512.0   278528.0  \n","8_brain_encoding.attention.Conv1d_query         16512.0   278528.0  \n","9_brain_encoding.attention.Conv1d_key           16512.0   278528.0  \n","10_brain_encoding.attention.Conv1d_fc           16512.0   278528.0  \n","11_brain_encoding.attention.BatchNorm1d_bn        256.0      128.0  \n","12_brain_encoding.Conv1d_finalconv1              2193.0    36992.0  \n","13_linear1                                       2193.0     2176.0  "],"text/html":["\n","  <div id=\"df-a0484a86-5d3d-40dd-956b-5f21810ef836\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Kernel Shape</th>\n","      <th>Output Shape</th>\n","      <th>Params</th>\n","      <th>Mult-Adds</th>\n","    </tr>\n","    <tr>\n","      <th>Layer</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0_brain_encoding.convs.sequence.0.Conv1d_0</th>\n","      <td>[64, 32, 4]</td>\n","      <td>[2, 32, 65]</td>\n","      <td>8224.0</td>\n","      <td>532480.0</td>\n","    </tr>\n","    <tr>\n","      <th>1_brain_encoding.convs.sequence.0.LeakyReLU_1</th>\n","      <td>-</td>\n","      <td>[2, 32, 65]</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2_brain_encoding.convs.sequence.1.Conv1d_0</th>\n","      <td>[32, 64, 4]</td>\n","      <td>[2, 64, 33]</td>\n","      <td>8256.0</td>\n","      <td>270336.0</td>\n","    </tr>\n","    <tr>\n","      <th>3_brain_encoding.convs.sequence.1.LeakyReLU_1</th>\n","      <td>-</td>\n","      <td>[2, 64, 33]</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4_brain_encoding.convs.sequence.2.Conv1d_0</th>\n","      <td>[64, 128, 4]</td>\n","      <td>[2, 128, 17]</td>\n","      <td>32896.0</td>\n","      <td>557056.0</td>\n","    </tr>\n","    <tr>\n","      <th>5_brain_encoding.convs.sequence.2.LeakyReLU_1</th>\n","      <td>-</td>\n","      <td>[2, 128, 17]</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>6_brain_encoding.LSTM_lstm</th>\n","      <td>-</td>\n","      <td>[17, 2, 128]</td>\n","      <td>340480.0</td>\n","      <td>336384.0</td>\n","    </tr>\n","    <tr>\n","      <th>7_brain_encoding.attention.Conv1d_content</th>\n","      <td>[128, 128, 1]</td>\n","      <td>[2, 128, 17]</td>\n","      <td>16512.0</td>\n","      <td>278528.0</td>\n","    </tr>\n","    <tr>\n","      <th>8_brain_encoding.attention.Conv1d_query</th>\n","      <td>[128, 128, 1]</td>\n","      <td>[2, 128, 17]</td>\n","      <td>16512.0</td>\n","      <td>278528.0</td>\n","    </tr>\n","    <tr>\n","      <th>9_brain_encoding.attention.Conv1d_key</th>\n","      <td>[128, 128, 1]</td>\n","      <td>[2, 128, 17]</td>\n","      <td>16512.0</td>\n","      <td>278528.0</td>\n","    </tr>\n","    <tr>\n","      <th>10_brain_encoding.attention.Conv1d_fc</th>\n","      <td>[128, 128, 1]</td>\n","      <td>[2, 128, 17]</td>\n","      <td>16512.0</td>\n","      <td>278528.0</td>\n","    </tr>\n","    <tr>\n","      <th>11_brain_encoding.attention.BatchNorm1d_bn</th>\n","      <td>[128]</td>\n","      <td>[2, 128, 17]</td>\n","      <td>256.0</td>\n","      <td>128.0</td>\n","    </tr>\n","    <tr>\n","      <th>12_brain_encoding.Conv1d_finalconv1</th>\n","      <td>[128, 17, 1]</td>\n","      <td>[2, 17, 17]</td>\n","      <td>2193.0</td>\n","      <td>36992.0</td>\n","    </tr>\n","    <tr>\n","      <th>13_linear1</th>\n","      <td>[128, 17]</td>\n","      <td>[2, 17, 17]</td>\n","      <td>2193.0</td>\n","      <td>2176.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a0484a86-5d3d-40dd-956b-5f21810ef836')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-a0484a86-5d3d-40dd-956b-5f21810ef836 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-a0484a86-5d3d-40dd-956b-5f21810ef836');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-df67106a-ff31-4cf0-8ad2-2a87d80b0c3d\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-df67106a-ff31-4cf0-8ad2-2a87d80b0c3d')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-df67106a-ff31-4cf0-8ad2-2a87d80b0c3d button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":77}]},{"cell_type":"markdown","source":["# losses and metrics"],"metadata":{"id":"N4TkGFvZjt2D"}},{"cell_type":"code","source":[],"metadata":{"id":"aGMPNPaSrHAI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# trainer\n"],"metadata":{"id":"82KWAuB6jPQo"}},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"8ea986fc372643389d1ab4c445659e9d","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution":{"iopub.execute_input":"2022-08-10T14:02:13.440820Z","iopub.status.busy":"2022-08-10T14:02:13.440281Z","iopub.status.idle":"2022-08-10T14:02:13.644455Z","shell.execute_reply":"2022-08-10T14:02:13.642614Z","shell.execute_reply.started":"2022-08-10T14:02:13.440752Z"},"id":"kIvZOIfjSwRK","source_hash":"451a140f","tags":[]},"outputs":[],"source":["class Trainer:\n","    def __init__(self, model, loader, optimizer, criterion, scheduler, max_epochs= 1, run_id= 'exp'):\n","\n","        self.model      = model\n","        self.loader     = loader\n","        self.optimizer  = optimizer\n","        self.criterion  = criterion\n","        self.scheduler = scheduler\n","\n","        self.train_losses           = []\n","        self.val_losses             = []\n","        self.prediction_probs       = []\n","        self.prediction_probs_test  = []\n","        self.generated_texts_test   = []\n","        self.epochs                 = 0\n","        self.max_epochs             = max_epochs\n","        self.run_id                 = run_id\n","\n","\n","    def calculate_loss(self, out, target):\n","        # output: (B, T, Vocab_size) - probability distributions\n","        # target: (B, T)\n","        # Read the documentation of CrossEntropyLoss and try to understand how it takes inputs\n","\n","        # Tip: If your target is of shape (B, T) it means that you have B batches with T words.\n","        # Tip: What is the total number of words in this batch?\n","        # Tip: Crossentropy calculates the loss between a label and its probability distribution.\n","\n","        out     = out.view(-1, out.size(-1)) # TODO\n","        targets = target.view(-1)  # TODO\n","        loss    = self.criterion(out, targets)\n","\n","        return loss\n","\n","\n","    def train(self):\n","\n","        self.model.train() # set to training mode\n","        self.model.to(DEVICE)\n","        epoch_loss  = 0\n","        num_batches = 0\n","\n","        for batch_num, (inputs, targets) in enumerate(tqdm(self.loader)):\n","\n","            # TODO: Complete the loop. You should be able to complete this without any helper comments after 3 HWs\n","            # Tip: Mixed precision training\n","            # For loss calculation, use the calculate_loss function. You need to complete it before using.\n","\n","            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n","\n","            self.optimizer.zero_grad()\n","\n","            output, _ = self.model(inputs)\n","            loss = self.calculate_loss(output, targets)\n","            loss.backward()\n","            self.optimizer.step()\n","\n","            loss_val = loss.item()\n","            epoch_loss += loss_val\n","            num_batches += 1\n","\n","        #epoch_loss = epoch_loss / (batch_num + 1)\n","        epoch_loss = epoch_loss / num_batches\n","        self.epochs += 1\n","        print('[TRAIN] \\tEpoch [%d/%d] \\tLoss: %.4f \\tLr: %.6f'\n","                      % (self.epochs, self.max_epochs, epoch_loss, self.optimizer.param_groups[0]['lr']))\n","        self.train_losses.append(epoch_loss)\n","\n","\n","    def test(self): # Don't change this function\n","\n","        self.model.eval() # set to eval mode\n","        prediction_probs     = self.model.predict(fixtures_pred['inp']).detach().cpu().numpy() # get predictions\n","        self.prediction_probs.append(prediction_probs)\n","\n","        generated_indexes_test   = self.model.generate(fixtures_gen_test, 10).detach().cpu().numpy() # generated predictions for 10 words\n","\n","        nll                   = get_prediction_nll(prediction_probs, fixtures_pred['out'])\n","        generated_texts_test  = make_generation_text(fixtures_gen_test, generated_indexes_test, VOCAB)\n","        self.val_losses.append(nll)\n","\n","        self.generated_texts_test.append(generated_texts_test)\n","\n","        # generate predictions for test data\n","        prediction_probs_test = self.model.predict(fixtures_pred_test['inp']).detach().cpu().numpy() # get predictions\n","        self.prediction_probs_test.append(prediction_probs_test)\n","\n","        print('[VAL] \\tEpoch [%d/%d] \\tLoss: %.4f'\n","                      % (self.epochs, self.max_epochs, nll))\n","        return nll\n","\n","\n","    def save(self): # Don't change this function\n","\n","        model_path = os.path.join('hw4/experiments', self.run_id, 'model-{}.pkl'.format(self.epochs))\n","        torch.save({'state_dict': self.model.state_dict()}, model_path)\n","        np.save(os.path.join('hw4/experiments', self.run_id, 'prediction-probs-{}.npy'.format(self.epochs)), self.prediction_probs[-1])\n","        np.save(os.path.join('hw4/experiments', self.run_id, 'prediction-probs-test-{}.npy'.format(self.epochs)), self.prediction_probs_test[-1])\n","\n","        with open(os.path.join('hw4/experiments', self.run_id, 'generated-texts-{}-test.txt'.format(self.epochs)), 'w') as fw:\n","            fw.write(self.generated_texts_test[-1])"]},{"cell_type":"markdown","source":["# experiments"],"metadata":{"id":"N4UE5_2vjUtQ"}},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"7fc44ee4771a42f996d0a00d35529fb6","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution":{"iopub.execute_input":"2022-08-10T14:02:13.852171Z","iopub.status.busy":"2022-08-10T14:02:13.850633Z","iopub.status.idle":"2022-08-10T14:02:13.927227Z","shell.execute_reply":"2022-08-10T14:02:13.924500Z","shell.execute_reply.started":"2022-08-10T14:02:13.852093Z"},"id":"TiUrjbEjSwRQ","source_hash":"f7524436","tags":[]},"outputs":[],"source":["configs = dict(\n","\n","    # model\n","    embedding_dim=600,\n","    hidden_dim=600,\n","    embedding_dropout=0.3,\n","    locked_dropout=0.5,\n","\n","    # loader\n","    batch_size = 128,\n","    sequence_length = 30,\n","\n","    # optimizer\n","    init_lr = 0.002,\n","    weight_decay = 5e-3,\n","\n","    # scheduler\n","    factor = 0.9,\n","    patience = 1,\n","\n","    # trainer\n","    num_epochs = 40)"]},{"cell_type":"code","execution_count":null,"metadata":{"cell_id":"4aaccf1c32fa480a9a15e8bb8bc4d9e4","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution":{"iopub.execute_input":"2022-08-10T14:02:14.110787Z","iopub.status.busy":"2022-08-10T14:02:14.109778Z","iopub.status.idle":"2022-08-10T14:02:14.929087Z","shell.execute_reply":"2022-08-10T14:02:14.925078Z","shell.execute_reply.started":"2022-08-10T14:02:14.110707Z"},"id":"DbHH6zXTSwRa","source_hash":"2acff566","tags":[]},"outputs":[],"source":["model       = LanguageModel(vocab_size=len(VOCAB), embedding_dim=configs['embedding_dim'], hidden_dim=configs['hidden_dim'],\n","                            embedding_dropout=configs['embedding_dropout'], locked_dropout=configs['locked_dropout'])\n","\n","loader      = DataLoaderForLanguageModeling(dataset, batch_size=configs['batch_size'], shuffle=True, drop_last=True, sequence_length=configs['sequence_length'])\n","inputs, targets = next(iter(loader))\n","\n","criterion   = torch.nn.CrossEntropyLoss()\n","\n","optimizer   = AdamW(model.parameters(), lr=configs['init_lr'], betas=(0.9, 0.999), eps=1e-8, weight_decay=configs['weight_decay'], amsgrad=False)\n","\n","scheduler   = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=configs['factor'], patience=configs['patience'], verbose=True)\n","\n","torchsummaryX.summary(model, inputs) #print(model)"]},{"cell_type":"code","execution_count":89,"metadata":{"cell_id":"aaff53cf948e44b7b9bd49cbcad0ac58","deepnote_cell_type":"code","deepnote_to_be_reexecuted":true,"execution":{"iopub.execute_input":"2022-08-10T14:02:13.931258Z","iopub.status.busy":"2022-08-10T14:02:13.930204Z","iopub.status.idle":"2022-08-10T14:02:14.107883Z","shell.execute_reply":"2022-08-10T14:02:14.105987Z","shell.execute_reply.started":"2022-08-10T14:02:13.931185Z"},"id":"2HCVG5YISwRW","source_hash":"c9f4594a","tags":[],"colab":{"base_uri":"https://localhost:8080/","height":245},"executionInfo":{"status":"error","timestamp":1701994394255,"user_tz":300,"elapsed":344,"user":{"displayName":"Kateryna Shapovalenko","userId":"16393911302329786226"}},"outputId":"653c72cf-fd33-4977-9d6e-30a64165988a"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-89-28a6de246b82>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m trainer = Trainer(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m       \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mloader\u001b[0m      \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moptimizer\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'Trainer' is not defined"]}],"source":["trainer = Trainer(\n","    model       = model,\n","    loader      = loader,\n","\n","    optimizer   = optimizer,\n","    criterion   = criterion,\n","    scheduler   = scheduler,\n","\n","    max_epochs  = configs['num_epochs'],\n","    run_id      = run_id)"]},{"cell_type":"markdown","source":["# evaluation"],"metadata":{"id":"dv73E9hkjYnz"}},{"cell_type":"markdown","source":["## viz"],"metadata":{"id":"EXoEy94Dj3kt"}},{"cell_type":"code","source":[],"metadata":{"id":"q_cyKW1Sj0ol"},"execution_count":null,"outputs":[]}]}