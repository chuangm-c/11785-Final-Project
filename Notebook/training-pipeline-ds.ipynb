{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyP4gA4nLNhHhhJjV+qZtMDC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"xHkY3CpQfM7Y"},"outputs":[],"source":["!pip install wandb torchsummaryX mne transformers -q"]},{"cell_type":"code","source":["### Torch\n","import  torch\n","import  torch.nn as nn\n","import  torch.nn.functional as F\n","from    torch.optim import lr_scheduler\n","from    torchsummaryX import summary\n","from    torch.utils.data import Dataset, DataLoader\n","import  torchaudio\n","import  torchaudio.transforms as tat\n","\n","### General\n","import  random\n","import  numpy as np\n","import  pandas as pd\n","import  scipy\n","import  gc\n","from    tqdm.auto import tqdm\n","import  os\n","import  datetime\n","import  time\n","import  wandb\n","import  matplotlib.pyplot as plt\n","import  seaborn as sns\n","\n","# wav2vec2 and EEG processing\n","from    transformers import (AutoProcessor, AutoModelForPreTraining,\n","                             CLIPProcessor, CLIPModel)\n","import  mne"],"metadata":{"id":"L80P0Fy3gSnQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# data [QA]"],"metadata":{"id":"YlPfcrpLgvxH"}},{"cell_type":"markdown","source":["## studies/brennan.py"],"metadata":{"id":"uyNbGpi1hS5H"}},{"cell_type":"code","source":["def _read_meta(fname):\n","    proc = loadmat(\n","        fname,\n","        squeeze_me=True,\n","        chars_as_strings=True,\n","        struct_as_record=True,\n","        simplify_cells=True,\n","    )[\"proc\"]\n","\n","    # ref = proc[\"implicitref\"]\n","    # ref_channels = proc[\"refchannels\"]\n","\n","    # subject_id = proc[\"subject\"]\n","    meta = proc[\"trl\"]\n","\n","    # TODO artefacts, ica, rejected components etc\n","    assert len(meta) == proc[\"tot_trials\"]\n","    assert proc[\"tot_chans\"] == 61\n","    bads = list(proc[\"impedence\"][\"bads\"])\n","    bads += list(proc[\"rejections\"][\"badchans\"])\n","\n","    columns = list(proc[\"varnames\"])\n","    if len(columns) != meta.shape[1]:\n","        columns = [\"start_sample\", \"stop_sample\", \"offset\"] + columns\n","        assert len(columns) == meta.shape[1]\n","    meta = pd.DataFrame(meta, columns=[\"_\" + i for i in columns])\n","    assert len(meta) == 2129  # FIXME retrieve subjects who have less trials?\n","\n","    # Add Brennan's annotations\n","    paths = get_paths()\n","    story = pd.read_csv(paths.download / \"AliceChapterOne-EEG.csv\")\n","    events = meta.join(story)\n","\n","    events[\"kind\"] = \"word\"\n","    events[\"condition\"] = \"sentence\"\n","    events[\"duration\"] = events.offset - events.onset\n","    columns = dict(Word=\"word\", Position=\"word_id\", Sentence=\"sequence_id\")\n","    events = events.rename(columns=columns)\n","    events[\"start\"] = events[\"_start_sample\"] / SFREQ\n","\n","    # add audio events\n","    wav_file = (\n","        paths.download / \"audio\" / \"DownTheRabbitHoleFinal_SoundFile%i.wav\"\n","    )\n","    sounds = []\n","    for segment, d in events.groupby(\"Segment\"):\n","        # Some wav files start BEFORE the onset of eeg recording...\n","        start = d.iloc[0].start - d.iloc[0].onset\n","        sound = dict(\n","            kind=\"sound\", start=start, filepath=str(wav_file) % segment\n","        )\n","        sounds.append(sound)\n","    events = pd.concat([events, pd.DataFrame(sounds)], ignore_index=True)\n","    events = events.sort_values(\"start\").reset_index()\n","\n","    # clean up\n","    keep = [\n","        \"start\",\n","        \"duration\",\n","        \"kind\",\n","        \"word\",\n","        \"word_id\",\n","        \"sequence_id\",\n","        \"condition\",\n","        \"filepath\",\n","    ]\n","    events = events[keep]\n","    events[['language', 'modality']] = 'english', 'audio'\n","    events = extract_sequence_info(events)\n","    events = events.event.create_blocks(groupby='sentence')\n","    events = events.event.validate()\n","\n","    return events\n","\n","\n","def _read_eeg(fname):\n","    fname = Path(fname)\n","    assert fname.exists()\n","    assert str(fname).endswith(\".mat\")\n","    mat = loadmat(\n","        fname,\n","        squeeze_me=True,\n","        chars_as_strings=True,\n","        struct_as_record=True,\n","        simplify_cells=True,\n","    )\n","    mat = mat[\"raw\"]\n","\n","    # sampling frequency\n","    sfreq = mat[\"hdr\"][\"Fs\"]\n","    assert sfreq == 500.0\n","    assert mat[\"fsample\"] == sfreq\n","\n","    # channels\n","    n_chans = mat[\"hdr\"][\"nChans\"]\n","    n_samples = mat[\"hdr\"][\"nSamples\"]\n","    ch_names = list(mat[\"hdr\"][\"label\"])\n","    assert len(ch_names) == n_chans\n","\n","    # vertical EOG\n","    assert ch_names[60] == \"VEOG\"\n","\n","    # audio channel\n","    add_audio_chan = False\n","    if len(ch_names) == 61:\n","        ch_names += [\"AUD\"]\n","        add_audio_chan = True\n","    assert ch_names[61] in (\"AUD\", \"Aux5\")\n","\n","    # check name\n","    for i, ch in enumerate(ch_names[:-2]):\n","        assert ch == str(i + 1 + (i >= 28))\n","\n","    # channel type\n","    assert set(mat[\"hdr\"][\"chantype\"]) == set([\"eeg\"])\n","    ch_types = [\"eeg\"] * 60 + [\"eog\", \"misc\"]\n","    assert set(mat[\"hdr\"][\"chanunit\"]) == set([\"uV\"])\n","\n","    # create MNE info\n","    info = mne.create_info(ch_names, sfreq, ch_types, verbose=False)\n","    subject_id = fname.name.split(\".mat\")[0]\n","    info[\"subject_info\"] = dict(his_id=subject_id, id=int(subject_id[1:]))\n","\n","    # time\n","    diff = np.diff(mat[\"time\"]) - 1 / sfreq\n","    tol = 1e-5\n","    assert np.all(diff < tol)\n","    assert np.all(diff > -tol)\n","    start, stop = mat[\"sampleinfo\"]\n","    assert start == 1\n","    assert stop == n_samples\n","    assert mat[\"hdr\"][\"nSamplesPre\"] == 0\n","    assert mat[\"hdr\"][\"nTrials\"] == 1\n","\n","    # data\n","    data = mat[\"trial\"]\n","    assert data.shape[0] == n_chans\n","    assert data.shape[1] == n_samples\n","    if add_audio_chan:\n","        data = np.vstack((data, np.zeros_like(data[0])))\n","\n","    # create mne objects\n","    info = mne.create_info(ch_names, sfreq, ch_types, verbose=False)\n","    raw = mne.io.RawArray(data * 1e-6, info, verbose=False)\n","    montage = mne.channels.make_standard_montage(\"easycap-M10\")\n","    raw.set_montage(montage)\n","\n","    assert raw.info[\"sfreq\"] == SFREQ\n","    assert len(raw.ch_names) == 62\n","\n","    return raw\n","\n","\n","class Brennan2019Recording(api.Recording):\n","\n","    data_url = \"https://deepblue.lib.umich.edu/data/concern/data_sets/\"\n","    data_url += \"bg257f92t\"\n","    paper_url = \"https://journals.plos.org/plosone/\"\n","    paper_url += \"article?id=10.1371/journal.pone.0207741\"\n","    doi = \"https://doi.org/10.1371/journal.pone.0207741\"\n","    licence = \"CC BY 4.0\"\n","    modality = \"audio\"\n","    language = \"english\"\n","    device = \"eeg\"\n","    description = \"\"\"EEG of Alice in WonderLand, By Brennan and Hale 2019.\n","    The eeg data was bandpassed between 0.1 and 200. Hz\"\"\"\n","\n","    @classmethod\n","    def iter(cls) -> tp.Iterator[\"Brennan2019Recording\"]:  # type: ignore\n","        \"\"\"Returns a generator of all recordings\"\"\"\n","        # download, extract, organize\n","        paths = get_paths()\n","        _prepare()\n","\n","        subjects = [\n","            f.name\n","            for f in (paths.download / \"proc\").iterdir()\n","            if (f.name.startswith(\"S\") and f.name.endswith(\".mat\"))\n","        ]\n","        assert len(subjects) == 42\n","        # remove bad subject s24 (metadata does not have enough trials)\n","        # FIXME retrieve these subjects?\n","        bads = [\n","            \"S24.mat\",\n","            \"S26.mat\",\n","            \"S27.mat\",\n","            \"S30.mat\",\n","            \"S32.mat\",\n","            \"S34.mat\",\n","            \"S35.mat\",\n","            \"S36.mat\",\n","        ]\n","        bads += [\"S02.mat\"]  # bad proc.trl?\n","        subjects = [s.split(\".\")[0] for s in subjects if s not in bads]\n","\n","        for subject in subjects:\n","            recording = cls(subject_uid=str(subject))\n","            yield recording\n","\n","    def __init__(self, subject_uid: str) -> None:\n","        super().__init__(subject_uid=subject_uid, recording_uid=subject_uid)\n","\n","    def _load_raw(self) -> mne.io.RawArray:\n","        paths = get_paths()\n","        raw = _read_eeg(paths.download / f\"{self.subject_uid}.mat\")\n","        return raw\n","\n","    def _load_events(self) -> pd.DataFrame:\n","        file = get_paths().download / \"proc\" / f\"{self.subject_uid}.mat\"\n","        events = _read_meta(file)\n","        return events"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":256},"id":"ZtOriEeZgSwd","executionInfo":{"status":"error","timestamp":1701805599372,"user_tz":300,"elapsed":82,"user":{"displayName":"Quentin Auster","userId":"15884522498219677551"}},"outputId":"0fe2188b-9c17-4719-aa25-8c48da521e0c"},"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-dc8dd24974d0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mBrennan2019Recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRecording\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdata_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://deepblue.lib.umich.edu/data/concern/data_sets/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdata_url\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"bg257f92t\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpaper_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"https://journals.plos.org/plosone/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'api' is not defined"]}]},{"cell_type":"markdown","source":["## features/audio.py"],"metadata":{"id":"gdEGTRICkido"}},{"cell_type":"code","source":["class MelSpectrum(base.Feature, CaptureInit):\n","    \"\"\"Outputs the sound waves with the features frequency\n","    \"\"\"\n","    event_kind = \"sound\"\n","\n","    def __init__(self, sample_rate: Frequency, n_mels=40, n_fft=512, in_sampling=16_000,\n","                 normalized=True, use_log_scale=True, log_scale_eps=1e-5,\n","                 norm_audio: bool = True) -> None:\n","        super().__init__(sample_rate)\n","        self.dimension = n_mels\n","        kwargs = self._init_kwargs\n","        kwargs.pop('sample_rate')\n","        self.cache = Cache(self.__class__.__name__, kwargs)\n","\n","        self.in_sampling = in_sampling\n","        self.n_mels = n_mels\n","        self.n_fft = n_fft\n","        self.hop_length = n_fft // 4\n","        self.use_log_scale = use_log_scale\n","        self.log_scale_eps = log_scale_eps\n","        self.normalized = normalized\n","        self.norm_audio = norm_audio\n","        self.trans = torchaudio.transforms.MelSpectrogram(\n","            sample_rate=self.in_sampling, n_mels=self.n_mels,\n","            n_fft=n_fft, hop_length=self.hop_length, normalized=normalized\n","        )\n","\n","        if use_log_scale:\n","            self.default_value = math.log10(log_scale_eps)\n","\n","    def _compute(self, filepath: Path, start: float, stop: float) -> torch.Tensor:\n","        wav, sr = _extract_wav_part(filepath, start, stop)\n","        wav = torch.mean(wav, dim=0)  # stereo to mono\n","        if self.norm_audio:\n","            wav = (wav - wav.mean()) / (1e-8 + wav.std())\n","        wav = julius.resample.ResampleFrac(old_sr=int(sr), new_sr=self.in_sampling)(wav)\n","\n","        # Two UserWarnings thrown internally by torch here: \"stft will require the return_complex\n","        # parameter be explicitly\" and \"The function torch.rfft is deprecated\". Remove this once\n","        # torch library updates to fix this\n","        with warnings.catch_warnings():\n","            warnings.simplefilter(\"ignore\")\n","            melspec = self.trans(wav)\n","        if self.use_log_scale:\n","            melspec = torch.log10(melspec + self.log_scale_eps)\n","        return melspec\n","\n","    def get(self, event: events.Sound) -> torch.Tensor:\n","        melspec = self.cache.get(\n","            self._compute, filepath=event.filepath,\n","            start=event.offset, stop=event.offset + event.duration)\n","        feature_samples = self.sample_rate.to_ind(event.stop - event.start)\n","        return F.interpolate(melspec[None], feature_samples)[0]\n","\n","\n","class Pitch(base.Feature, CaptureInit):\n","    \"\"\"Pitch from the waveform.\n","    \"\"\"\n","\n","    event_kind = \"sound\"\n","\n","    def __init__(self, sample_rate: Frequency, min_f0=100.0, max_f0=350.0, harmonic_thresh=0.1,\n","                 frame_length_in_samples=256, frame_space_in_samples=64) -> None:\n","        super().__init__(sample_rate)\n","        kwargs = self._init_kwargs\n","        kwargs.pop('sample_rate')\n","        self.cache = Cache(self.__class__.__name__, kwargs)\n","\n","        self.frame_length_in_samples = frame_length_in_samples\n","        self.frame_space_in_samples = frame_space_in_samples\n","        self.harmonic_thresh = harmonic_thresh\n","        self.min_f0 = min_f0\n","        self.max_f0 = max_f0\n","        self.in_sampling = 16_000\n","\n","    @property\n","    def _cache_params(self):\n","        return self._init_args_kwargs\n","\n","    def _compute(self, filepath: Path, start: float, stop: float) -> torch.Tensor:\n","        wav_stereo, sr = _extract_wav_part(filepath, start, stop)\n","        wav = torch.mean(wav_stereo, axis=0)  # Stereo to mono\n","        wav = julius.resample.ResampleFrac(old_sr=int(sr), new_sr=self.in_sampling)(wav)\n","\n","        pitches, harmonic_rates, argmins, times = compute_yin(\n","            sig=wav.numpy(),\n","            sr=self.in_sampling,\n","            w_len=self.frame_length_in_samples,\n","            w_step=self.frame_space_in_samples,\n","            harmo_thresh=self.harmonic_thresh,\n","            f0_min=self.min_f0,\n","            f0_max=self.max_f0)\n","        out = torch.FloatTensor(pitches)\n","        return out\n","\n","    def get(self, event: events.Sound) -> torch.Tensor:\n","        pitches = self.cache.get(\n","            self._compute, filepath=event.filepath,\n","            start=event.offset, stop=event.offset + event.duration)\n","        feature_samples = self.sample_rate.to_ind(event.stop - event.start)\n","        out = F.interpolate(pitches[None, None], feature_samples)[0, 0]\n","        return out[None]\n","\n","\n","class _BaseWav2Vec(base.Feature, CaptureInit):\n","    \"\"\"\n","    Parent class for Wav2VecTr and Wav2VecConv\n","    \"\"\"\n","\n","    event_kind = \"sound\"\n","    model_name = \"facebook/wav2vec2-large-xlsr-53\"\n","\n","    def __init__(self, sample_rate: Frequency,\n","                 normalized: bool = True, random: bool = False,\n","                 device: str = \"cpu\") -> None:\n","        super().__init__(sample_rate)\n","        args: tp.Any = self.model_name\n","        if random:\n","            args = (self.model_name, random)\n","        self.cache = Cache(\"Wav2VecEmbedding\", args, mode=\"memmap\")\n","        self.normalized = normalized\n","        self.device = device\n","        self.random = random\n","        # Huggingface logging\n","        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","        os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"critical\"\n","        self._model_cache = MemoryCache(\"Wav2VecEmbedding\", \"model\")\n","        self._extractor_cache = MemoryCache(\"Wav2VecEmbedding\", \"extractor\")\n","\n","    @property\n","    def model(self) -> tp.Any:\n","        from transformers import Wav2Vec2Model\n","        if self.random:\n","            return self._model_cache.get(self._get_random_model)\n","        else:\n","            return self._model_cache.get(Wav2Vec2Model.from_pretrained, self.model_name)\n","\n","    def _get_random_model(self):\n","        from transformers import Wav2Vec2Model, Wav2Vec2Config\n","        config = Wav2Vec2Config.from_pretrained(self.model_name)\n","        return Wav2Vec2Model(config)\n","\n","    @property\n","    def feature_extractor(self) -> tp.Any:\n","        from transformers import Wav2Vec2FeatureExtractor\n","        return self._extractor_cache.get(Wav2Vec2FeatureExtractor.from_pretrained, self.model_name)\n","\n","    def _preprocess_wav(self, filepath: Union[Path, str],\n","                        start: float, stop: float) -> torch.Tensor:\n","        wav, sr = _extract_wav_part(filepath, start, stop)\n","        logger.debug(\n","            \"Preprocessing Wav on %s, start %.1f, stop %.1f, duration %.1f\",\n","            filepath, start, stop, stop - start)\n","        wav = torch.mean(wav, dim=0)  # stereo to mono\n","        model_sr = self.feature_extractor.sampling_rate\n","        wav = julius.resample.ResampleFrac(old_sr=int(sr), new_sr=model_sr)(wav)\n","\n","        # [1, T]\n","        out = self.feature_extractor(wav,\n","                                     return_tensors=\"pt\",\n","                                     sampling_rate=model_sr,\n","                                     do_normalize=self.normalized).input_values\n","        return out\n","\n","    def _compute_hidden_states(\n","            self, name: str, filepath: Path, start: float, stop: float,\n","            layers: tp.Optional[tp.List[int]] = None) -> torch.Tensor:\n","        input_values = self._preprocess_wav(filepath=filepath, start=start, stop=stop)\n","\n","        self.model.to(self.device)\n","        self.model.eval()  # needs to be in eval mode\n","        with torch.no_grad():\n","            outputs = self.model(input_values.to(self.device), output_hidden_states=True)\n","        out: tp.Any = outputs.get(name)\n","        if isinstance(out, tuple):\n","            out = torch.stack(out)\n","        if layers is not None:\n","            out = out[layers].mean(0)\n","        return out.detach().cpu().clone().numpy()\n","\n","    def _get_cached_tensor(\n","            self, event: events.Sound, overlap: events.DataSlice, name: str,\n","            layers: tp.Optional[tp.List[int]] = None,\n","    ) -> torch.Tensor:\n","        outputs = self.cache.get(\n","            self._compute_hidden_states, start=event.offset, stop=event.offset + event.duration,\n","            filepath=event.filepath, name=name, layers=layers)\n","        embd_sr = outputs.shape[-2] / event.duration\n","        # safety, to make sure we extract the right dim... but maybe slow\n","        if event.duration >= 0.5:\n","            assert 42 < embd_sr < 52, (f\"Unexpected sampling rate for embedding {embd_sr}\",\n","                                       event.duration, outputs.shape[-2])\n","        # if the above assert fails, event duration may be inconsistent with actual wav duration\n","        # or the wav2vec output sampling rate has changed.\n","        # we'd need to either find a way to get the embedding sampling rate independently, or\n","        # figure out the duration in another way\n","        sr = Frequency(embd_sr)\n","        start, stop = [sr.to_ind(x - event.start) for x in (overlap.start, overlap.stop)]\n","        start = min(start, outputs.shape[-2] - 1)\n","        stop = max(start + 1, stop)\n","        chunk = outputs[..., start: stop, :]\n","        # load into memory (probably unnecessary, but lets avoid weird issues)\n","        chunk = np.array(chunk, copy=True)\n","        return torch.from_numpy(chunk)\n","\n","    def get(self, event: events.Sound) -> torch.Tensor:\n","        raise RuntimeError(f\"Only get_on_overlap is available for {self.__class__.__name__}\")\n","\n","\n","class Wav2VecTransformer(_BaseWav2Vec):\n","    \"\"\"Outputs the Wav2Vec transformer layers\n","    \"\"\"\n","    event_kind = \"sound\"\n","    dimension = 1024\n","\n","    def __init__(self, sample_rate: Frequency,\n","                 normalized: bool = True,\n","                 layers: tp.Tuple[int, ...] = (14, 15, 16, 17, 18),\n","                 random: bool = False,\n","                 device: str = \"cpu\") -> None:\n","        super().__init__(sample_rate=sample_rate, normalized=normalized,\n","                         device=device, random=random)\n","        self.layers = layers\n","\n","    def get_on_overlap(self, event: events.Sound, overlap: events.DataSlice) -> torch.Tensor:\n","        outputs = self._get_cached_tensor(\n","            event, overlap=overlap,\n","            name=\"hidden_states\", layers=list(self.layers))\n","        outputs = outputs[0].transpose(0, 1)  # [1, T, D] -> [T, D] -> [D, T]\n","        return F.interpolate(outputs[None], overlap.duration_ind)[0]\n","\n","\n","class Wav2VecConvolution(_BaseWav2Vec):\n","    \"\"\"Outputs the Wav2Vec convolutional layers\n","    \"\"\"\n","    event_kind = \"sound\"\n","    dimension = 512\n","\n","    def get_on_overlap(self, event: events.Sound, overlap: events.DataSlice) -> torch.Tensor:\n","        outputs = self._get_cached_tensor(event, overlap=overlap, name=\"extract_features\")\n","        # [1, T, D] -> [T, D] -> [D, T]\n","        outputs = outputs[0].transpose(0, 1)  # [1, T, D] -> [T, D] -> [D, T]\n","        out = F.interpolate(outputs[None], overlap.duration_ind)[0]\n","        return out\n","\n","\n","class Wav2VecChunk(_BaseWav2Vec):\n","    \"\"\"Outputs a chunk of the waveform compatible to be an input of Wav2Vec Model\"\"\"\n","\n","    event_kind = \"sound\"\n","    dimension = 1\n","    model_name = \"facebook/wav2vec2-large-xlsr-53\"\n","    normalizable = False\n","\n","    def __init__(self, sample_rate: Frequency,\n","                 normalized: bool = True,\n","                 random: bool = False,\n","                 device: str = \"cpu\") -> None:\n","        # Forcing the SR to 16k for this feature (base::FeaturesBuilder()\n","        # doesn't handle multiple SRs)\n","        super().__init__(sample_rate=Frequency(16000), normalized=normalized,\n","                         device=device, random=random)\n","\n","    @property\n","    def feature_extractor(self) -> tp.Any:\n","        from transformers import Wav2Vec2FeatureExtractor\n","\n","        return self._extractor_cache.get(\n","            Wav2Vec2FeatureExtractor.from_pretrained, self.model_name\n","        )\n","\n","    def get(self, event: events.Sound) -> torch.Tensor:\n","        # Possible improv.: add cache here to read full .wav once (small time reduction expected)\n","        wav = self._preprocess_wav(\n","            filepath=event.filepath,\n","            start=event.offset,\n","            stop=event.offset + event.duration,\n","        )\n","        return wav\n","\n","\n","def _extract_wav_part(\n","    filepath: Union[Path, str], onset: float, offset: float\n",") -> tp.Tuple[torch.Tensor, Frequency]:\n","    \"\"\"Extract a chunk of a wave file based on onset and offset in seconds\n","    \"\"\"\n","    info = torchaudio.info(str(filepath))\n","    sr = Frequency(info.sample_rate)\n","    wav = torchaudio.load(\n","        filepath, frame_offset=sr.to_ind(onset), num_frames=sr.to_ind(offset - onset))[0]\n","    delta = abs(wav.shape[-1] / sr - offset + onset)\n","    assert delta <= 0.1, (delta, filepath, onset, offset, onset - offset)\n","    return wav, sr"],"metadata":{"id":"CM8lj6z2ksDA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## scaling"],"metadata":{"id":"6GOPeoktqWwK"}},{"cell_type":"code","source":[],"metadata":{"id":"LnzXhyepqnn2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## train / test split"],"metadata":{"id":"kbmOuIlpqoek"}},{"cell_type":"code","source":[],"metadata":{"id":"ht0RIbmVqmwN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## dataset definition"],"metadata":{"id":"gMGTtnzfhdjZ"}},{"cell_type":"code","source":["class BrainAudioDataset(torch.utils.data.Dataset):\n","\n","    def __init__():\n","        super().__init__()\n","        pass\n","\n","    def __len__():\n","        pass\n","\n","    def __getitem__():\n","        \"\"\"\n","        inputs:\n","        - idx:\n","\n","        outputs:\n","        - eeg:   (1, C, T)\n","        - mfcc:  (1, C, T)\n","        - words: ??\n","        \"\"\"\n","        pass\n","\n","    def collate_fn(self, batch):\n","        pass\n"],"metadata":{"id":"pFTwPi-bjM6t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## dataloader definition"],"metadata":{"id":"ZyxB-LPwhmJs"}},{"cell_type":"code","source":["#TODO"],"metadata":{"id":"L4EqQafWgSzE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# models [KS]"],"metadata":{"id":"g-bEfzz6ho3g"}},{"cell_type":"markdown","source":["## common.py"],"metadata":{"id":"4hN8UrY_h0fG"}},{"cell_type":"code","source":["class ScaledEmbedding(nn.Module):\n","    \"\"\"Scale up learning rate for the embedding, otherwise, it can move too slowly.\n","    \"\"\"\n","    def __init__(self, num_embeddings: int, embedding_dim: int, scale: float = 10.):\n","        super().__init__()\n","        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n","        self.embedding.weight.data /= scale\n","        self.scale = scale\n","\n","    @property\n","    def weight(self):\n","        return self.embedding.weight * self.scale\n","\n","    def forward(self, x):\n","        return self.embedding(x) * self.scale\n","\n","\n","class SubjectLayers(nn.Module):\n","    \"\"\"Per subject linear layer.\"\"\"\n","    def __init__(self, in_channels: int, out_channels: int, n_subjects: int, init_id: bool = False):\n","        super().__init__()\n","        self.weights = nn.Parameter(torch.randn(n_subjects, in_channels, out_channels))\n","        if init_id:\n","            assert in_channels == out_channels\n","            self.weights.data[:] = torch.eye(in_channels)[None]\n","        self.weights.data *= 1 / in_channels**0.5\n","\n","    def forward(self, x, subjects):\n","        _, C, D = self.weights.shape\n","        weights = self.weights.gather(0, subjects.view(-1, 1, 1).expand(-1, C, D))\n","        return torch.einsum(\"bct,bcd->bdt\", x, weights)\n","\n","    def __repr__(self):\n","        S, C, D = self.weights.shape\n","        return f\"SubjectLayers({C}, {D}, {S})\"\n","\n","\n","class LayerScale(nn.Module):\n","    \"\"\"Layer scale from [Touvron et al 2021] (https://arxiv.org/pdf/2103.17239.pdf).\n","    This rescales diagonaly residual outputs close to 0 initially, then learnt.\n","    \"\"\"\n","    def __init__(self, channels: int, init: float = 0.1, boost: float = 5.):\n","        super().__init__()\n","        self.scale = nn.Parameter(torch.zeros(channels, requires_grad=True))\n","        self.scale.data[:] = init / boost\n","        self.boost = boost\n","\n","    def forward(self, x):\n","        return (self.boost * self.scale[:, None]) * x\n","\n","\n","class ConvSequence(nn.Module):\n","\n","    def __init__(self, channels: tp.Sequence[int], kernel: int = 4, dilation_growth: int = 1,\n","                 dilation_period: tp.Optional[int] = None, stride: int = 2,\n","                 dropout: float = 0.0, leakiness: float = 0.0, groups: int = 1,\n","                 decode: bool = False, batch_norm: bool = False, dropout_input: float = 0,\n","                 skip: bool = False, scale: tp.Optional[float] = None, rewrite: bool = False,\n","                 activation_on_last: bool = True, post_skip: bool = False, glu: int = 0,\n","                 glu_context: int = 0, glu_glu: bool = True, activation: tp.Any = None) -> None:\n","        super().__init__()\n","        dilation = 1\n","        channels = tuple(channels)\n","        self.skip = skip\n","        self.sequence = nn.ModuleList()\n","        self.glus = nn.ModuleList()\n","        if activation is None:\n","            activation = partial(nn.LeakyReLU, leakiness)\n","        Conv = nn.Conv1d if not decode else nn.ConvTranspose1d\n","        # build layers\n","        for k, (chin, chout) in enumerate(zip(channels[:-1], channels[1:])):\n","            layers: tp.List[nn.Module] = []\n","            is_last = k == len(channels) - 2\n","\n","            # Set dropout for the input of the conv sequence if defined\n","            if k == 0 and dropout_input:\n","                assert 0 < dropout_input < 1\n","                layers.append(nn.Dropout(dropout_input))\n","\n","            # conv layer\n","            if dilation_growth > 1:\n","                assert kernel % 2 != 0, \"Supports only odd kernel with dilation for now\"\n","            if dilation_period and (k % dilation_period) == 0:\n","                dilation = 1\n","            pad = kernel // 2 * dilation\n","            layers.append(Conv(chin, chout, kernel, stride, pad,\n","                               dilation=dilation, groups=groups if k > 0 else 1))\n","            dilation *= dilation_growth\n","            # non-linearity\n","            if activation_on_last or not is_last:\n","                if batch_norm:\n","                    layers.append(nn.BatchNorm1d(num_features=chout))\n","                layers.append(activation())\n","                if dropout:\n","                    layers.append(nn.Dropout(dropout))\n","                if rewrite:\n","                    layers += [nn.Conv1d(chout, chout, 1), nn.LeakyReLU(leakiness)]\n","                    # layers += [nn.Conv1d(chout, 2 * chout, 1), nn.GLU(dim=1)]\n","            if chin == chout and skip:\n","                if scale is not None:\n","                    layers.append(LayerScale(chout, scale))\n","                if post_skip:\n","                    layers.append(Conv(chout, chout, 1, groups=chout, bias=False))\n","\n","            self.sequence.append(nn.Sequential(*layers))\n","            if glu and (k + 1) % glu == 0:\n","                ch = 2 * chout if glu_glu else chout\n","                act = nn.GLU(dim=1) if glu_glu else activation()\n","                self.glus.append(\n","                    nn.Sequential(\n","                        nn.Conv1d(chout, ch, 1 + 2 * glu_context, padding=glu_context), act))\n","            else:\n","                self.glus.append(None)\n","\n","    def forward(self, x: tp.Any) -> tp.Any:\n","        for module_idx, module in enumerate(self.sequence):\n","            old_x = x\n","            x = module(x)\n","            if self.skip and x.shape == old_x.shape:\n","                x = x + old_x\n","            glu = self.glus[module_idx]\n","            if glu is not None:\n","                x = glu(x)\n","        return x\n","\n","\n","class DualPathRNN(nn.Module):\n","    def __init__(self, channels: int, depth: int, inner_length: int = 10):\n","        super().__init__()\n","        self.lstms = nn.ModuleList([nn.LSTM(channels, channels, 1) for _ in range(depth * 4)])\n","        self.inner_length = inner_length\n","\n","    def forward(self, x: torch.Tensor):\n","        B, C, L = x.shape\n","        IL = self.inner_length\n","        x = pad_multiple(x, self.inner_length)\n","        x = x.permute(2, 0, 1).contiguous()\n","        for idx, lstm in enumerate(self.lstms):\n","            y = x.reshape(-1, IL, B, C)\n","            if idx % 2 == 0:\n","                y = y.transpose(0, 1).reshape(IL, -1, C)\n","            else:\n","                y = y.reshape(-1, IL * B, C)\n","            y, _ = lstm(x)\n","            if idx % 2 == 0:\n","                y = y.reshape(IL, -1, B, C).transpose(0, 1).reshape(-1, B, C)\n","            else:\n","                y = y.reshape(-1, B, C)\n","            x = x + y\n","\n","            if idx % 2 == 1:\n","                x = x.flip(dims=(0,))\n","        return x[:L].permute(1, 2, 0).contiguous()\n","\n","\n","class PositionGetter:\n","    INVALID = -0.1\n","\n","    def __init__(self) -> None:\n","        self._cache: tp.Dict[int, torch.Tensor] = {}\n","        self._invalid_names: tp.Set[str] = set()\n","\n","    def get_recording_layout(self, recording: Recording) -> torch.Tensor:\n","        index = recording.recording_index\n","        if index in self._cache:\n","            return self._cache[index]\n","        else:\n","            info = recording.mne_info\n","            layout = mne.find_layout(info)\n","            indexes: tp.List[int] = []\n","            valid_indexes: tp.List[int] = []\n","            for meg_index, name in enumerate(info.ch_names):\n","                name = name.rsplit(\"-\", 1)[0]\n","                try:\n","                    indexes.append(layout.names.index(name))\n","                except ValueError:\n","                    if name not in self._invalid_names:\n","                        logger.warning(\n","                            \"Channels %s not in layout for recording %s of %s.\",\n","                            name,\n","                            recording.study_name(),\n","                            recording.recording_uid)\n","                        self._invalid_names.add(name)\n","                else:\n","                    valid_indexes.append(meg_index)\n","\n","            positions = torch.full((len(info.ch_names), 2), self.INVALID)\n","            x, y = layout.pos[indexes, :2].T\n","            x = (x - x.min()) / (x.max() - x.min())\n","            y = (y - y.min()) / (y.max() - y.min())\n","            x = torch.from_numpy(x).float()\n","            y = torch.from_numpy(y).float()\n","            positions[valid_indexes, 0] = x\n","            positions[valid_indexes, 1] = y\n","            self._cache[index] = positions\n","            return positions\n","\n","    def get_positions(self, batch):\n","        meg = batch.meg\n","        B, C, T = meg.shape\n","        positions = torch.full((B, C, 2), self.INVALID, device=meg.device)\n","        for idx in range(len(batch)):\n","            recording = batch._recordings[idx]\n","            rec_pos = self.get_recording_layout(recording)\n","            positions[idx, :len(rec_pos)] = rec_pos.to(meg.device)\n","        return positions\n","\n","    def is_invalid(self, positions):\n","        return (positions == self.INVALID).all(dim=-1)\n","\n","\n","class FourierEmb(nn.Module):\n","    \"\"\"\n","    Fourier positional embedding.\n","    Unlike trad. embedding this is not using exponential periods\n","    for cosines and sinuses, but typical `2 pi k` which can represent\n","    any function over [0, 1]. As this function would be necessarily periodic,\n","    we take a bit of margin and do over [-0.2, 1.2].\n","    \"\"\"\n","    def __init__(self, dimension: int = 256, margin: float = 0.2):\n","        super().__init__()\n","        n_freqs = (dimension // 2)**0.5\n","        assert int(n_freqs ** 2 * 2) == dimension\n","        self.dimension = dimension\n","        self.margin = margin\n","\n","    def forward(self, positions):\n","        *O, D = positions.shape\n","        assert D == 2\n","        *O, D = positions.shape\n","        n_freqs = (self.dimension // 2)**0.5\n","        freqs_y = torch.arange(n_freqs).to(positions)\n","        freqs_x = freqs_y[:, None]\n","        width = 1 + 2 * self.margin\n","        positions = positions + self.margin\n","        p_x = 2 * math.pi * freqs_x / width\n","        p_y = 2 * math.pi * freqs_y / width\n","        positions = positions[..., None, None, :]\n","        loc = (positions[..., 0] * p_x + positions[..., 1] * p_y).view(*O, -1)\n","        emb = torch.cat([\n","            torch.cos(loc),\n","            torch.sin(loc),\n","        ], dim=-1)\n","        return emb\n","\n","\n","class ChannelDropout(nn.Module):\n","    def __init__(self, dropout: float = 0.1, rescale: bool = True):\n","        \"\"\"\n","        Args:\n","            dropout: dropout radius in normalized [0, 1] coordinates.\n","            rescale: at valid, rescale all channels.\n","        \"\"\"\n","        super().__init__()\n","        self.dropout = dropout\n","        self.rescale = rescale\n","        self.position_getter = PositionGetter()\n","\n","    def forward(self, meg, batch):\n","        if not self.dropout:\n","            return meg\n","\n","        B, C, T = meg.shape\n","        meg = meg.clone()\n","        positions = self.position_getter.get_positions(batch)\n","        valid = (~self.position_getter.is_invalid(positions)).float()\n","        meg = meg * valid[:, :, None]\n","\n","        if self.training:\n","            center_to_ban = torch.rand(2, device=meg.device)\n","            kept = (positions - center_to_ban).norm(dim=-1) > self.dropout\n","            meg = meg * kept.float()[:, :, None]\n","            if self.rescale:\n","                proba_kept = torch.zeros(B, C, device=meg.device)\n","                n_tests = 100\n","                for _ in range(n_tests):\n","                    center_to_ban = torch.rand(2, device=meg.device)\n","                    kept = (positions - center_to_ban).norm(dim=-1) > self.dropout\n","                    proba_kept += kept.float() / n_tests\n","                meg = meg / (1e-8 + proba_kept[:, :, None])\n","\n","        return meg\n","\n","\n","class ChannelMerger(nn.Module):\n","    def __init__(self, chout: int, pos_dim: int = 256,\n","                 dropout: float = 0, usage_penalty: float = 0.,\n","                 n_subjects: int = 200, per_subject: bool = False):\n","        super().__init__()\n","        assert pos_dim % 4 == 0\n","        self.position_getter = PositionGetter()\n","        self.per_subject = per_subject\n","        if self.per_subject:\n","            self.heads = nn.Parameter(torch.randn(n_subjects, chout, pos_dim, requires_grad=True))\n","        else:\n","            self.heads = nn.Parameter(torch.randn(chout, pos_dim, requires_grad=True))\n","        self.heads.data /= pos_dim ** 0.5\n","        self.dropout = dropout\n","        self.embedding = FourierEmb(pos_dim)\n","        self.usage_penalty = usage_penalty\n","        self._penalty = torch.tensor(0.)\n","\n","    @property\n","    def training_penalty(self):\n","        return self._penalty.to(next(self.parameters()).device)\n","\n","    def forward(self, meg, batch):\n","        B, C, T = meg.shape\n","        meg = meg.clone()\n","        positions = self.position_getter.get_positions(batch)\n","        embedding = self.embedding(positions)\n","        score_offset = torch.zeros(B, C, device=meg.device)\n","        score_offset[self.position_getter.is_invalid(positions)] = float('-inf')\n","\n","        if self.training and self.dropout:\n","            center_to_ban = torch.rand(2, device=meg.device)\n","            radius_to_ban = self.dropout\n","            banned = (positions - center_to_ban).norm(dim=-1) <= radius_to_ban\n","            score_offset[banned] = float('-inf')\n","\n","        if self.per_subject:\n","            _, cout, pos_dim = self.heads.shape\n","            subject = batch.subject_index\n","            heads = self.heads.gather(0, subject.view(-1, 1, 1).expand(-1, cout, pos_dim))\n","        else:\n","            heads = self.heads[None].expand(B, -1, -1)\n","\n","        scores = torch.einsum(\"bcd,bod->boc\", embedding, heads)\n","        scores += score_offset[:, None]\n","        weights = torch.softmax(scores, dim=2)\n","        out = torch.einsum(\"bct,boc->bot\", meg, weights)\n","        if self.training and self.usage_penalty > 0.:\n","            usage = weights.mean(dim=(0, 1)).sum()\n","            self._penalty = self.usage_penalty * usage\n","        return out"],"metadata":{"id":"fb7FawLrhp-D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## convrnn.py"],"metadata":{"id":"UEyB-PsLiHoT"}},{"cell_type":"code","source":[],"metadata":{"id":"H0k0R7qNiQnm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## features.py"],"metadata":{"id":"yXdiTMU0h2xS"}},{"cell_type":"code","source":[],"metadata":{"id":"UyNpE6BtiRBO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## simpleconv.py"],"metadata":{"id":"iIdr-kGSh2zw"}},{"cell_type":"code","source":[],"metadata":{"id":"p7oM0GGHhqA2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## new model [CM]"],"metadata":{"id":"-cUoXhRYoPWB"}},{"cell_type":"code","source":[],"metadata":{"id":"mqKTR5BmsOZa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## decoder\n","\n","audio -> wav2vec2 -> embedding space <- our model <- eeg\n","\n","once in common embedding space, use CLIP\n","\n","From CLIP --> decode to words"],"metadata":{"id":"Oxf0pcOFpYjI"}},{"cell_type":"markdown","source":["# losses and metrics"],"metadata":{"id":"N4TkGFvZjt2D"}},{"cell_type":"markdown","source":["## clip [DS]"],"metadata":{"id":"VY5hMIVkj_oj"}},{"cell_type":"code","source":[],"metadata":{"id":"spuoOZ9Gju3y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## wer [CM]"],"metadata":{"id":"uSY_MAO6j8hI"}},{"cell_type":"markdown","source":["### general"],"metadata":{"id":"fR6YV-9TrENJ"}},{"cell_type":"code","source":[],"metadata":{"id":"h-9qo2zfkAp1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### vocab-specific"],"metadata":{"id":"wVIA4rXSrFt3"}},{"cell_type":"code","source":[],"metadata":{"id":"aGMPNPaSrHAI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# trainer\n"],"metadata":{"id":"82KWAuB6jPQo"}},{"cell_type":"code","source":[],"metadata":{"id":"_SIEgYN4jS97"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# experiments"],"metadata":{"id":"N4UE5_2vjUtQ"}},{"cell_type":"code","source":[],"metadata":{"id":"rVoKYgt-j0RE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# evaluation"],"metadata":{"id":"dv73E9hkjYnz"}},{"cell_type":"markdown","source":["## viz"],"metadata":{"id":"EXoEy94Dj3kt"}},{"cell_type":"code","source":[],"metadata":{"id":"q_cyKW1Sj0ol"},"execution_count":null,"outputs":[]}]}