{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"collapsed_sections":["VGGW29McBsXA","VpwsQQRCWtoz","LpDBVV_I77cU","28OV6Qkp8K5h","xMz2kKoj8K5p"],"machine_shape":"hm","gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"daa9df36f16d44dfa64cac2c2af7cc46":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_50530c35211f486ea38af42e08331c42","IPY_MODEL_350b5a328a8c4cd5a1aed1edbbbc3443","IPY_MODEL_dcecd0822ada4d9abf40f0cd2fce2931"],"layout":"IPY_MODEL_3e2226286fe84427b256eba0c53a1fb2"}},"50530c35211f486ea38af42e08331c42":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a2949d979d2449959fc238a6e32e4e23","placeholder":"​","style":"IPY_MODEL_3d7b6902e1d84fbaa8a6e4840eb71cce","value":"100%"}},"350b5a328a8c4cd5a1aed1edbbbc3443":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1f5e74b965424448bf806e3378760730","max":12,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3fb6f5a759fd45749ca23300eff39c52","value":12}},"dcecd0822ada4d9abf40f0cd2fce2931":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_57ea95edf31e452aaf104f882801538d","placeholder":"​","style":"IPY_MODEL_8825b4ddb60444b785ca72fc2705c4c1","value":" 12/12 [00:44&lt;00:00,  3.36s/it]"}},"3e2226286fe84427b256eba0c53a1fb2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a2949d979d2449959fc238a6e32e4e23":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3d7b6902e1d84fbaa8a6e4840eb71cce":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1f5e74b965424448bf806e3378760730":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3fb6f5a759fd45749ca23300eff39c52":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"57ea95edf31e452aaf104f882801538d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8825b4ddb60444b785ca72fc2705c4c1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"74fc1ecbeb78482a877ea17c3083fe06":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_52babf0eefa04a3588fd8ba8f1e43b30","IPY_MODEL_8e1bded7defc403b85f2c58416dbf8f0","IPY_MODEL_f7fa314845ef4a4289b2b81c366c9ce1"],"layout":"IPY_MODEL_5aaea44146654d4ebffc518799212bcf"}},"52babf0eefa04a3588fd8ba8f1e43b30":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a8ab1d11d12049459bbddbc5e14cc905","placeholder":"​","style":"IPY_MODEL_b058833640a144ab9d63eeac7b4f1606","value":"100%"}},"8e1bded7defc403b85f2c58416dbf8f0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f8bdd615c6ab43e28102c0b89a60ac62","max":348,"min":0,"orientation":"horizontal","style":"IPY_MODEL_24e3296a881544c4bf99b1e380ed03f8","value":348}},"f7fa314845ef4a4289b2b81c366c9ce1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_87c7ebd29fea414bb8ad5aacd5ffee58","placeholder":"​","style":"IPY_MODEL_0751ebffe0654bc1b860c8ee4c896c79","value":" 348/348 [00:00&lt;00:00, 25141.12it/s]"}},"5aaea44146654d4ebffc518799212bcf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8ab1d11d12049459bbddbc5e14cc905":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b058833640a144ab9d63eeac7b4f1606":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f8bdd615c6ab43e28102c0b89a60ac62":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"24e3296a881544c4bf99b1e380ed03f8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"87c7ebd29fea414bb8ad5aacd5ffee58":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0751ebffe0654bc1b860c8ee4c896c79":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# installs and imports"],"metadata":{"id":"VGGW29McBsXA"}},{"cell_type":"code","source":["!pip install wandb torchsummaryX mne transformers Levenshtein adamp jiwer -q"],"metadata":{"id":"pOgws5nEhBsx","executionInfo":{"status":"ok","timestamp":1702225426114,"user_tz":300,"elapsed":14622,"user":{"displayName":"Kateryna Shapovalenko","userId":"16393911302329786226"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e08de245-bf0e-479c-c99f-ee15b631e0d7"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.4/169.4 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.8/252.8 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m99.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for adamp (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["### Torch\n","import  torch\n","import  torch.nn as nn\n","import  torch.nn.functional as F\n","from    torch.optim import lr_scheduler\n","from    torchsummaryX import summary\n","from    torch.utils.data import Dataset, DataLoader, random_split\n","import  torchaudio\n","import  torchaudio.transforms as tat\n","from    torch.nn.utils.rnn import (pad_sequence, pack_padded_sequence,\n","                                   pad_packed_sequence)\n","from torch.optim import AdamW\n","from adamp import AdamP\n","\n","### General\n","import  random\n","import  numpy as np\n","import  pandas as pd\n","import  scipy\n","import  gc\n","from    tqdm.auto import tqdm\n","import  os\n","import  datetime\n","import  time\n","import  wandb\n","import  matplotlib.pyplot as plt\n","import  seaborn as sns\n","import re\n","import seaborn\n","import Levenshtein as lev\n","\n","### Other\n","\n","from functools import partial\n","import logging\n","import math\n","import typing as tp\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","import torch.multiprocessing as mp\n","mp.set_start_method('spawn', force=True)\n","\n","# wav2vec2 and EEG processing\n","from    transformers import (AutoProcessor, AutoModelForPreTraining,\n","                             CLIPProcessor, CLIPModel, Wav2Vec2ForCTC, Wav2Vec2Processor)\n","import  mne\n","from transformers import Wav2Vec2Model"],"metadata":{"id":"L80P0Fy3gSnQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# working directory"],"metadata":{"id":"VpwsQQRCWtoz"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"id":"doTTk4H_WxMu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DATA_PATH  = '/content/gdrive/MyDrive/11785-IDLf23/Final_project/0_Data/'\n","\n","BRAIN_PATH = os.path.join(DATA_PATH, 'brennan_and_hale_v2')\n","AUDIO_PATH = os.path.join(DATA_PATH, 'brennan_and_hale_v2/audio')\n","PPROC_PATH = os.path.join(DATA_PATH, 'brennan_and_hale_v2/proc/timelock-preprocessing')\n","\n","EEG_PATH   = os.path.join(DATA_PATH, 'eeg-segments')\n","EMBED_PATH = os.path.join(DATA_PATH, 'brennan_wav2vec2_embeddings')\n","\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"OElDc-hLWrbu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# configs"],"metadata":{"id":"558QHQh5xpIj"}},{"cell_type":"code","source":["config = dict(\n","\n","    # dataloaders ----------------------------------------------------------------\n","    batch_size = 4,\n","    transforms = None,\n","    reduce_data_ratio = 1, # 1 if we want to use the entire dataset\n","\n","    # model ----------------------------------------------------------------\n","\n","    ###\n","\n","    ###\n","\n","    ###\n","\n","    # optimizer ----------------------------------------------------------------\n","    learning_rate = 0.0001,\n","    weight_decay = 5e-3,\n","\n","    # scheduler ----------------------------------------------------------------\n","    factor = 0.9,\n","    patience = 3,\n","\n","    # trainer ----------------------------------------------------------------\n","    epochs = 30)"],"metadata":{"id":"iThqLq6axoj3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# dataset and dataloaders [QA]"],"metadata":{"id":"YlPfcrpLgvxH"}},{"cell_type":"markdown","source":["## dataset definition"],"metadata":{"id":"gMGTtnzfhdjZ"}},{"cell_type":"code","source":["class BrainAudioDataset(torch.utils.data.Dataset):\n","    \"\"\"\n","    Hybrid Memory Efficient Dataset.\n","    Loads all audio embeddings in __init__(). ~1-2min.\n","    Loads eeg in __getitem__().\n","    Loading everything in __init__() crashes.\n","    Loading everything in __getitem__() is super slow.\n","\n","    Load eeg\n","    Pad (add AUD of zeros) to be 62 chan\n","    Mask the bad chans\n","    \"\"\"\n","\n","    def __init__(self, eeg_path=EEG_PATH, embed_path=EMBED_PATH, transforms=None):\n","        super().__init__()\n","\n","        self.eeg_root = eeg_path\n","        self.embed_root = embed_path\n","        self.transforms = transforms\n","\n","        eeg_fnames = sorted(os.listdir(self.eeg_root))\n","        embed_fnames = sorted(os.listdir(self.embed_root))\n","\n","        gc.collect()\n","\n","        self.audio_embeddings = {}\n","        for segment_idx, fname in enumerate(tqdm(embed_fnames), start=1):\n","            audio_fpath = os.path.join(self.embed_root, fname)\n","            audio = torch.load(audio_fpath)\n","            audio_embed = audio.hidden_states[-1]\n","            audio_embed = audio_embed.squeeze(0)\n","            audio_embed = audio_embed.to(DEVICE)\n","            self.audio_embeddings[segment_idx] = audio_embed\n","            del audio, audio_embed\n","            gc.collect()\n","\n","        self.eegs = {}\n","        for idx, fname in enumerate(tqdm(eeg_fnames)):\n","            # Extract subject id and segment (to be returned in __getitem__())\n","            subject_idx, segment_idx = self.extract_info(fname)\n","\n","            # Load eeg filepath\n","            eeg_fpath = os.path.join(self.eeg_root, fname)\n","            self.eegs[idx] = (subject_idx, int(segment_idx), eeg_fpath)\n","\n","        self.length = len(self.eegs)\n","\n","        # montage = mne.channels.make_standard_montage(\"easycap-M10\")\n","\n","    def __len__(self):\n","\n","        return self.length\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        - eeg           : (1, T_eeg, C_eeg)\n","        - audio_embed   : (1, T_audio, 1024)\n","        \"\"\"\n","        # Get eeg\n","        _, segment_idx, eeg_fpath = self.eegs[idx]\n","        eeg = np.load(eeg_fpath)\n","        eeg = torch.tensor(eeg.transpose(), device=DEVICE)\n","\n","        # If nchan < 62, zero pad for the audio channel (should be done in preproc)\n","\n","        # Remove bad chan (should be done in prepoc)\n","\n","        # Retrieve pre-loaded audio embedding\n","        audio_embed = self.audio_embeddings[segment_idx]\n","        return eeg, audio_embed, len(eeg), len(audio_embed)\n","\n","    def collate_fn(self, batch):\n","\n","        eeg, audio_embed, len_eeg, len_audio_embed = zip(*batch)\n","\n","        # pack the mfccs and transcripts using the pad_sequence function from pytorch\n","        batch_eeg_pad = pad_sequence(eeg, batch_first=True)\n","        batch_audio_embed_pad = pad_sequence(audio_embed, batch_first=True)\n","        del eeg, audio_embed\n","\n","        # Apply transformations\n","        if self.transforms is not None:\n","            batch_eeg_pad = self.transforms(batch_eeg_pad)\n","\n","        return (batch_eeg_pad,\n","                batch_audio_embed_pad,\n","                torch.tensor(len_eeg, dtype=torch.int64),\n","                torch.tensor(len_audio_embed, dtype=torch.int64))\n","\n","    def read_sfp(self, file_path):\n","        \"\"\"\n","        Reads a BESA SFP (Surface Point) file (locations of sensors)\n","        \"\"\"\n","\n","        electrodes = {}\n","        with open(file_path, 'r') as file:\n","            for line in file:\n","                parts = line.strip().split()  # Split by whitespace\n","                if len(parts) == 4:\n","                    # Parse the electrode name and coordinates\n","                    name = parts[0]\n","                    x, y, z = map(float, parts[1:])  # Convert strings to floats\n","                    electrodes[name] = (x, y, z)\n","\n","        return electrodes\n","\n","    def extract_info(self, fname):\n","        # This pattern looks for any text (non-digits) followed by digits, a hyphen, and more digits\n","        match = re.match(r'([A-Za-z]+[0-9]+)-([0-9]+).npy', fname)\n","        if match:\n","            return match.group(1), match.group(2)\n","        else:\n","            return None\n"],"metadata":{"id":"_qxzB4c-aAkg","executionInfo":{"status":"ok","timestamp":1702225028313,"user_tz":300,"elapsed":149,"user":{"displayName":"Kateryna Shapovalenko","userId":"16393911302329786226"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["## dataloader definition"],"metadata":{"id":"ZyxB-LPwhmJs"}},{"cell_type":"code","source":["full_dataset = BrainAudioDataset()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["daa9df36f16d44dfa64cac2c2af7cc46","50530c35211f486ea38af42e08331c42","350b5a328a8c4cd5a1aed1edbbbc3443","dcecd0822ada4d9abf40f0cd2fce2931","3e2226286fe84427b256eba0c53a1fb2","a2949d979d2449959fc238a6e32e4e23","3d7b6902e1d84fbaa8a6e4840eb71cce","1f5e74b965424448bf806e3378760730","3fb6f5a759fd45749ca23300eff39c52","57ea95edf31e452aaf104f882801538d","8825b4ddb60444b785ca72fc2705c4c1","74fc1ecbeb78482a877ea17c3083fe06","52babf0eefa04a3588fd8ba8f1e43b30","8e1bded7defc403b85f2c58416dbf8f0","f7fa314845ef4a4289b2b81c366c9ce1","5aaea44146654d4ebffc518799212bcf","a8ab1d11d12049459bbddbc5e14cc905","b058833640a144ab9d63eeac7b4f1606","f8bdd615c6ab43e28102c0b89a60ac62","24e3296a881544c4bf99b1e380ed03f8","87c7ebd29fea414bb8ad5aacd5ffee58","0751ebffe0654bc1b860c8ee4c896c79"]},"id":"XOwY0sFhtiPx","executionInfo":{"status":"ok","timestamp":1702225078079,"user_tz":300,"elapsed":48817,"user":{"displayName":"Kateryna Shapovalenko","userId":"16393911302329786226"}},"outputId":"bb2a3020-2d3b-4494-9dcb-991dc7c9b0d0"},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/12 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"daa9df36f16d44dfa64cac2c2af7cc46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/348 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74fc1ecbeb78482a877ea17c3083fe06"}},"metadata":{}}]},{"cell_type":"code","source":["TRAIN_PORTION = 0.8\n","VAL_PORTION   = 0.1\n","TEST_PORTION  = 0.1\n","\n","# Splitting the dataset /// train_len = int(len(full_dataset) * TRAIN_PORTION)\n","torch.manual_seed(1)\n","train_data, val_data, test_data = random_split(full_dataset, [TRAIN_PORTION, VAL_PORTION, TEST_PORTION])\n","\n","train_loader = torch.utils.data.DataLoader(\n","    dataset      = train_data,\n","    batch_size   = config['batch_size'],\n","    shuffle      = True,\n","    drop_last    = False,\n","    num_workers  = 4,\n","    pin_memory   = True,\n","    collate_fn   = full_dataset.collate_fn\n",")\n","\n","val_loader = torch.utils.data.DataLoader(\n","    dataset      = val_data,\n","    batch_size   = config['batch_size'],\n","    shuffle      = False,\n","    drop_last    = False,\n","    num_workers  = 4,\n","    pin_memory   = True,\n","    collate_fn   = full_dataset.collate_fn\n",")\n","\n","test_loader = torch.utils.data.DataLoader(\n","    dataset      = test_data,\n","    batch_size   = config['batch_size'],\n","    shuffle      = False,\n","    drop_last    = False,\n","    num_workers  = 4,\n","    pin_memory   = True,\n","    collate_fn   = full_dataset.collate_fn\n",")"],"metadata":{"id":"1Fugny6Lrmr0","executionInfo":{"status":"ok","timestamp":1702225329532,"user_tz":300,"elapsed":161,"user":{"displayName":"Kateryna Shapovalenko","userId":"16393911302329786226"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["print('-'*80)\n","print(f'Len full data:      {len(full_dataset)}')\n","print(f'Len train data:     {len(train_data)}')\n","print(f'Len val data:       {len(val_data)}')\n","print(f'Len test data:      {len(test_data)}')\n","print('-'*80)\n","print(f'Len Train Loader:   {train_loader.__len__()}')\n","print(f'Len Val Loader:     {val_loader.__len__()}')\n","print(f'Len Test Loader:    {test_loader.__len__()}')\n","\n","gc.collect()\n","\n","for batch in train_loader:\n","    eeg, audio_embedding, l_eeg, l_audio = batch\n","    print(eeg.shape, audio_embedding.shape, l_eeg.shape, l_audio.shape)\n","    print(eeg.dtype, audio_embedding.dtype, l_eeg.dtype, l_audio.dtype)\n","    del eeg, audio_embedding, l_eeg, l_audio\n","    gc.collect()\n","    break\n","\n","for batch in val_loader:\n","    eeg, audio_embedding, l_eeg, l_audio = batch\n","    print(eeg.shape, audio_embedding.shape, l_eeg.shape, l_audio.shape)\n","    print(eeg.dtype, audio_embedding.dtype, l_eeg.dtype, l_audio.dtype)\n","    del eeg, audio_embedding, l_eeg, l_audio\n","    gc.collect()\n","    break\n","\n","for batch in test_loader:\n","    eeg, audio_embedding, l_eeg, l_audio = batch\n","    print(eeg.shape, audio_embedding.shape, l_eeg.shape, l_audio.shape)\n","    print(eeg.dtype, audio_embedding.dtype, l_eeg.dtype, l_audio.dtype)\n","    del eeg, audio_embedding, l_eeg, l_audio\n","    gc.collect()\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":617},"id":"QnuiZJ93ttqU","executionInfo":{"status":"error","timestamp":1702225375267,"user_tz":300,"elapsed":42549,"user":{"displayName":"Kateryna Shapovalenko","userId":"16393911302329786226"}},"outputId":"c6cd55bd-c720-44e6-ef10-3b910e5dda95"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["--------------------------------------------------------------------------------\n","Len full data:      348\n","Len train data:     279\n","Len val data:       35\n","Len test data:      34\n","--------------------------------------------------------------------------------\n","Len Train Loader:   70\n","Len Val Loader:     9\n","Len Test Loader:    9\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1132\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1133\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/signal_handling.py\u001b[0m in \u001b[0;36mhandler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# Python can still get and update the process status successfully.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0m_error_if_any_worker_fails\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprevious_handler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 4451) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-f9d2382219c5>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0meeg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_eeg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_audio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meeg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_embedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_eeg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_audio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1328\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1329\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1282\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1283\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1284\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1285\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1286\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1143\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m                 \u001b[0mpids_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1145\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'DataLoader worker (pid(s) {pids_str}) exited unexpectedly'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmpty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 4451) exited unexpectedly"]}]},{"cell_type":"markdown","source":["# meta models desc"],"metadata":{"id":"2r2nluekdz4K"}},{"cell_type":"markdown","source":["Relevant Meta folders: [Features folder](https://github.com/facebookresearch/brainmagick/tree/main/bm/features) and [Model folder](https://github.com/facebookresearch/brainmagick/tree/main/bm/models).\n","\n","\n","**Shared model components and models:**\n","\n","1. **[common.py](https://github.com/facebookresearch/brainmagick/blob/main/bm/models/common.py)** - collection of components used by other models\n","\n","  - ScaledEmbedding\n","\n","  - SubjectLayers\n","\n","  - LayerScale\n","\n","  - ConvSequence\n","\n","  - DualPathRNN\n","\n","  - PositionGetter\n","\n","  - FourierEmb\n","\n","  - ChannelDropout\n","\n","  - ChannelMerger\n","\n","2. **[convrnn.py](https://github.com/facebookresearch/brainmagick/blob/main/bm/models/convrnn.py)** - a model used both as an a encoder and a decoder.\n","\n","  - LSTM\n","\n","  - Attention\n","\n","  - ConvRNN\n","  \n","    - SubjectLayers (subject layer)\n","\n","    - ScaledEmbedding (subject embedding)\n","    \n","    - LSTM (bidirectional)\n","\n","    - Attention (multi-head dot product)\n","\n","    - ConvSequence (decoder)\n","  \n","    - Conv1d or Conv1d + ReLU + Conv1d (final)\n","\n","3. **[simpleconv.py](https://github.com/facebookresearch/brainmagick/blob/main/bm/models/simpleconv.py)**\n","\n","  - SimpleConv\n","\n","    - takes a sample of channels (subsampled_meg_channels)\n","    \n","    - ChannelDropout\n","\n","    - ChannelMerger\n","\n","    - Conv1d + activations (initial layer)\n","\n","    - SubjectLayers (subject layer)\n","\n","    - ta.transforms.Spectrogram (short-time fourier transform)\n","\n","    - ScaledEmbedding (subject embedding)\n","\n","    - ConvSequence (encoder)\n","\n","    - DualPathRNN\n","  \n","    - Conv1d or Conv1d + ReLU + Conv1d (final)\n","\n","4. **[features.py](https://github.com/facebookresearch/brainmagick/blob/main/bm/models/features.py)** - model to extract features\n","\n","  - DeepMel(ConvSequence)\n","\n","**Combined encoders:**\n","\n","1. **[deep_mel.yaml](https://github.com/facebookresearch/brainmagick/blob/main/bm/conf/feature_model/deep_mel.yaml)** - calls **features.py** - feature model\n","\n","2. **[convrnn.yaml](https://github.com/facebookresearch/brainmagick/blob/main/bm/conf/model/convrnn.yaml)** - calls **convrnn.py**\n","\n","**Combined decoders:**\n","\n","1. **[clip_conv.yaml](https://github.com/facebookresearch/brainmagick/blob/main/bm/conf/model/clip_conv.yaml/)** - calls **simpleconv.py** - default model\n","\n","2. **[decoder_convrnn.yaml](https://github.com/facebookresearch/brainmagick/blob/main/bm/conf/model/decoder_convrnn.yaml)** - calls **convrnn.py**\n","\n","\n","Other: All the imports https://github.com/facebookresearch/brainmagick/blob/main/requirements.txt"],"metadata":{"id":"MioCdwZACrZ1"}},{"cell_type":"markdown","source":["# models [KS]"],"metadata":{"id":"g-bEfzz6ho3g"}},{"cell_type":"markdown","source":["## replicated meta models"],"metadata":{"id":"LpDBVV_I77cU"}},{"cell_type":"markdown","source":["### brain encoder (replicated meta's convrnn)"],"metadata":{"id":"28OV6Qkp8K5h"}},{"cell_type":"code","source":["class SubjectLayers(nn.Module): # learn how different are the subjects and based on that normalize the EEG\n","    def __init__(self, in_channels, out_channels, n_subjects):\n","        super().__init__()\n","        self.weights = nn.Parameter(torch.randn(n_subjects, in_channels, out_channels)) # initialize weights for each subject\n","        self.weights.data *= 1 / in_channels**0.5 # normalize weights\n","\n","    def forward(self, x, n_subjects):\n","        _, C, D = self.weights.shape\n","        subject_weights = self.weights.gather(0, n_subjects.view(-1, 1, 1).expand(-1, C, D)) # select the appropriate weights for each subject in the batch\n","        transformed_eeg = torch.einsum(\"bct,bcd->bdt\", x, subject_weights) # apply the subject-specific transformations\n","\n","        return transformed_eeg\n","\n","class ScaledEmbedding(nn.Module): # assign a unique vector to each subject (similar to positional embedding)\n","\n","    def __init__(self, n_subjects, embedding_dim, scale):\n","\n","        super().__init__()\n","        self.embedding = nn.Embedding(n_subjects, embedding_dim)\n","        self.embedding.weight.data /= scale\n","        self.scale = scale\n","\n","    def forward(self, x):\n","        scaled_embedding = self.embedding(x) * self.scale\n","        return scaled_embedding\n","\n","class LayerScale(nn.Module):\n","    def __init__(self, channels, init = 0.1, boost = 5.):\n","        super().__init__()\n","        self.scale = nn.Parameter(torch.zeros(channels, requires_grad=True))\n","        self.scale.data[:] = init / boost\n","        self.boost = boost\n","\n","    def forward(self, x):\n","        return (self.boost * self.scale[:, None]) * x\n","\n","class ConvSequence(nn.Module):\n","\n","    def __init__(self,\n","                 channels = [16, 32, 64, 128],\n","                 kernel = 4, dilation_growth = 1, dilation_period = None,\n","                 stride = 2, dropout = 0.0, leakiness = 0.0,\n","                 groups = 1, decode = False, batch_norm = False,\n","                 dropout_input = 0, skip = False, scale = None,\n","                 rewrite = False, activation_on_last = True,\n","                 post_skip = False, glu = 0, glu_context = 0,\n","                 glu_glu = True, activation = None):\n","\n","        super().__init__()\n","        dilation = 1\n","        channels = tuple(channels)\n","        self.skip = skip\n","        self.sequence = nn.ModuleList()\n","        self.glus = nn.ModuleList()\n","        if activation is None:\n","            activation = partial(nn.LeakyReLU, leakiness)\n","        Conv = nn.Conv1d if not decode else nn.ConvTranspose1d\n","        # build layers\n","        for k, (chin, chout) in enumerate(zip(channels[:-1], channels[1:])):\n","            layers: tp.List[nn.Module] = []\n","            is_last = k == len(channels) - 2\n","\n","            # Set dropout for the input of the conv sequence if defined\n","            if k == 0 and dropout_input:\n","                assert 0 < dropout_input < 1\n","                layers.append(nn.Dropout(dropout_input))\n","\n","            # conv layer\n","            if dilation_growth > 1:\n","                assert kernel % 2 != 0 # supports only odd kernel with dilation\n","            if dilation_period and (k % dilation_period) == 0:\n","                dilation = 1\n","            pad = kernel // 2 * dilation\n","            layers.append(Conv(chin, chout, kernel, stride, pad,\n","                               dilation=dilation, groups=groups if k > 0 else 1))\n","            dilation *= dilation_growth\n","            # non-linearity\n","            if activation_on_last or not is_last:\n","                if batch_norm:\n","                    layers.append(nn.BatchNorm1d(num_features=chout))\n","                layers.append(activation())\n","                if dropout:\n","                    layers.append(nn.Dropout(dropout))\n","                if rewrite:\n","                    layers += [nn.Conv1d(chout, chout, 1), nn.LeakyReLU(leakiness)]\n","            if chin == chout and skip:\n","                if scale is not None:\n","                    layers.append(LayerScale(chout, scale))\n","                if post_skip:\n","                    layers.append(Conv(chout, chout, 1, groups=chout, bias=False))\n","\n","            self.sequence.append(nn.Sequential(*layers))\n","            if glu and (k + 1) % glu == 0:\n","                ch = 2 * chout if glu_glu else chout\n","                act = nn.GLU(dim=1) if glu_glu else activation()\n","                self.glus.append(\n","                    nn.Sequential(\n","                        nn.Conv1d(chout, ch, 1 + 2 * glu_context, padding=glu_context), act))\n","            else:\n","                self.glus.append(None)\n","\n","    def forward(self, x):\n","        for module_idx, module in enumerate(self.sequence):\n","            old_x = x\n","            x = module(x)\n","            if self.skip and x.shape == old_x.shape:\n","                x = x + old_x\n","            glu = self.glus[module_idx]\n","            if glu is not None:\n","                x = glu(x)\n","        return x\n","\n","class Attention(nn.Module): # scaled dot product with relative position encoding and local attention\n","    def __init__(self, channels, radius = 50, heads = 4):\n","        super().__init__()\n","        assert channels % heads == 0\n","        self.content = nn.Conv1d(channels, channels, 1)\n","        self.query = nn.Conv1d(channels, channels, 1)\n","        self.key = nn.Conv1d(channels, channels, 1)\n","        self.embedding = nn.Embedding(radius * 2 + 1, channels // heads)\n","        weight = self.embedding.weight.data\n","        weight[:] = weight.cumsum(0) / torch.arange(1, len(weight) + 1).float().view(-1, 1).sqrt()\n","        self.heads = heads\n","        self.radius = radius\n","        self.bn = nn.BatchNorm1d(channels)\n","        self.fc = nn.Conv1d(channels, channels, 1)\n","        self.scale = nn.Parameter(torch.full([channels], 0.1))\n","\n","    def forward(self, x):\n","\n","        def _split(y):\n","            return y.view(y.shape[0], self.heads, -1, y.shape[2])\n","\n","        content = _split(self.content(x))\n","        query = _split(self.query(x))\n","        key = _split(self.key(x))\n","\n","        batch_size, _, dim, length = content.shape\n","\n","        dots = torch.einsum(\"bhct,bhcs->bhts\", query, key) # first index `t` is query, second index `s` is key.\n","\n","        steps = torch.arange(length, device=x.device)\n","        relative = (steps[:, None] - steps[None, :])\n","        embs = self.embedding.weight.gather(0, self.radius + relative.clamp_(-self.radius, self.radius).view(-1, 1).expand(-1, dim))\n","        embs = embs.view(length, length, -1)\n","        dots += 0.3 * torch.einsum(\"bhct,tsc->bhts\", query, embs)\n","        dots = torch.where(\n","            relative.abs() <= self.radius, dots, torch.tensor(-float('inf')).to(embs))\n","\n","        weights = torch.softmax(dots, dim=-1)\n","        out = torch.einsum(\"bhts,bhcs->bhct\", weights, content)\n","        out += 0.3 * torch.einsum(\"bhts,tsc->bhct\", weights, embs)\n","        out = out.reshape(batch_size, -1, length)\n","        out = F.relu(self.bn(self.fc(out))) * self.scale.view(1, -1, 1)\n","        return out\n","\n","\"\"\"\n","# TEST 1\n","def test_subject_layers():\n","    batch_size = 2\n","    in_channels = 3\n","    out_channels = 2\n","    n_subjects = 1\n","    time_steps = 2\n","    eeg_data = torch.randn(batch_size, in_channels, time_steps)\n","    print(eeg_data)\n","    subjects = torch.randint(0, n_subjects, (batch_size,))\n","    print(subjects)\n","    subject_layers = SubjectLayers(in_channels, out_channels, n_subjects)\n","    output = subject_layers(eeg_data, subjects)\n","    expected_shape = (batch_size, out_channels, time_steps)\n","    assert output.shape == expected_shape, f\"Output shape mismatch: expected {expected_shape}, got {output.shape}\"\n","test_subject_layers()\n","\n","# TEST 2\n","def scaled_embedding():\n","  n_subjects = 10\n","  embedding_dim = 5\n","  scale = 10\n","  scaled_embedding = ScaledEmbedding(n_subjects, embedding_dim, scale)\n","  subject_indices = torch.tensor([1, 2])\n","  embeddings = scaled_embedding(subject_indices)\n","  print(embeddings)\n","scaled_embedding()\n","\n","# TEST 3\n","conv_sequence = ConvSequence(channels=[16, 32, 64, 128])\n","input_tensor = torch.randn(1, 16, 50) # (Batch Size, Channels, Length)\n","output = conv_sequence(input_tensor)\n","output.shape\n","#summary(conv_sequence, input_size=(16, 50))\n","\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":174},"executionInfo":{"status":"ok","timestamp":1702067439382,"user_tz":300,"elapsed":110,"user":{"displayName":"Kateryna Shapovalenko","userId":"16393911302329786226"}},"outputId":"ea5d75a9-dbe1-4a5b-c9af-ccece92fd670","id":"C6pBO73b8K5o"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n# TEST 1\\ndef test_subject_layers():\\n    batch_size = 2\\n    in_channels = 3\\n    out_channels = 2\\n    n_subjects = 1\\n    time_steps = 2\\n    eeg_data = torch.randn(batch_size, in_channels, time_steps) \\n    print(eeg_data)\\n    subjects = torch.randint(0, n_subjects, (batch_size,))\\n    print(subjects)\\n    subject_layers = SubjectLayers(in_channels, out_channels, n_subjects)\\n    output = subject_layers(eeg_data, subjects)\\n    expected_shape = (batch_size, out_channels, time_steps)\\n    assert output.shape == expected_shape, f\"Output shape mismatch: expected {expected_shape}, got {output.shape}\"\\ntest_subject_layers()\\n\\n# TEST 2\\ndef scaled_embedding():\\n  n_subjects = 10\\n  embedding_dim = 5    \\n  scale = 10      \\n  scaled_embedding = ScaledEmbedding(n_subjects, embedding_dim, scale)\\n  subject_indices = torch.tensor([1, 2]) \\n  embeddings = scaled_embedding(subject_indices)\\n  print(embeddings)\\nscaled_embedding()\\n\\n# TEST 3\\nconv_sequence = ConvSequence(channels=[16, 32, 64, 128])\\ninput_tensor = torch.randn(1, 16, 50) # (Batch Size, Channels, Length)\\noutput = conv_sequence(input_tensor)\\noutput.shape\\n#summary(conv_sequence, input_size=(16, 50))\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":73}]},{"cell_type":"code","source":["class EEG_Encoder(nn.Module):\n","\n","    def __init__(self,\n","\n","                 in_channels = 64, n_subjects = 30,\n","\n","                 # subject layers\n","                 out_channels = 128,\n","\n","                 # conv\n","                 conv_chanels = [16, 32, 64, 128],\n","                 kernel = 4, stride = 2, conv_dropout = 0,\n","                 batch_norm = False, dropout_input = 0,\n","                 leakiness = 0,\n","\n","                 # lstm\n","                 hidden_size = 128, lstm_layers = 4, lstm_dropout = 0.1,\n","\n","                 # attention\n","                 attention_heads = 4, subject_dim = 64, embedding_scale = 1.0):\n","\n","        super().__init__()\n","\n","        # subject layers (normalization across subjects)\n","        self.subject_layers = SubjectLayers(in_channels, out_channels, n_subjects)\n","\n","        # scaled embedding (optional)\n","        # self.subject_embedding = ScaledEmbedding(n_subjects, subject_dim, embedding_scale)\n","\n","        # convsequence\n","        self.convs = ConvSequence(channels=conv_chanels, kernel = kernel_size,\n","                                  stride = stride, dropout = conv_dropout,\n","                                  batch_norm = batch_norm, leakiness = leakiness,\n","                                  dropout_input = dropout_input)\n","\n","        # lstm\n","        self.lstm = nn.LSTM(input_size    = out_channels,\n","                            hidden_size   = hidden_size//2,\n","                            num_layers    = lstm_layers,\n","                            dropout       = lstm_dropout,\n","                            bidirectional = True,\n","                            batch_first    = True)\n","\n","        # attention\n","        self.attention = Attention(hidden_size, heads=attention_heads)\n","\n","        # final linear layer\n","        self.finalconv1 = nn.Conv1d(hidden_size, out_channels, kernel_size = 1)\n","\n","    def forward(self, eeg_inputs): # to pass as an additional paramater n_subjects=10\n","\n","        # subject layers\n","        # subject_indices = torch.arange(n_subjects).to(eeg_inputs.device)\n","        # normalized_eeg_inputs = self.subject_layers(eeg_inputs, subject_indices)\n","\n","        # scaled embedding // scaled_embedding = self.subject_embedding(subject_indices)\n","\n","        # convsequence\n","        print('eeg_inputs before convs', eeg_inputs.shape)\n","        out = self.convs(eeg_inputs)\n","        print('eeg_inputs after convs', out.shape)\n","        print(\"After convs:\", torch.isnan(out).any())\n","\n","        # lstm\n","        out = out.permute(2, 0, 1)\n","        out, _ = self.lstm(out)\n","        out = out.permute(1, 2, 0)\n","        print(\"After LSTM:\", torch.isnan(out).any())\n","\n","        # attention\n","        out = out + self.attention(out)\n","        print(\"After Attention:\", torch.isnan(out).any())\n","\n","        # decoder\n","        # out = self.decoder(out)\n","\n","        # final\n","        out = self.finalconv1(out)\n","        print(\"Final Output:\", torch.isnan(out).any())\n","\n","        return out\n","\n","# TEST\n","in_channels = 64\n","n_subjects = 30\n","out_channels = 128\n","conv_channels = [64, 32, 64, 128]\n","kernel_size = 4\n","stride = 2\n","conv_dropout = 0\n","batch_norm = False\n","dropout_input = 0\n","leakiness = 0\n","hidden_size = 128\n","lstm_layers = 4\n","lstm_dropout = 0.1\n","attention_heads = 4\n","subject_dim = 64\n","embedding_scale = 1.0\n","\n","eeg_encoder = EEG_Encoder(in_channels, n_subjects, out_channels, conv_channels, kernel_size, stride,\n","                          conv_dropout, batch_norm, dropout_input, leakiness, hidden_size, lstm_layers,\n","                          lstm_dropout, attention_heads, subject_dim, embedding_scale)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","eeg_encoder.to(device)\n","batch_size = 2\n","in_channels = 64\n","sequence_length = 128\n","x_sample = torch.rand(batch_size, in_channels, sequence_length)\n","summary(eeg_encoder, x_sample.to(device))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1702067441217,"user_tz":300,"elapsed":251,"user":{"displayName":"Kateryna Shapovalenko","userId":"16393911302329786226"}},"outputId":"73430680-9d8e-49ab-c211-1c18cff3cc90","id":"Zdm4QK5E8K5p"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["eeg_inputs before convs torch.Size([2, 64, 128])\n","eeg_inputs after convs torch.Size([2, 128, 17])\n","After convs: tensor(False)\n","After LSTM: tensor(False)\n","After Attention: tensor(False)\n","Final Output: tensor(False)\n","================================================================================\n","                                 Kernel Shape  Output Shape    Params  \\\n","Layer                                                                   \n","0_convs.sequence.0.Conv1d_0       [64, 32, 4]   [2, 32, 65]    8.224k   \n","1_convs.sequence.0.LeakyReLU_1              -   [2, 32, 65]         -   \n","2_convs.sequence.1.Conv1d_0       [32, 64, 4]   [2, 64, 33]    8.256k   \n","3_convs.sequence.1.LeakyReLU_1              -   [2, 64, 33]         -   \n","4_convs.sequence.2.Conv1d_0      [64, 128, 4]  [2, 128, 17]   32.896k   \n","5_convs.sequence.2.LeakyReLU_1              -  [2, 128, 17]         -   \n","6_lstm                                      -  [17, 2, 128]  397.312k   \n","7_attention.Conv1d_content      [128, 128, 1]  [2, 128, 17]   16.512k   \n","8_attention.Conv1d_query        [128, 128, 1]  [2, 128, 17]   16.512k   \n","9_attention.Conv1d_key          [128, 128, 1]  [2, 128, 17]   16.512k   \n","10_attention.Conv1d_fc          [128, 128, 1]  [2, 128, 17]   16.512k   \n","11_attention.BatchNorm1d_bn             [128]  [2, 128, 17]     256.0   \n","12_finalconv1                   [128, 128, 1]  [2, 128, 17]   16.512k   \n","\n","                               Mult-Adds  \n","Layer                                     \n","0_convs.sequence.0.Conv1d_0      532.48k  \n","1_convs.sequence.0.LeakyReLU_1         -  \n","2_convs.sequence.1.Conv1d_0     270.336k  \n","3_convs.sequence.1.LeakyReLU_1         -  \n","4_convs.sequence.2.Conv1d_0     557.056k  \n","5_convs.sequence.2.LeakyReLU_1         -  \n","6_lstm                          393.216k  \n","7_attention.Conv1d_content      278.528k  \n","8_attention.Conv1d_query        278.528k  \n","9_attention.Conv1d_key          278.528k  \n","10_attention.Conv1d_fc          278.528k  \n","11_attention.BatchNorm1d_bn        128.0  \n","12_finalconv1                   278.528k  \n","--------------------------------------------------------------------------------\n","                         Totals\n","Total params           529.504k\n","Trainable params       529.504k\n","Non-trainable params        0.0\n","Mult-Adds             3.145856M\n","================================================================================\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchsummaryX/torchsummaryX.py:101: FutureWarning: The default value of numeric_only in DataFrame.sum is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n","  df_sum = df.sum()\n"]},{"output_type":"execute_result","data":{"text/plain":["                                 Kernel Shape  Output Shape    Params  \\\n","Layer                                                                   \n","0_convs.sequence.0.Conv1d_0       [64, 32, 4]   [2, 32, 65]    8224.0   \n","1_convs.sequence.0.LeakyReLU_1              -   [2, 32, 65]       NaN   \n","2_convs.sequence.1.Conv1d_0       [32, 64, 4]   [2, 64, 33]    8256.0   \n","3_convs.sequence.1.LeakyReLU_1              -   [2, 64, 33]       NaN   \n","4_convs.sequence.2.Conv1d_0      [64, 128, 4]  [2, 128, 17]   32896.0   \n","5_convs.sequence.2.LeakyReLU_1              -  [2, 128, 17]       NaN   \n","6_lstm                                      -  [17, 2, 128]  397312.0   \n","7_attention.Conv1d_content      [128, 128, 1]  [2, 128, 17]   16512.0   \n","8_attention.Conv1d_query        [128, 128, 1]  [2, 128, 17]   16512.0   \n","9_attention.Conv1d_key          [128, 128, 1]  [2, 128, 17]   16512.0   \n","10_attention.Conv1d_fc          [128, 128, 1]  [2, 128, 17]   16512.0   \n","11_attention.BatchNorm1d_bn             [128]  [2, 128, 17]     256.0   \n","12_finalconv1                   [128, 128, 1]  [2, 128, 17]   16512.0   \n","\n","                                Mult-Adds  \n","Layer                                      \n","0_convs.sequence.0.Conv1d_0      532480.0  \n","1_convs.sequence.0.LeakyReLU_1        NaN  \n","2_convs.sequence.1.Conv1d_0      270336.0  \n","3_convs.sequence.1.LeakyReLU_1        NaN  \n","4_convs.sequence.2.Conv1d_0      557056.0  \n","5_convs.sequence.2.LeakyReLU_1        NaN  \n","6_lstm                           393216.0  \n","7_attention.Conv1d_content       278528.0  \n","8_attention.Conv1d_query         278528.0  \n","9_attention.Conv1d_key           278528.0  \n","10_attention.Conv1d_fc           278528.0  \n","11_attention.BatchNorm1d_bn         128.0  \n","12_finalconv1                    278528.0  "],"text/html":["\n","  <div id=\"df-ee881caf-4e35-4964-bc90-9dff920db6bb\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Kernel Shape</th>\n","      <th>Output Shape</th>\n","      <th>Params</th>\n","      <th>Mult-Adds</th>\n","    </tr>\n","    <tr>\n","      <th>Layer</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0_convs.sequence.0.Conv1d_0</th>\n","      <td>[64, 32, 4]</td>\n","      <td>[2, 32, 65]</td>\n","      <td>8224.0</td>\n","      <td>532480.0</td>\n","    </tr>\n","    <tr>\n","      <th>1_convs.sequence.0.LeakyReLU_1</th>\n","      <td>-</td>\n","      <td>[2, 32, 65]</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2_convs.sequence.1.Conv1d_0</th>\n","      <td>[32, 64, 4]</td>\n","      <td>[2, 64, 33]</td>\n","      <td>8256.0</td>\n","      <td>270336.0</td>\n","    </tr>\n","    <tr>\n","      <th>3_convs.sequence.1.LeakyReLU_1</th>\n","      <td>-</td>\n","      <td>[2, 64, 33]</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4_convs.sequence.2.Conv1d_0</th>\n","      <td>[64, 128, 4]</td>\n","      <td>[2, 128, 17]</td>\n","      <td>32896.0</td>\n","      <td>557056.0</td>\n","    </tr>\n","    <tr>\n","      <th>5_convs.sequence.2.LeakyReLU_1</th>\n","      <td>-</td>\n","      <td>[2, 128, 17]</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>6_lstm</th>\n","      <td>-</td>\n","      <td>[17, 2, 128]</td>\n","      <td>397312.0</td>\n","      <td>393216.0</td>\n","    </tr>\n","    <tr>\n","      <th>7_attention.Conv1d_content</th>\n","      <td>[128, 128, 1]</td>\n","      <td>[2, 128, 17]</td>\n","      <td>16512.0</td>\n","      <td>278528.0</td>\n","    </tr>\n","    <tr>\n","      <th>8_attention.Conv1d_query</th>\n","      <td>[128, 128, 1]</td>\n","      <td>[2, 128, 17]</td>\n","      <td>16512.0</td>\n","      <td>278528.0</td>\n","    </tr>\n","    <tr>\n","      <th>9_attention.Conv1d_key</th>\n","      <td>[128, 128, 1]</td>\n","      <td>[2, 128, 17]</td>\n","      <td>16512.0</td>\n","      <td>278528.0</td>\n","    </tr>\n","    <tr>\n","      <th>10_attention.Conv1d_fc</th>\n","      <td>[128, 128, 1]</td>\n","      <td>[2, 128, 17]</td>\n","      <td>16512.0</td>\n","      <td>278528.0</td>\n","    </tr>\n","    <tr>\n","      <th>11_attention.BatchNorm1d_bn</th>\n","      <td>[128]</td>\n","      <td>[2, 128, 17]</td>\n","      <td>256.0</td>\n","      <td>128.0</td>\n","    </tr>\n","    <tr>\n","      <th>12_finalconv1</th>\n","      <td>[128, 128, 1]</td>\n","      <td>[2, 128, 17]</td>\n","      <td>16512.0</td>\n","      <td>278528.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ee881caf-4e35-4964-bc90-9dff920db6bb')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-ee881caf-4e35-4964-bc90-9dff920db6bb button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-ee881caf-4e35-4964-bc90-9dff920db6bb');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-5b819eb1-924b-4660-8020-962d87050f61\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5b819eb1-924b-4660-8020-962d87050f61')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-5b819eb1-924b-4660-8020-962d87050f61 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":74}]},{"cell_type":"markdown","source":["### audio encoder (replicated meta's deepmel)"],"metadata":{"id":"xMz2kKoj8K5p"}},{"cell_type":"code","source":["# Speech model\n","\n","class LayerScale(nn.Module):\n","    def __init__(self, channels: int, init: float = 0.1, boost: float = 5.):\n","        super().__init__()\n","        self.scale = nn.Parameter(torch.zeros(channels, requires_grad=True))\n","        self.scale.data[:] = init / boost\n","        self.boost = boost\n","\n","    def forward(self, x):\n","        return (self.boost * self.scale[:, None]) * x\n","\n","class ConvSequence(nn.Module):\n","\n","    def __init__(self,\n","                 channels: tp.Sequence[int],\n","                 kernel: int = 4,\n","                 dilation_growth: int = 1,\n","                 dilation_period: tp.Optional[int] = None,\n","                 stride: int = 2,\n","                 dropout: float = 0.0,\n","                 leakiness: float = 0.0,\n","                 groups: int = 1,\n","                 decode: bool = False,\n","                 batch_norm: bool = False,\n","                 dropout_input: float = 0,\n","                 skip: bool = False,\n","                 scale: tp.Optional[float] = None,\n","                 rewrite: bool = False,\n","                 activation_on_last: bool = True,\n","                 post_skip: bool = False,\n","                 glu: int = 0,\n","                 glu_context: int = 0,\n","                 glu_glu: bool = True,\n","                 activation: tp.Any = None) -> None:\n","        super().__init__()\n","        dilation = 1\n","        channels = tuple(channels)\n","        self.skip = skip\n","        self.sequence = nn.ModuleList()\n","        self.glus = nn.ModuleList()\n","        if activation is None:\n","            activation = partial(nn.LeakyReLU, leakiness)\n","        Conv = nn.Conv1d if not decode else nn.ConvTranspose1d\n","        # build layers\n","        for k, (chin, chout) in enumerate(zip(channels[:-1], channels[1:])):\n","            layers: tp.List[nn.Module] = []\n","            is_last = k == len(channels) - 2\n","\n","            # Set dropout for the input of the conv sequence if defined\n","            if k == 0 and dropout_input:\n","                assert 0 < dropout_input < 1\n","                layers.append(nn.Dropout(dropout_input))\n","\n","            # conv layer\n","            if dilation_growth > 1:\n","                assert kernel % 2 != 0, \"Supports only odd kernel with dilation for now\"\n","            if dilation_period and (k % dilation_period) == 0:\n","                dilation = 1\n","            pad = kernel // 2 * dilation\n","            layers.append(Conv(chin, chout, kernel, stride, pad,\n","                               dilation=dilation, groups=groups if k > 0 else 1))\n","            dilation *= dilation_growth\n","            # non-linearity\n","            if activation_on_last or not is_last:\n","                if batch_norm:\n","                    layers.append(nn.BatchNorm1d(num_features=chout))\n","                layers.append(activation())\n","                if dropout:\n","                    layers.append(nn.Dropout(dropout))\n","                if rewrite:\n","                    layers += [nn.Conv1d(chout, chout, 1), nn.LeakyReLU(leakiness)]\n","                    # layers += [nn.Conv1d(chout, 2 * chout, 1), nn.GLU(dim=1)]\n","            if chin == chout and skip:\n","                if scale is not None:\n","                    layers.append(LayerScale(chout, scale))\n","                if post_skip:\n","                    layers.append(Conv(chout, chout, 1, groups=chout, bias=False))\n","\n","            self.sequence.append(nn.Sequential(*layers))\n","            if glu and (k + 1) % glu == 0:\n","                ch = 2 * chout if glu_glu else chout\n","                act = nn.GLU(dim=1) if glu_glu else activation()\n","                self.glus.append(\n","                    nn.Sequential(\n","                        nn.Conv1d(chout, ch, 1 + 2 * glu_context, padding=glu_context), act))\n","            else:\n","                self.glus.append(None)\n","\n","    def forward(self, x: tp.Any) -> tp.Any:\n","        for module_idx, module in enumerate(self.sequence):\n","            old_x = x\n","            x = module(x)\n","            if self.skip and x.shape == old_x.shape:\n","                x = x + old_x\n","            glu = self.glus[module_idx]\n","            if glu is not None:\n","                x = glu(x)\n","        return x\n","\n","class DeepMel(ConvSequence):\n","    \"\"\"DeepMel model that extracts features from the Mel spectrogram.\n","\n","    Parameters\n","    ----------\n","    n_in_channels :\n","        Number of input channels.\n","    n_hidden_channels :\n","        Number of channels in hidden layers.\n","    n_hidden_layers :\n","        Number of hidden layers.\n","    n_out_channels :\n","        Number of output channels.\n","    kwargs:\n","        Additional keyword arguments to pass to ConvSequence.\n","    \"\"\"\n","    def __init__(self,\n","                 n_in_channels: int,\n","                 n_hidden_channels: int,\n","                 n_hidden_layers: int,\n","                 n_out_channels: int, **kwargs):\n","\n","        channels = \\\n","            [n_in_channels] + [n_hidden_channels] * (n_hidden_layers - 1) + [n_out_channels]\n","\n","        super().__init__(channels, **kwargs)\n","\n","# Test the model\n","n_in_channels = 16\n","n_hidden_channels = 32\n","n_hidden_layers = 3\n","n_out_channels = 64\n","model = DeepMel(n_in_channels, n_hidden_channels, n_hidden_layers, n_out_channels)\n","input_size = (n_in_channels, 128)  # Example input size (channels, sequence length)\n","summary(model, input_size=input_size, device=\"cpu\")"],"metadata":{"id":"86EOmCRk8K5p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### brain-audio model (replicated meta)"],"metadata":{"id":"aUtputBv8W0J"}},{"cell_type":"code","source":["class BrainAudioModel(torch.nn.Module):\n","\n","  def __init__(# brain encoding\n","               self, in_channels, model_chout\n","\n","               # audio encoding\n","\n","               # brain to audio probabilities\n","\n","               ):\n","\n","    super().__init__()\n","\n","    # brain encoding\n","    self.brain_encoding = EEG_Encoder(in_channels=in_channels,\n","                                      n_subjects=30,\n","                                      out_channels=model_chout,\n","                                      conv_chanels=[64, 32, 64, 128],\n","                                      kernel=4,\n","                                      stride=2,\n","                                      conv_dropout=0,\n","                                      batch_norm=False,\n","                                      dropout_input=0,\n","                                      leakiness=0,\n","                                      hidden_size=128,\n","                                      lstm_layers=4,\n","                                      lstm_dropout=0.1,\n","                                      attention_heads=4,\n","                                      subject_dim=64,\n","                                      embedding_scale=1.0)\n","\n","    # audio encoding\n","    #self.audio_encoding = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\") # https://huggingface.co/facebook/wav2vec2-base-960h\n","\n","    # brain to audio probabilities\n","    self.clip = ClipLoss(linear=None, twin=True, pool=False, tmin=None, tmax=None,\n","                         tmin_train=None, tmax_train=None, dset_args=None, center=False)\n","\n","    # linear layers to match the dimensions\n","    self.linear1 = nn.Linear(128, model_chout)\n","\n","  \"\"\"\n","  def freeze_audio_embedding(self): # we need to call it after initializing the model model.freeze_audio_embedding()\n","    for param in self.audio_embedding.parameters():\n","      param.requires_grad = False\n","  \"\"\"\n","\n","  def forward(self, raw_eeg_inputs, audio_hidden_states):  # raw_audio_inputs\n","\n","    # brain encoding\n","    brain_encoder_outputs = self.brain_encoding(raw_eeg_inputs)\n","    print('brain_encoder_outputs', brain_encoder_outputs.shape)\n","\n","    print('audio_hidden_states', audio_hidden_states.shape)\n","\n","    # audio encoding (optional, could be passed as a parameter to forward)\n","    #audio_encoder_outputs = self.audio_encoding(raw_audio_inputs).last_hidden_state\n","    #audio_encoder_outputs = self.linear(audio_encoder_outputs)\n","    audio_hidden_states_processed = self.linear1(audio_hidden_states)\n","    print('audio_hidden_states_processed', audio_hidden_states_processed.shape)\n","\n","    # brain to audio probabilities\n","    brain_encoder_outputs_reshaped = brain_encoder_outputs.permute(0, 2, 1) # reshaping to [B, C, T]\n","    audio_hidden_states_reshaped = audio_hidden_states_processed.permute(0, 2, 1) # reshaping to [B, C, T]\n","    probabilities = self.clip.get_probabilities(brain_encoder_outputs_reshaped, audio_hidden_states_reshaped)\n","    print('probabilities', probabilities)\n","    return probabilities\n","\n","# TEST\n","in_channels = 64  # EEG input channels\n","model_chout = 17  # output channels\n","model = BrainAudioModel(in_channels, model_chout)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","eeg_input_size = (2, in_channels, 128)  # Batch size, channels, sequence length\n","audio_hidden_state_size = (2, model_chout, 128)  # Assuming the same sequence length and batch size\n","eeg_dummy_input = torch.randn(eeg_input_size).to(device)\n","audio_hidden_state_dummy = torch.randn(audio_hidden_state_size).to(device)\n","summary(model, eeg_dummy_input, audio_hidden_state_dummy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1702067513553,"user_tz":300,"elapsed":286,"user":{"displayName":"Kateryna Shapovalenko","userId":"16393911302329786226"}},"outputId":"bab64d71-d31f-4818-8af3-203df2965033","id":"w1LUMscn8W0J"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["eeg_inputs before convs torch.Size([2, 64, 128])\n","eeg_inputs after convs torch.Size([2, 128, 17])\n","After convs: tensor(False)\n","After LSTM: tensor(True)\n","After Attention: tensor(True)\n","Final Output: tensor(True)\n","brain_encoder_outputs torch.Size([2, 17, 17])\n","audio_hidden_states torch.Size([2, 17, 128])\n","audio_hidden_states_processed torch.Size([2, 17, 17])\n","probabilities tensor([[nan, nan],\n","        [nan, nan]])\n","===============================================================================================\n","                                                Kernel Shape  Output Shape  \\\n","Layer                                                                        \n","0_brain_encoding.convs.sequence.0.Conv1d_0       [64, 32, 4]   [2, 32, 65]   \n","1_brain_encoding.convs.sequence.0.LeakyReLU_1              -   [2, 32, 65]   \n","2_brain_encoding.convs.sequence.1.Conv1d_0       [32, 64, 4]   [2, 64, 33]   \n","3_brain_encoding.convs.sequence.1.LeakyReLU_1              -   [2, 64, 33]   \n","4_brain_encoding.convs.sequence.2.Conv1d_0      [64, 128, 4]  [2, 128, 17]   \n","5_brain_encoding.convs.sequence.2.LeakyReLU_1              -  [2, 128, 17]   \n","6_brain_encoding.LSTM_lstm                                 -  [17, 2, 128]   \n","7_brain_encoding.attention.Conv1d_content      [128, 128, 1]  [2, 128, 17]   \n","8_brain_encoding.attention.Conv1d_query        [128, 128, 1]  [2, 128, 17]   \n","9_brain_encoding.attention.Conv1d_key          [128, 128, 1]  [2, 128, 17]   \n","10_brain_encoding.attention.Conv1d_fc          [128, 128, 1]  [2, 128, 17]   \n","11_brain_encoding.attention.BatchNorm1d_bn             [128]  [2, 128, 17]   \n","12_brain_encoding.Conv1d_finalconv1             [128, 17, 1]   [2, 17, 17]   \n","13_linear1                                         [128, 17]   [2, 17, 17]   \n","\n","                                                Params Mult-Adds  \n","Layer                                                             \n","0_brain_encoding.convs.sequence.0.Conv1d_0      8.224k   532.48k  \n","1_brain_encoding.convs.sequence.0.LeakyReLU_1        -         -  \n","2_brain_encoding.convs.sequence.1.Conv1d_0      8.256k  270.336k  \n","3_brain_encoding.convs.sequence.1.LeakyReLU_1        -         -  \n","4_brain_encoding.convs.sequence.2.Conv1d_0     32.896k  557.056k  \n","5_brain_encoding.convs.sequence.2.LeakyReLU_1        -         -  \n","6_brain_encoding.LSTM_lstm                     340.48k  336.384k  \n","7_brain_encoding.attention.Conv1d_content      16.512k  278.528k  \n","8_brain_encoding.attention.Conv1d_query        16.512k  278.528k  \n","9_brain_encoding.attention.Conv1d_key          16.512k  278.528k  \n","10_brain_encoding.attention.Conv1d_fc          16.512k  278.528k  \n","11_brain_encoding.attention.BatchNorm1d_bn       256.0     128.0  \n","12_brain_encoding.Conv1d_finalconv1             2.193k   36.992k  \n","13_linear1                                      2.193k    2.176k  \n","-----------------------------------------------------------------------------------------------\n","                         Totals\n","Total params           460.546k\n","Trainable params       460.546k\n","Non-trainable params        0.0\n","Mult-Adds             2.849664M\n","===============================================================================================\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchsummaryX/torchsummaryX.py:101: FutureWarning: The default value of numeric_only in DataFrame.sum is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n","  df_sum = df.sum()\n"]},{"output_type":"execute_result","data":{"text/plain":["                                                Kernel Shape  Output Shape  \\\n","Layer                                                                        \n","0_brain_encoding.convs.sequence.0.Conv1d_0       [64, 32, 4]   [2, 32, 65]   \n","1_brain_encoding.convs.sequence.0.LeakyReLU_1              -   [2, 32, 65]   \n","2_brain_encoding.convs.sequence.1.Conv1d_0       [32, 64, 4]   [2, 64, 33]   \n","3_brain_encoding.convs.sequence.1.LeakyReLU_1              -   [2, 64, 33]   \n","4_brain_encoding.convs.sequence.2.Conv1d_0      [64, 128, 4]  [2, 128, 17]   \n","5_brain_encoding.convs.sequence.2.LeakyReLU_1              -  [2, 128, 17]   \n","6_brain_encoding.LSTM_lstm                                 -  [17, 2, 128]   \n","7_brain_encoding.attention.Conv1d_content      [128, 128, 1]  [2, 128, 17]   \n","8_brain_encoding.attention.Conv1d_query        [128, 128, 1]  [2, 128, 17]   \n","9_brain_encoding.attention.Conv1d_key          [128, 128, 1]  [2, 128, 17]   \n","10_brain_encoding.attention.Conv1d_fc          [128, 128, 1]  [2, 128, 17]   \n","11_brain_encoding.attention.BatchNorm1d_bn             [128]  [2, 128, 17]   \n","12_brain_encoding.Conv1d_finalconv1             [128, 17, 1]   [2, 17, 17]   \n","13_linear1                                         [128, 17]   [2, 17, 17]   \n","\n","                                                 Params  Mult-Adds  \n","Layer                                                               \n","0_brain_encoding.convs.sequence.0.Conv1d_0       8224.0   532480.0  \n","1_brain_encoding.convs.sequence.0.LeakyReLU_1       NaN        NaN  \n","2_brain_encoding.convs.sequence.1.Conv1d_0       8256.0   270336.0  \n","3_brain_encoding.convs.sequence.1.LeakyReLU_1       NaN        NaN  \n","4_brain_encoding.convs.sequence.2.Conv1d_0      32896.0   557056.0  \n","5_brain_encoding.convs.sequence.2.LeakyReLU_1       NaN        NaN  \n","6_brain_encoding.LSTM_lstm                     340480.0   336384.0  \n","7_brain_encoding.attention.Conv1d_content       16512.0   278528.0  \n","8_brain_encoding.attention.Conv1d_query         16512.0   278528.0  \n","9_brain_encoding.attention.Conv1d_key           16512.0   278528.0  \n","10_brain_encoding.attention.Conv1d_fc           16512.0   278528.0  \n","11_brain_encoding.attention.BatchNorm1d_bn        256.0      128.0  \n","12_brain_encoding.Conv1d_finalconv1              2193.0    36992.0  \n","13_linear1                                       2193.0     2176.0  "],"text/html":["\n","  <div id=\"df-a0484a86-5d3d-40dd-956b-5f21810ef836\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Kernel Shape</th>\n","      <th>Output Shape</th>\n","      <th>Params</th>\n","      <th>Mult-Adds</th>\n","    </tr>\n","    <tr>\n","      <th>Layer</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0_brain_encoding.convs.sequence.0.Conv1d_0</th>\n","      <td>[64, 32, 4]</td>\n","      <td>[2, 32, 65]</td>\n","      <td>8224.0</td>\n","      <td>532480.0</td>\n","    </tr>\n","    <tr>\n","      <th>1_brain_encoding.convs.sequence.0.LeakyReLU_1</th>\n","      <td>-</td>\n","      <td>[2, 32, 65]</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2_brain_encoding.convs.sequence.1.Conv1d_0</th>\n","      <td>[32, 64, 4]</td>\n","      <td>[2, 64, 33]</td>\n","      <td>8256.0</td>\n","      <td>270336.0</td>\n","    </tr>\n","    <tr>\n","      <th>3_brain_encoding.convs.sequence.1.LeakyReLU_1</th>\n","      <td>-</td>\n","      <td>[2, 64, 33]</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4_brain_encoding.convs.sequence.2.Conv1d_0</th>\n","      <td>[64, 128, 4]</td>\n","      <td>[2, 128, 17]</td>\n","      <td>32896.0</td>\n","      <td>557056.0</td>\n","    </tr>\n","    <tr>\n","      <th>5_brain_encoding.convs.sequence.2.LeakyReLU_1</th>\n","      <td>-</td>\n","      <td>[2, 128, 17]</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>6_brain_encoding.LSTM_lstm</th>\n","      <td>-</td>\n","      <td>[17, 2, 128]</td>\n","      <td>340480.0</td>\n","      <td>336384.0</td>\n","    </tr>\n","    <tr>\n","      <th>7_brain_encoding.attention.Conv1d_content</th>\n","      <td>[128, 128, 1]</td>\n","      <td>[2, 128, 17]</td>\n","      <td>16512.0</td>\n","      <td>278528.0</td>\n","    </tr>\n","    <tr>\n","      <th>8_brain_encoding.attention.Conv1d_query</th>\n","      <td>[128, 128, 1]</td>\n","      <td>[2, 128, 17]</td>\n","      <td>16512.0</td>\n","      <td>278528.0</td>\n","    </tr>\n","    <tr>\n","      <th>9_brain_encoding.attention.Conv1d_key</th>\n","      <td>[128, 128, 1]</td>\n","      <td>[2, 128, 17]</td>\n","      <td>16512.0</td>\n","      <td>278528.0</td>\n","    </tr>\n","    <tr>\n","      <th>10_brain_encoding.attention.Conv1d_fc</th>\n","      <td>[128, 128, 1]</td>\n","      <td>[2, 128, 17]</td>\n","      <td>16512.0</td>\n","      <td>278528.0</td>\n","    </tr>\n","    <tr>\n","      <th>11_brain_encoding.attention.BatchNorm1d_bn</th>\n","      <td>[128]</td>\n","      <td>[2, 128, 17]</td>\n","      <td>256.0</td>\n","      <td>128.0</td>\n","    </tr>\n","    <tr>\n","      <th>12_brain_encoding.Conv1d_finalconv1</th>\n","      <td>[128, 17, 1]</td>\n","      <td>[2, 17, 17]</td>\n","      <td>2193.0</td>\n","      <td>36992.0</td>\n","    </tr>\n","    <tr>\n","      <th>13_linear1</th>\n","      <td>[128, 17]</td>\n","      <td>[2, 17, 17]</td>\n","      <td>2193.0</td>\n","      <td>2176.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a0484a86-5d3d-40dd-956b-5f21810ef836')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-a0484a86-5d3d-40dd-956b-5f21810ef836 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-a0484a86-5d3d-40dd-956b-5f21810ef836');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-df67106a-ff31-4cf0-8ad2-2a87d80b0c3d\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-df67106a-ff31-4cf0-8ad2-2a87d80b0c3d')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-df67106a-ff31-4cf0-8ad2-2a87d80b0c3d button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":77}]},{"cell_type":"markdown","source":["## brain encoder (our: cnn + lstm + transformer)"],"metadata":{"id":"c0TqyTlnM7uJ"}},{"cell_type":"code","source":["class PermuteBlock(torch.nn.Module):\n","\n","    def forward(self, x):\n","        return x.transpose(1, 2)\n","\n","class PositionalEncoding(nn.Module):\n","\n","    def __init__(self, input_size, max_input_seq_len, dropout):\n","\n","        super(PositionalEncoding, self).__init__()\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","        pe = torch.zeros(max_input_seq_len, input_size)\n","        position = torch.arange(0, max_input_seq_len, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, input_size, 2).float() * (-torch.log(torch.tensor(10000.0)) / input_size))\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0)\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","\n","        x = x + self.pe[:, :x.size(1), :]\n","        x = self.dropout(x)\n","\n","        return x\n","\n","# Transformer  --------------------------------------------------------------------\n","\n","class TransformerEncoder(torch.nn.Module):\n","\n","    def __init__(self, tf_input_size, num_heads, dropout):\n","\n","        super().__init__()\n","\n","        self.self_attn    = torch.nn.MultiheadAttention(tf_input_size, num_heads)\n","\n","        self.ln           = nn.LayerNorm(tf_input_size)\n","\n","        self.feed_forward = nn.Sequential(nn.Linear(tf_input_size, tf_input_size * 4),\n","                                          nn.ReLU(),\n","                                          nn.Dropout(dropout),\n","                                          nn.Linear(tf_input_size * 4, tf_input_size))\n","\n","        self.feed_forward_simple = nn.Linear(tf_input_size, tf_input_size)\n","\n","    def forward(self, x, mask=None):\n","\n","        # attention\n","        attention_context, attention_weights = self.self_attn(x, x, x, mask)\n","\n","        # residual connection 1\n","        res1 = x + attention_context\n","\n","        # layer norm 1\n","        ln1 = self.ln(res1)\n","\n","        # feed forward\n","        ff = self.feed_forward(ln1)\n","\n","        # residual connection 2\n","        res2 = ln1 + ff\n","\n","        # layer norm 2\n","        ln2 = self.ln(res2)\n","\n","        return ln2\n","\n","# Listener: CNN -> BLSTM -> positional -> transformer -------------------------------------------\n","\n","class EEG_Transformer_Encoder(torch.nn.Module):\n","\n","    def __init__(self, input_size, lstm_layers, listener_hidden_size,\n","                 num_heads_listener, tf_blocks_listener, dropout):\n","\n","        super().__init__()\n","\n","        # embedding 1: cnn\n","        self.cnn = torch.nn.Sequential(\n","\n","            PermuteBlock(),\n","\n","            # conv 1\n","            nn.Conv1d(input_size, listener_hidden_size // 4, kernel_size=2, stride=2),\n","            nn.BatchNorm1d(listener_hidden_size // 4),\n","            nn.GELU(),\n","\n","            # conv 2\n","            nn.Conv1d(listener_hidden_size // 4, listener_hidden_size // 2, kernel_size=2, stride=2),\n","            nn.BatchNorm1d(listener_hidden_size // 2),\n","            nn.GELU(),\n","\n","            # conv 3\n","            nn.Conv1d(listener_hidden_size // 2, listener_hidden_size, kernel_size=2, stride=2),\n","            nn.BatchNorm1d(listener_hidden_size),\n","            nn.GELU(),\n","\n","            PermuteBlock())\n","\n","        # embedding 2: blstm\n","        self.blstm          = nn.LSTM(input_size    = listener_hidden_size,\n","                                      hidden_size   = listener_hidden_size // 2,\n","                                      num_layers    = lstm_layers,\n","                                      batch_first   = True,\n","                                      bidirectional = True,\n","                                      dropout=0)\n","\n","        # embedding 3: positional\n","        self.max_input_seq_len   = 31914 // num_heads_listener\n","        self.positional_encoding = PositionalEncoding(input_size=listener_hidden_size,\n","                                                      max_input_seq_len=self.max_input_seq_len,\n","                                                      dropout=dropout)\n","\n","        # transformer (same input-output shape: tf_input_size --> tf_input_size)\n","        transformer_blocks = []\n","        for i in range(tf_blocks_listener):\n","            transformer_blocks.append(\n","                TransformerEncoder(\n","                    tf_input_size=listener_hidden_size,\n","                    num_heads=num_heads_listener,\n","                    dropout=dropout))\n","        self.transformer_encoder = nn.Sequential(*transformer_blocks)\n","\n","    def forward(self, x, x_len): # x = raw_eeg, x_len = raw_eeg_len\n","\n","        # embedding 1: cnn\n","        x = self.cnn(x)\n","        x_len = torch.clamp(x_len, max=x.shape[1])\n","\n","        # embedding 2: blstm\n","        x_packed                = pack_padded_sequence(x, x_len.cpu(), batch_first=True, enforce_sorted=False)\n","        lstm_out, _             = self.blstm(x_packed)\n","        output, output_lengths  = pad_packed_sequence(lstm_out, batch_first=True)\n","\n","        # embedding 3: positional\n","        output  = self.positional_encoding(output)\n","\n","        # transformer\n","        output  = self.transformer_encoder(output)\n","\n","        return output, output_lengths\n","\n","\n","# TEST\n","def test_transformer_listener():\n","    model = EEG_Transformer_Encoder(\n","        input_size=62,\n","        lstm_layers=1,\n","        listener_hidden_size=512,\n","        num_heads_listener=2,\n","        tf_blocks_listener=2,\n","        dropout=0.0).to(DEVICE)\n","\n","    # Sample input\n","    batch_size = 2\n","    seq_len = 1000\n","    input_dim = 62\n","    x_sample = torch.rand(batch_size, seq_len, input_dim).to(DEVICE)\n","    x_lengths = torch.full((batch_size,), seq_len, dtype=torch.int64).to(DEVICE)\n","\n","    # Forward pass\n","    output, output_lengths = model(x_sample, x_lengths)\n","\n","    # Check output dimensions\n","    assert output.size(0) == batch_size\n","    assert output.size(2) == 512\n","    assert all(length <= seq_len for length in output_lengths)\n","\n","    summary(model, x_sample, x_lengths)\n","    print(\"All tests passed.\")\n","\n","test_transformer_listener()"],"metadata":{"id":"9UopS7ongNNl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702218215531,"user_tz":300,"elapsed":472,"user":{"displayName":"Kateryna Shapovalenko","userId":"16393911302329786226"}},"outputId":"16bda1d8-7ab6-413d-d4e3-23c0c1dee072"},"execution_count":243,"outputs":[{"output_type":"stream","name":"stdout","text":["=====================================================================================================\n","                                                  Kernel Shape  \\\n","Layer                                                            \n","0_cnn.PermuteBlock_0                                         -   \n","1_cnn.Conv1d_1                                    [62, 128, 2]   \n","2_cnn.BatchNorm1d_2                                      [128]   \n","3_cnn.GELU_3                                                 -   \n","4_cnn.Conv1d_4                                   [128, 256, 2]   \n","5_cnn.BatchNorm1d_5                                      [256]   \n","6_cnn.GELU_6                                                 -   \n","7_cnn.Conv1d_7                                   [256, 512, 2]   \n","8_cnn.BatchNorm1d_8                                      [512]   \n","9_cnn.GELU_9                                                 -   \n","10_cnn.PermuteBlock_10                                       -   \n","11_blstm                                                     -   \n","12_positional_encoding.Dropout_dropout                       -   \n","13_transformer_encoder.0.LayerNorm_ln                    [512]   \n","14_transformer_encoder.0.feed_forward.Linear_0     [512, 2048]   \n","15_transformer_encoder.0.feed_forward.ReLU_1                 -   \n","16_transformer_encoder.0.feed_forward.Dropout_2              -   \n","17_transformer_encoder.0.feed_forward.Linear_3     [2048, 512]   \n","18_transformer_encoder.0.LayerNorm_ln                    [512]   \n","19_transformer_encoder.1.LayerNorm_ln                    [512]   \n","20_transformer_encoder.1.feed_forward.Linear_0     [512, 2048]   \n","21_transformer_encoder.1.feed_forward.ReLU_1                 -   \n","22_transformer_encoder.1.feed_forward.Dropout_2              -   \n","23_transformer_encoder.1.feed_forward.Linear_3     [2048, 512]   \n","24_transformer_encoder.1.LayerNorm_ln                    [512]   \n","\n","                                                   Output Shape     Params  \\\n","Layer                                                                        \n","0_cnn.PermuteBlock_0                              [2, 62, 1000]          -   \n","1_cnn.Conv1d_1                                    [2, 128, 500]      16.0k   \n","2_cnn.BatchNorm1d_2                               [2, 128, 500]      256.0   \n","3_cnn.GELU_3                                      [2, 128, 500]          -   \n","4_cnn.Conv1d_4                                    [2, 256, 250]    65.792k   \n","5_cnn.BatchNorm1d_5                               [2, 256, 250]      512.0   \n","6_cnn.GELU_6                                      [2, 256, 250]          -   \n","7_cnn.Conv1d_7                                    [2, 512, 125]   262.656k   \n","8_cnn.BatchNorm1d_8                               [2, 512, 125]     1.024k   \n","9_cnn.GELU_9                                      [2, 512, 125]          -   \n","10_cnn.PermuteBlock_10                            [2, 125, 512]          -   \n","11_blstm                                             [250, 512]   1.57696M   \n","12_positional_encoding.Dropout_dropout            [2, 125, 512]          -   \n","13_transformer_encoder.0.LayerNorm_ln             [2, 125, 512]     1.024k   \n","14_transformer_encoder.0.feed_forward.Linear_0   [2, 125, 2048]  1.050624M   \n","15_transformer_encoder.0.feed_forward.ReLU_1     [2, 125, 2048]          -   \n","16_transformer_encoder.0.feed_forward.Dropout_2  [2, 125, 2048]          -   \n","17_transformer_encoder.0.feed_forward.Linear_3    [2, 125, 512]  1.049088M   \n","18_transformer_encoder.0.LayerNorm_ln             [2, 125, 512]          -   \n","19_transformer_encoder.1.LayerNorm_ln             [2, 125, 512]     1.024k   \n","20_transformer_encoder.1.feed_forward.Linear_0   [2, 125, 2048]  1.050624M   \n","21_transformer_encoder.1.feed_forward.ReLU_1     [2, 125, 2048]          -   \n","22_transformer_encoder.1.feed_forward.Dropout_2  [2, 125, 2048]          -   \n","23_transformer_encoder.1.feed_forward.Linear_3    [2, 125, 512]  1.049088M   \n","24_transformer_encoder.1.LayerNorm_ln             [2, 125, 512]          -   \n","\n","                                                 Mult-Adds  \n","Layer                                                       \n","0_cnn.PermuteBlock_0                                     -  \n","1_cnn.Conv1d_1                                      7.936M  \n","2_cnn.BatchNorm1d_2                                  128.0  \n","3_cnn.GELU_3                                             -  \n","4_cnn.Conv1d_4                                     16.384M  \n","5_cnn.BatchNorm1d_5                                  256.0  \n","6_cnn.GELU_6                                             -  \n","7_cnn.Conv1d_7                                     32.768M  \n","8_cnn.BatchNorm1d_8                                  512.0  \n","9_cnn.GELU_9                                             -  \n","10_cnn.PermuteBlock_10                                   -  \n","11_blstm                                         1.572864M  \n","12_positional_encoding.Dropout_dropout                   -  \n","13_transformer_encoder.0.LayerNorm_ln                512.0  \n","14_transformer_encoder.0.feed_forward.Linear_0   1.048576M  \n","15_transformer_encoder.0.feed_forward.ReLU_1             -  \n","16_transformer_encoder.0.feed_forward.Dropout_2          -  \n","17_transformer_encoder.0.feed_forward.Linear_3   1.048576M  \n","18_transformer_encoder.0.LayerNorm_ln                512.0  \n","19_transformer_encoder.1.LayerNorm_ln                512.0  \n","20_transformer_encoder.1.feed_forward.Linear_0   1.048576M  \n","21_transformer_encoder.1.feed_forward.ReLU_1             -  \n","22_transformer_encoder.1.feed_forward.Dropout_2          -  \n","23_transformer_encoder.1.feed_forward.Linear_3   1.048576M  \n","24_transformer_encoder.1.LayerNorm_ln                512.0  \n","-----------------------------------------------------------------------------------------------------\n","                          Totals\n","Total params           6.124672M\n","Trainable params       6.124672M\n","Non-trainable params         0.0\n","Mult-Adds             62.858112M\n","=====================================================================================================\n","All tests passed.\n"]}]},{"cell_type":"markdown","source":["## clip loss (copied from meta)"],"metadata":{"id":"JfDaVYRosRGc"}},{"cell_type":"code","source":["class ClipLoss(torch.nn.Module):\n","\n","    def __init__(self, linear=None, twin=True, pool=False, tmin=None, tmax=None,\n","                 tmin_train=None, tmax_train=None, dset_args=None, center=False):\n","        super().__init__()\n","        self.linear = None\n","        self.pool = pool\n","        self.center = center\n","        if linear is not None:\n","            self.linear_est = torch.nn.LazyLinear(linear)\n","            if twin:\n","                self.linear_gt = self.linear_est\n","            else:\n","                self.linear_gt = torch.nn.LazyLinear(linear)\n","        self.tmin = tmin\n","        self.tmax = tmax\n","        self.tmin_train = tmin_train\n","        self.tmax_train = tmax_train\n","        self.dset_args = dset_args\n","\n","    def trim_samples(self, estimates, candidates):\n","        \"\"\"Given estimates that is [B1, C, T] and candidates\n","        which is [B2, C, T], return estimates_trim of size [B1, C, T']\n","        and candidates_trim of size [B2, C, T'], such that T'\n","        corresponds to the samples between [self.tmin, self.tmax]\n","        \"\"\"\n","        if self.training and (self.tmin_train is not None or self.tmax_train is not None):\n","            tmin, tmax = self.tmin_train, self.tmax_train\n","        else:\n","            tmin, tmax = self.tmin, self.tmax\n","        if (tmin is not None) or (tmax is not None):\n","            assert self.dset_args is not None\n","            assert self.dset_args.tmin is not None\n","            dset_tmin = self.dset_args.tmin\n","        if tmin is None:\n","            trim_min = 0\n","        else:\n","            assert tmin >= dset_tmin, 'clip.tmin should be above dset.tmin'\n","            trim_min = int((-dset_tmin + tmin) * self.dset_args.sample_rate)\n","        if tmax is None:\n","            trim_max = estimates.shape[-1]\n","        else:\n","            trim_max = int((-dset_tmin + tmax) * self.dset_args.sample_rate)\n","        estimates_trim = estimates[..., trim_min:trim_max]\n","        candidates_trim = candidates[..., trim_min:trim_max]\n","        return estimates_trim, candidates_trim\n","\n","    def get_scores(self, estimates: torch.Tensor, candidates: torch.Tensor):\n","        \"\"\"Given estimates that is [B, C, T] and candidates\n","        which is [B', C, T], return a [B, B'] matrix of scores of matching.\n","        \"\"\"\n","        estimates, candidates = self.trim_samples(estimates, candidates)\n","        if self.linear:\n","            estimates = self.linear_est(estimates)\n","            candidates = self.linear_gt(candidates)\n","        if self.pool:\n","            estimates = estimates.mean(dim=2, keepdim=True)\n","            candidates = candidates.mean(dim=2, keepdim=True)\n","        if self.center:\n","            estimates = estimates - estimates.mean(dim=(1, 2), keepdim=True)\n","            candidates = candidates - candidates.mean(dim=(1, 2), keepdim=True)\n","        inv_norms = 1 / (1e-8 + candidates.norm(dim=(1, 2), p=2))\n","        scores = torch.einsum(\"bct,oct,o->bo\", estimates, candidates, inv_norms)\n","        return scores\n","\n","    def get_probabilities(self, estimates, candidates):\n","        \"\"\"Given estimates that is [B, C, T] and candidates\n","        which is [B', C, T], return a [B, B'] matrix of probabilities of matching.\n","        \"\"\"\n","        scores = self.get_scores(estimates, candidates)\n","        probabilities = F.softmax(scores, dim=1)\n","        return probabilities\n","\n","    def forward(self, estimate, candidate, mask=None):\n","        \"\"\"Warning: estimate and candidate are not symmetrical.\n","        If estimate of shape [B, C, T] and candidate of size [B', C, T]\n","        with B'>=B, the first B samples of candidate are targets, while\n","        the remaining B'-B samples of candidate are only used as negatives.\n","        \"\"\"\n","        assert estimate.size(0) <= candidate.size(0), \"need at least as many targets as estimates\"\n","        scores = self.get_scores(estimate, candidate)\n","        target = torch.arange(len(scores), device=estimate.device)\n","        loss = F.cross_entropy(scores, target)\n","        return loss\n","\n","# TEST\n","batch_size = 3\n","num_channels = 4\n","time_steps = 5\n","estimates = torch.randn(batch_size, num_channels, time_steps)\n","candidates = torch.randn(batch_size, num_channels, time_steps)\n","clip_loss = ClipLoss()\n","\n","probabilities = clip_loss.get_probabilities(estimates, candidates) # --> output in the model's forward function\n","loss = clip_loss(estimates, candidates) # --> use a loss\n","\n","print(probabilities)\n","print(loss)\n","\n","### Each row in the probability matrix corresponds to a set of EEG data,\n","#   and each column corresponds to a set of audio data.\n","#   The values in the matrix are probabilities that indicate\n","#   how likely it is that a given set of EEG data matches a given set of audio data."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702223481555,"user_tz":300,"elapsed":2,"user":{"displayName":"Kateryna Shapovalenko","userId":"16393911302329786226"}},"outputId":"9149ecd4-f9b4-4d8c-8511-818a1af73c37","id":"ru6ojQNwsRGl"},"execution_count":389,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.8012, 0.1432, 0.0556],\n","        [0.3515, 0.3947, 0.2538],\n","        [0.2379, 0.5154, 0.2468]])\n","tensor(0.8502)\n"]}]},{"cell_type":"markdown","source":["## brain-audio model (our)"],"metadata":{"id":"C1UH6pcWh-0G"}},{"cell_type":"code","source":["class BrainAudioModel(torch.nn.Module):\n","\n","  def __init__(self, input_size, lstm_layers, listener_hidden_size,\n","               num_heads_listener, tf_blocks_listener, dropout,\n","               linear_output_size):\n","\n","    super().__init__()\n","\n","    # brain encoding\n","    self.brain_encoding = EEG_Transformer_Encoder(input_size, lstm_layers,\n","                                                  listener_hidden_size,\n","                                                  num_heads_listener, tf_blocks_listener,\n","                                                  dropout)\n","\n","    # brain to audio probabilities\n","    self.clip = ClipLoss(linear=linear_output_size)\n","\n","    # linear layer to match the dimensions if needed\n","    self.linear1 = nn.Linear(listener_hidden_size, linear_output_size)\n","\n","  def forward(self, raw_eeg_inputs, raw_eeg_inputs_len, embedded_audio_inputs = None):\n","\n","    print('\\nraw_eeg_inputs.shape', raw_eeg_inputs.shape) # [B, T, C]\n","    print('embedded_audio_inputs.shape', embedded_audio_inputs.shape) # [B, T, C]\n","\n","    # brain encoding\n","    brain_encoder_outputs, _ = self.brain_encoding(raw_eeg_inputs, raw_eeg_inputs_len)\n","    print('\\nbrain_encoder_outputs.shape', brain_encoder_outputs.shape)\n","\n","    brain_encoder_outputs = self.linear1(brain_encoder_outputs)\n","\n","    # brain to audio probabilities\n","    brain_encoder_outputs_reshaped = brain_encoder_outputs.permute(0, 2, 1)  # reshaping to [B, C, T]\n","    audio_hidden_states_reshaped = embedded_audio_inputs.permute(0, 2, 1)  # reshaping to [B, C, T]\n","\n","    print('\\nbrain_encoder_outputs_reshaped.shape', brain_encoder_outputs_reshaped.shape)\n","    print('audio_hidden_states_reshaped.shape', audio_hidden_states_reshaped.shape)\n","\n","    probabilities = self.clip.get_probabilities(brain_encoder_outputs_reshaped, audio_hidden_states_reshaped)\n","    print('\\nprobabilities', probabilities)\n","    print('\\nprobabilities.shape', probabilities.shape)\n","\n","    return probabilities, brain_encoder_outputs_reshaped, audio_hidden_states_reshaped\n","\n","# TEST\n","def test_combined():\n","    model = BrainAudioModel(input_size=62, lstm_layers=2,\n","                            listener_hidden_size=256,\n","                            num_heads_listener=8,\n","                            tf_blocks_listener=4, dropout=0.1,\n","                            linear_output_size=1024).to(DEVICE)\n","    batch_size = 2\n","    seq_len_eeg = 1000\n","    input_dim_eeg = 62\n","\n","    seq_len_audio = 2000\n","    input_dim_audio = 1024\n","\n","    x_sample = torch.rand(batch_size, seq_len_eeg, input_dim_eeg).to(DEVICE)\n","    x_lengths = torch.full((batch_size,), seq_len_eeg, dtype=torch.int64).to(DEVICE)\n","    y_sample = torch.rand(batch_size, seq_len_audio, input_dim_audio).to(DEVICE)\n","\n","    # Forward pass\n","    probabilities = model(x_sample, x_lengths, y_sample)\n","\n","    # Check output dimensions\n","\n","    summary(model, x_sample, x_lengths, y_sample)\n","\n","test_combined()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702223483688,"user_tz":300,"elapsed":436,"user":{"displayName":"Kateryna Shapovalenko","userId":"16393911302329786226"}},"outputId":"5d552def-5ddd-4a7f-b0a1-0b76d255b3c1","id":"_q2k0AQlf_XX"},"execution_count":390,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","raw_eeg_inputs.shape torch.Size([2, 1000, 62])\n","embedded_audio_inputs.shape torch.Size([2, 2000, 1024])\n","\n","brain_encoder_outputs.shape torch.Size([2, 125, 256])\n","\n","brain_encoder_outputs_reshaped.shape torch.Size([2, 1024, 125])\n","audio_hidden_states_reshaped.shape torch.Size([2, 1024, 2000])\n","\n","probabilities tensor([[0.6911, 0.3089],\n","        [0.6605, 0.3395]], grad_fn=<SoftmaxBackward0>)\n","\n","probabilities.shape torch.Size([2, 2])\n","\n","raw_eeg_inputs.shape torch.Size([2, 1000, 62])\n","embedded_audio_inputs.shape torch.Size([2, 2000, 1024])\n","\n","brain_encoder_outputs.shape torch.Size([2, 125, 256])\n","\n","brain_encoder_outputs_reshaped.shape torch.Size([2, 1024, 125])\n","audio_hidden_states_reshaped.shape torch.Size([2, 1024, 2000])\n","\n","probabilities tensor([[0.6723, 0.3277],\n","        [0.7041, 0.2959]])\n","\n","probabilities.shape torch.Size([2, 2])\n","==================================================================================================================\n","                                                     Kernel Shape  \\\n","Layer                                                               \n","0_brain_encoding.cnn.PermuteBlock_0                             -   \n","1_brain_encoding.cnn.Conv1d_1                         [62, 64, 2]   \n","2_brain_encoding.cnn.BatchNorm1d_2                           [64]   \n","3_brain_encoding.cnn.GELU_3                                     -   \n","4_brain_encoding.cnn.Conv1d_4                        [64, 128, 2]   \n","5_brain_encoding.cnn.BatchNorm1d_5                          [128]   \n","6_brain_encoding.cnn.GELU_6                                     -   \n","7_brain_encoding.cnn.Conv1d_7                       [128, 256, 2]   \n","8_brain_encoding.cnn.BatchNorm1d_8                          [256]   \n","9_brain_encoding.cnn.GELU_9                                     -   \n","10_brain_encoding.cnn.PermuteBlock_10                           -   \n","11_brain_encoding.LSTM_blstm                                    -   \n","12_brain_encoding.positional_encoding.Dropout_d...              -   \n","13_brain_encoding.transformer_encoder.0.LayerNo...          [256]   \n","14_brain_encoding.transformer_encoder.0.feed_fo...    [256, 1024]   \n","15_brain_encoding.transformer_encoder.0.feed_fo...              -   \n","16_brain_encoding.transformer_encoder.0.feed_fo...              -   \n","17_brain_encoding.transformer_encoder.0.feed_fo...    [1024, 256]   \n","18_brain_encoding.transformer_encoder.0.LayerNo...          [256]   \n","19_brain_encoding.transformer_encoder.1.LayerNo...          [256]   \n","20_brain_encoding.transformer_encoder.1.feed_fo...    [256, 1024]   \n","21_brain_encoding.transformer_encoder.1.feed_fo...              -   \n","22_brain_encoding.transformer_encoder.1.feed_fo...              -   \n","23_brain_encoding.transformer_encoder.1.feed_fo...    [1024, 256]   \n","24_brain_encoding.transformer_encoder.1.LayerNo...          [256]   \n","25_brain_encoding.transformer_encoder.2.LayerNo...          [256]   \n","26_brain_encoding.transformer_encoder.2.feed_fo...    [256, 1024]   \n","27_brain_encoding.transformer_encoder.2.feed_fo...              -   \n","28_brain_encoding.transformer_encoder.2.feed_fo...              -   \n","29_brain_encoding.transformer_encoder.2.feed_fo...    [1024, 256]   \n","30_brain_encoding.transformer_encoder.2.LayerNo...          [256]   \n","31_brain_encoding.transformer_encoder.3.LayerNo...          [256]   \n","32_brain_encoding.transformer_encoder.3.feed_fo...    [256, 1024]   \n","33_brain_encoding.transformer_encoder.3.feed_fo...              -   \n","34_brain_encoding.transformer_encoder.3.feed_fo...              -   \n","35_brain_encoding.transformer_encoder.3.feed_fo...    [1024, 256]   \n","36_brain_encoding.transformer_encoder.3.LayerNo...          [256]   \n","37_linear1                                            [256, 1024]   \n","\n","                                                      Output Shape    Params  \\\n","Layer                                                                          \n","0_brain_encoding.cnn.PermuteBlock_0                  [2, 62, 1000]         -   \n","1_brain_encoding.cnn.Conv1d_1                         [2, 64, 500]      8.0k   \n","2_brain_encoding.cnn.BatchNorm1d_2                    [2, 64, 500]     128.0   \n","3_brain_encoding.cnn.GELU_3                           [2, 64, 500]         -   \n","4_brain_encoding.cnn.Conv1d_4                        [2, 128, 250]   16.512k   \n","5_brain_encoding.cnn.BatchNorm1d_5                   [2, 128, 250]     256.0   \n","6_brain_encoding.cnn.GELU_6                          [2, 128, 250]         -   \n","7_brain_encoding.cnn.Conv1d_7                        [2, 256, 125]   65.792k   \n","8_brain_encoding.cnn.BatchNorm1d_8                   [2, 256, 125]     512.0   \n","9_brain_encoding.cnn.GELU_9                          [2, 256, 125]         -   \n","10_brain_encoding.cnn.PermuteBlock_10                [2, 125, 256]         -   \n","11_brain_encoding.LSTM_blstm                            [250, 256]  790.528k   \n","12_brain_encoding.positional_encoding.Dropout_d...   [2, 125, 256]         -   \n","13_brain_encoding.transformer_encoder.0.LayerNo...   [2, 125, 256]     512.0   \n","14_brain_encoding.transformer_encoder.0.feed_fo...  [2, 125, 1024]  263.168k   \n","15_brain_encoding.transformer_encoder.0.feed_fo...  [2, 125, 1024]         -   \n","16_brain_encoding.transformer_encoder.0.feed_fo...  [2, 125, 1024]         -   \n","17_brain_encoding.transformer_encoder.0.feed_fo...   [2, 125, 256]    262.4k   \n","18_brain_encoding.transformer_encoder.0.LayerNo...   [2, 125, 256]         -   \n","19_brain_encoding.transformer_encoder.1.LayerNo...   [2, 125, 256]     512.0   \n","20_brain_encoding.transformer_encoder.1.feed_fo...  [2, 125, 1024]  263.168k   \n","21_brain_encoding.transformer_encoder.1.feed_fo...  [2, 125, 1024]         -   \n","22_brain_encoding.transformer_encoder.1.feed_fo...  [2, 125, 1024]         -   \n","23_brain_encoding.transformer_encoder.1.feed_fo...   [2, 125, 256]    262.4k   \n","24_brain_encoding.transformer_encoder.1.LayerNo...   [2, 125, 256]         -   \n","25_brain_encoding.transformer_encoder.2.LayerNo...   [2, 125, 256]     512.0   \n","26_brain_encoding.transformer_encoder.2.feed_fo...  [2, 125, 1024]  263.168k   \n","27_brain_encoding.transformer_encoder.2.feed_fo...  [2, 125, 1024]         -   \n","28_brain_encoding.transformer_encoder.2.feed_fo...  [2, 125, 1024]         -   \n","29_brain_encoding.transformer_encoder.2.feed_fo...   [2, 125, 256]    262.4k   \n","30_brain_encoding.transformer_encoder.2.LayerNo...   [2, 125, 256]         -   \n","31_brain_encoding.transformer_encoder.3.LayerNo...   [2, 125, 256]     512.0   \n","32_brain_encoding.transformer_encoder.3.feed_fo...  [2, 125, 1024]  263.168k   \n","33_brain_encoding.transformer_encoder.3.feed_fo...  [2, 125, 1024]         -   \n","34_brain_encoding.transformer_encoder.3.feed_fo...  [2, 125, 1024]         -   \n","35_brain_encoding.transformer_encoder.3.feed_fo...   [2, 125, 256]    262.4k   \n","36_brain_encoding.transformer_encoder.3.LayerNo...   [2, 125, 256]         -   \n","37_linear1                                          [2, 125, 1024]  263.168k   \n","\n","                                                   Mult-Adds  \n","Layer                                                         \n","0_brain_encoding.cnn.PermuteBlock_0                        -  \n","1_brain_encoding.cnn.Conv1d_1                         3.968M  \n","2_brain_encoding.cnn.BatchNorm1d_2                      64.0  \n","3_brain_encoding.cnn.GELU_3                                -  \n","4_brain_encoding.cnn.Conv1d_4                         4.096M  \n","5_brain_encoding.cnn.BatchNorm1d_5                     128.0  \n","6_brain_encoding.cnn.GELU_6                                -  \n","7_brain_encoding.cnn.Conv1d_7                         8.192M  \n","8_brain_encoding.cnn.BatchNorm1d_8                     256.0  \n","9_brain_encoding.cnn.GELU_9                                -  \n","10_brain_encoding.cnn.PermuteBlock_10                      -  \n","11_brain_encoding.LSTM_blstm                        786.432k  \n","12_brain_encoding.positional_encoding.Dropout_d...         -  \n","13_brain_encoding.transformer_encoder.0.LayerNo...     256.0  \n","14_brain_encoding.transformer_encoder.0.feed_fo...  262.144k  \n","15_brain_encoding.transformer_encoder.0.feed_fo...         -  \n","16_brain_encoding.transformer_encoder.0.feed_fo...         -  \n","17_brain_encoding.transformer_encoder.0.feed_fo...  262.144k  \n","18_brain_encoding.transformer_encoder.0.LayerNo...     256.0  \n","19_brain_encoding.transformer_encoder.1.LayerNo...     256.0  \n","20_brain_encoding.transformer_encoder.1.feed_fo...  262.144k  \n","21_brain_encoding.transformer_encoder.1.feed_fo...         -  \n","22_brain_encoding.transformer_encoder.1.feed_fo...         -  \n","23_brain_encoding.transformer_encoder.1.feed_fo...  262.144k  \n","24_brain_encoding.transformer_encoder.1.LayerNo...     256.0  \n","25_brain_encoding.transformer_encoder.2.LayerNo...     256.0  \n","26_brain_encoding.transformer_encoder.2.feed_fo...  262.144k  \n","27_brain_encoding.transformer_encoder.2.feed_fo...         -  \n","28_brain_encoding.transformer_encoder.2.feed_fo...         -  \n","29_brain_encoding.transformer_encoder.2.feed_fo...  262.144k  \n","30_brain_encoding.transformer_encoder.2.LayerNo...     256.0  \n","31_brain_encoding.transformer_encoder.3.LayerNo...     256.0  \n","32_brain_encoding.transformer_encoder.3.feed_fo...  262.144k  \n","33_brain_encoding.transformer_encoder.3.feed_fo...         -  \n","34_brain_encoding.transformer_encoder.3.feed_fo...         -  \n","35_brain_encoding.transformer_encoder.3.feed_fo...  262.144k  \n","36_brain_encoding.transformer_encoder.3.LayerNo...     256.0  \n","37_linear1                                          262.144k  \n","------------------------------------------------------------------------------------------------------------------\n","                          Totals\n","Total params           3.249216M\n","Trainable params       3.249216M\n","Non-trainable params         0.0\n","Mult-Adds             19.404224M\n","==================================================================================================================\n"]}]},{"cell_type":"markdown","source":["## audio-to-word decoder & wer metrics"],"metadata":{"id":"N4TkGFvZjt2D"}},{"cell_type":"code","source":["# Load the pre-trained model\n","wav2vec_path = '/content/gdrive/MyDrive/11785-IDLf23/Final_project/pretrained-models/'\n","wav2vec_final_layer = torch.load(wav2vec_path + 'wav2vec2-final-layer.pkl').to(DEVICE)\n","wav2vec_processor = torch.load(wav2vec_path + 'wav2vec2-processor.pkl')\n","\n","# Decode predictions\n","def decode_predictions(predictions):\n","\n","    logits = wav2vec_final_layer(predictions)\n","    predicted_ids = logits.argmax(dim=-1)\n","    transcriptions = wav2vec_processor.batch_decode(predicted_ids)\n","\n","    return transcriptions\n","\n","# Calculate the evaluation metric\n","def decode_predictions_and_evaluate(predictions, targets):\n","\n","    batch_size = predictions.size(0)\n","    total_wer_lev = 0.0\n","    total_correct_general = 0\n","    total_words_general = 0\n","    total_correct_vocab = 0\n","    total_words_vocab = 0\n","\n","    for i in range(batch_size):\n","\n","        # Decode logits of predictions and targets into textual transcriptions\n","        pred_words = decode_predictions(predictions[i].unsqueeze(0))\n","        target_words = decode_predictions(targets[i].unsqueeze(0))\n","\n","        # Ensure pred_words and target_words are lists of strings\n","        if not isinstance(pred_words, list):\n","            pred_words = [pred_words]\n","        if not isinstance(target_words, list):\n","            target_words = [target_words]\n","\n","        # METRIC 1: WER Levenshtein -------------------------------------------------------------------------------\n","        \"\"\"\n","        Average Levenshtein distance-based WER across the batch.\n","        It indicates the average # of single-character edits (insertions, deletions, substitutions)\n","        required to change the predicted sentences into the target sentences,\n","        normalized by the number of words in the target sentences.\n","        A value of 2.25 suggests that, on average, about 2 to 3 edits are needed per word to correct the predictions.\n","        \"\"\"\n","        wer_lev = lev.distance(' '.join(pred_words), ' '.join(target_words)) / max(len(target_words), 1)\n","        total_wer_lev += wer_lev\n","\n","        # METRIC 2: WER General -------------------------------------------------------------------------------\n","        \"\"\"\n","        WER General is calculated as the simple proportion of correctly identified words,\n","        where both position and order are crucial.\n","        WER General of 75% indicates that 75% of the words in the predictions were incorrect\n","        when compared to the target words, taking into account the exact sequence in which they appear.\n","        \"\"\"\n","        correct_general = sum(pw == tw for pw, tw in zip(pred_words, target_words))\n","        total_correct_general += correct_general\n","        total_words_general += len(target_words)\n","\n","        # METRIC 3: WER Vocab -------------------------------------------------------------------------------\n","        \"\"\"\n","        This metric calculates the proportion of words in the predictions that are present in the target vocabulary,\n","        regardless of their position or order.\n","        A WER of 75% here indicates that 75% of the words in the predictions were not found in the target vocabulary.\n","        This metric is more lenient, focusing on the presence of predicted words within the overall pool of words\n","        used in the targets, without considering their specific sequence or placement.\n","        \"\"\"\n","        vocabulary = set(target_words)\n","        correct_vocab = sum(pw in vocabulary for pw in pred_words)\n","        total_correct_vocab += correct_vocab\n","        total_words_vocab += len(pred_words)\n","\n","    avg_wer_lev = total_wer_lev / batch_size\n","    wer_general = (1 - total_correct_general / total_words_general) * 100\n","    accuracy_general = 100 - wer_general\n","    wer_vocab = (1 - total_correct_vocab / total_words_vocab) * 100\n","    accuracy_vocab = 100 - wer_vocab\n","\n","    return avg_wer_lev, wer_general, accuracy_general, wer_vocab, accuracy_vocab\n","\n","# TEST -------------------------------------------------------------------------------\n","dummy_targets = torch.randn(10, 1024)\n","dummy_predictions = torch.randn(10, 1024)\n","decoded_targets = decode_predictions(dummy_targets)\n","decoded_predictions = decode_predictions(dummy_predictions)\n","avg_wer_lev, wer_general, accuracy_general, wer_vocab, accuracy_vocab = decode_predictions_and_evaluate(dummy_predictions, dummy_targets)\n","#target_words = [\"hello\", \"world\", \"test\", \"laptop\"]\n","#pred_words = [\"hello\", \"test\", \"deep learning\", \"11765\"]\n","#print(\"dummy_targets\\n\", dummy_targets)\n","#print(\"dummy_predictions\\n\", dummy_predictions)\n","#print(\"decoded_targets\", decoded_targets)\n","#print(\"decoded_predictions\", decoded_predictions)\n","#print(\"avg_wer_lev\", avg_wer_lev)\n","#print(\"wer_general\", wer_general)\n","#print(\"accuracy_general\", accuracy_general)\n","#print(\"wer_vocab\", wer_vocab)\n","#print(\"accuracy_vocab\", accuracy_vocab)"],"metadata":{"executionInfo":{"status":"ok","timestamp":1702223486055,"user_tz":300,"elapsed":278,"user":{"displayName":"Kateryna Shapovalenko","userId":"16393911302329786226"}},"id":"qwveRaEyKXcg"},"execution_count":391,"outputs":[]},{"cell_type":"markdown","source":["# training and validation"],"metadata":{"id":"qWHTmtK7NBUb"}},{"cell_type":"code","source":["def plot_attention(attention):\n","    plt.clf()\n","    seaborn.heatmap(attention, cmap='GnBu')\n","    plt.show()\n","\n","def save_model(model, optimizer, scheduler, metric, epoch, path):\n","    torch.save(\n","        {'model_state_dict'         : model.state_dict(),\n","         'optimizer_state_dict'     : optimizer.state_dict(),\n","         'scheduler_state_dict'     : scheduler.state_dict(),\n","         metric[0]                  : metric[1],\n","         'epoch'                    : epoch},\n","         path)\n","\n","def load_model(best_path, epoch_path, model, mode= 'best', metric= 'valid_acc', optimizer= None, scheduler= None):\n","\n","    if mode == 'best':\n","        checkpoint  = torch.load(best_path)\n","        print(\"Loading best checkpoint: \", checkpoint[metric])\n","    else:\n","        checkpoint  = torch.load(epoch_path)\n","        print(\"Loading epoch checkpoint: \", checkpoint[metric])\n","\n","    model.load_state_dict(checkpoint['model_state_dict'], strict= False)\n","\n","    if optimizer != None:\n","        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","        #optimizer.param_groups[0]['lr'] = 1.5e-3\n","        optimizer.param_groups[0]['weight_decay'] = 1e-5\n","    if scheduler != None:\n","        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n","\n","    epoch   = checkpoint['epoch']\n","    metric  = torch.load(best_path)[metric]\n","\n","    return [model, optimizer, scheduler, epoch, metric]\n","\n","class TimeElapsed():\n","    def __init__(self):\n","        self.start  = -1\n","\n","    def time_elapsed(self):\n","        if self.start == -1:\n","            self.start = time.time()\n","        else:\n","            end = time.time() - self.start\n","            hrs, rem    = divmod(end, 3600)\n","            min, sec    = divmod(rem, 60)\n","            min         = min + 60*hrs\n","            print(\"Time Elapsed: {:0>2}:{:02}\".format(int(min),int(sec)))\n","            self.start  = -1\n","\n","def weights_init_kaiming(m):\n","    classname = m.__class__.__name__\n","    if classname.find('Conv') != -1:\n","        torch.nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n","    elif classname.find('Linear') != -1:\n","        torch.nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n","        if m.bias is not None:\n","            torch.nn.init.constant_(m.bias.data, 0.0)\n","    elif classname.find('BatchNorm') != -1:\n","        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n","        torch.nn.init.constant_(m.bias.data, 0.0)"],"metadata":{"id":"aGMPNPaSrHAI","executionInfo":{"status":"ok","timestamp":1702223487571,"user_tz":300,"elapsed":103,"user":{"displayName":"Kateryna Shapovalenko","userId":"16393911302329786226"}}},"execution_count":392,"outputs":[]},{"cell_type":"code","source":["def train(model, dataloader, criterion, optimizer):\n","\n","    model.train()\n","\n","    total_loss = 0.0\n","\n","    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n","\n","    for i, (eeg, audio, eeg_lengths, audio_lengths) in enumerate(dataloader): # should start from 0\n","\n","        eeg, audio = eeg.to(DEVICE), audio.to(DEVICE)\n","\n","        optimizer.zero_grad()\n","\n","        probabilities, brain_encoder_outputs_reshaped, audio_hidden_states_reshaped = model(eeg, eeg_lengths, audio)\n","        loss = criterion(brain_encoder_outputs_reshaped, audio_hidden_states_reshaped)\n","        total_loss += loss.item()\n","        loss.backward()\n","        optimizer.step()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=3) # optional\n","\n","        batch_bar.set_postfix(loss=\"{:.04f}\".format(total_loss/(i+1)),\n","                              lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])))\n","        batch_bar.update()\n","\n","        del eeg, audio, eeg_lengths, audio_lengths\n","        torch.cuda.empty_cache()\n","\n","    loss = total_loss / len(dataloader)\n","    batch_bar.close()\n","\n","    return loss\n","\n","def validate(model, dataloader):\n","\n","    model.eval()\n","\n","    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc=\"Val\")\n","\n","    total_loss = 0.0\n","    total_avg_wer_lev = 0.0\n","    total_wer_general = 0.0\n","    total_accuracy_general = 0.0\n","    total_wer_vocab = 0.0\n","    total_accuracy_vocab = 0.0\n","\n","    with torch.no_grad():\n","\n","        for i, (eeg, audio, eeg_lengths, audio_lengths) in enumerate(dataloader):\n","\n","            eeg = eeg.to(DEVICE)\n","\n","            # Get model predictions\n","            probabilities, brain_encoder_outputs_reshaped, audio_hidden_states_reshaped = model(eeg, eeg_lengths, audio)\n","\n","            # Calculate loss\n","            loss = criterion(brain_encoder_outputs_reshaped, audio_hidden_states_reshaped)\n","            total_loss += loss.item()\n","\n","            # Decode predictions and targets, then calculate WER\n","            #decoded_predictions = decode_predictions(brain_encoder_outputs_reshaped)\n","            #decoded_targets = decode_predictions(audio_hidden_states_reshaped)\n","            avg_wer_lev, wer_general, accuracy_general, wer_vocab, accuracy_vocab = decode_predictions_and_evaluate(brain_encoder_outputs_reshaped,\n","                                                                                                         audio_hidden_states_reshaped)\n","            total_avg_wer_lev += avg_wer_lev\n","            total_wer_general += wer_general\n","            total_accuracy_general += accuracy_general\n","            total_wer_vocab += wer_vocab\n","            total_accuracy_vocab += accuracy_vocab\n","\n","            batch_bar.set_postfix(loss=\"{:.04f}\".format(total_loss / (i + 1)),\n","                                  wer_lev=\"{:.02f}\".format(total_avg_wer_lev / (i + 1)),\n","                                  wer_general=\"{:.02f}\".format(total_wer_general / (i + 1)),\n","                                  accuracy_general=\"{:.02f}\".format(total_accuracy_general / (i + 1)),\n","                                  wer_vocab=\"{:.02f}\".format(total_wer_vocab / (i + 1)),\n","                                  accuracy_vocab=\"{:.02f}\".format(total_accuracy_vocab / (i + 1)))\n","            batch_bar.update()\n","\n","            del eeg, audio, eeg_lengths, audio_lengths\n","            torch.cuda.empty_cache()\n","\n","    batch_bar.close()\n","    loss = total_loss / len(dataloader)\n","    wer_lev = total_avg_wer_lev / len(dataloader)\n","    wer_general = total_wer_general / len(dataloader)\n","    accuracy_general = total_accuracy_general / len(dataloader)\n","    wer_vocab = total_wer_vocab / len(dataloader)\n","    accuracy_vocab = total_accuracy_vocab / len(dataloader)\n","\n","    return loss, wer_lev, wer_general, accuracy_general, wer_vocab, accuracy_vocab"],"metadata":{"id":"eQHnPFA7ojio","executionInfo":{"status":"ok","timestamp":1702223488781,"user_tz":300,"elapsed":2,"user":{"displayName":"Kateryna Shapovalenko","userId":"16393911302329786226"}}},"execution_count":393,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AvRxu4fDyQv2"},"source":["# wandb\n"]},{"cell_type":"code","execution_count":399,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":208},"executionInfo":{"elapsed":6290,"status":"ok","timestamp":1702223648752,"user":{"displayName":"Kateryna Shapovalenko","userId":"16393911302329786226"},"user_tz":300},"id":"dg9f2Z_lddsq","outputId":"7a9a5a43-50a8-490e-9c47-792860584dea"},"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkshapova\u001b[0m (\u001b[33midl2023\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.16.1"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20231210_155407-rynti3eh</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/idl2023/final_project/runs/rynti3eh' target=\"_blank\">test</a></strong> to <a href='https://wandb.ai/idl2023/final_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/idl2023/final_project' target=\"_blank\">https://wandb.ai/idl2023/final_project</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/idl2023/final_project/runs/rynti3eh' target=\"_blank\">https://wandb.ai/idl2023/final_project/runs/rynti3eh</a>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["['/content/wandb/run-20231210_155407-rynti3eh/files/model_architecture.txt']"]},"metadata":{},"execution_count":399}],"source":["import wandb\n","wandb.login(key=\"3e9cb37b9f485bc61ca01d7f5ac0130ff698fb6a\")\n","\n","run = wandb.init(\n","    name = \"test\", # wandb creates random run names if you skip this field\n","    reinit = True, # allows reinitalizing runs when you re-run this cell\n","    #id = \"niwgbub6\", # insert specific run id here if you want to resume a previous run\n","    #resume = \"allow\", # you need this to resume previous runs, but comment out reinit = True when using this\n","    project = \"final_project\", # project should be created in your wandb account\n","    config = config) # wandb config for your run\n","\n","model_arch_file = \"model_architecture.txt\" # save your model architecture in a txt file, and save the file to Wandb\n","\n","with open(model_arch_file, \"w\") as f:\n","    f.write(str(model))\n","wandb.save(model_arch_file)"]},{"cell_type":"markdown","source":["# experiments"],"metadata":{"id":"N4UE5_2vjUtQ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"a9LN0l5VUk_s","scrolled":true},"outputs":[],"source":["torch.cuda.empty_cache()\n","gc.collect()\n","\n","model = BrainAudioModel(input_size           = 62,\n","                        lstm_layers          = 2,\n","                        listener_hidden_size = 256,\n","                        num_heads_listener   = 8,\n","                        tf_blocks_listener   = 4,\n","                        dropout              = 0.1,\n","                        linear_output_size   = 1024)\n","\n","model.apply(weights_init_kaiming)\n","\n","model = model.to(DEVICE)\n","\n","optimizer = AdamW(model.parameters(), lr=config['learning_rate'], betas=(0.9, 0.999),\n","                  eps=1e-8, weight_decay=config['weight_decay'], amsgrad=False) #optimizer = AdamP(model.parameters(), lr=config['learning_rate'], betas=(0.9, 0.999), weight_decay=config['weight_decay'])\n","\n","criterion = ClipLoss(linear=None, twin=True, pool=False, tmin=None, tmax=None,\n","                     tmin_train=None, tmax_train=None, dset_args=None, center=False)\n","\n","#scaler = torch.cuda.amp.GradScaler() # optional\n","\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=config['factor'],\n","                                                       patience=config['patience'], verbose=True)\n","\n","# TEST ------------------------------------------------------------------------------------------------------------\n","def test_combined():\n","    seq_len_eeg = 1000\n","    input_dim_eeg = 62\n","\n","    seq_len_audio = 2000\n","    input_dim_audio = 1024\n","\n","    eeg_sample = torch.rand(config['batch_size'], seq_len_eeg, input_dim_eeg).to(DEVICE)\n","    eeg_lengths = torch.full((config['batch_size'],), seq_len_eeg, dtype=torch.int64).to(DEVICE)\n","    audio_sample = torch.rand(config['batch_size'], seq_len_audio, input_dim_audio).to(DEVICE)\n","\n","    # Forward pass\n","    probabilities = model(eeg_sample, eeg_lengths, audio_sample)\n","\n","    # Check output dimensions\n","    summary(model, eeg_sample, eeg_lengths, audio_sample)\n","\n","test_combined()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OkNhc8qwl-yk"},"outputs":[],"source":["# PLEASE RUN THIS CELL ONLY IF YOU NEED TO RESUME TRAINING FROM A CERTAIN CHECKPOINT\n","\"\"\"\n","# Load the most recent checkpoint if needed\n","checkpoint_path = \"/content/drive/MyDrive/HW4_P2_Checkpoints/checkpoint_epoch11_valid_dist8.8932.pth\"\n","model, optimizer, scheduler, last_epoch_completed, best_wer = load_model(checkpoint_path, checkpoint_path, model, metric='valid_dist', optimizer=optimizer, scheduler=scheduler)\n","\n","# Introduce changes if needeed\n","new_lr = 0.00005\n","for param_group in optimizer.param_groups:\n","    param_group['lr'] = new_lr\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1, verbose=True)\n","\n","# Print diagnostic information if needed\n","print(f\"Model: {model}\")\n","print(f\"Optimizer: {optimizer}\")\n","print(f\"Current learning rate from optimizer: {optimizer.param_groups[0]['lr']}\")\n","print(f\"Scheduler: {scheduler}\")\n","print(f\"Last completed epoch from checkpoint: {last_epoch_completed}\")\n","print(f\"Best Levenshtein distance from checkpoint: {best_wer}\")\n","\"\"\""]},{"cell_type":"code","execution_count":396,"metadata":{"id":"ZoCcolsMT7h9","executionInfo":{"status":"ok","timestamp":1702223582802,"user_tz":300,"elapsed":131,"user":{"displayName":"Kateryna Shapovalenko","userId":"16393911302329786226"}}},"outputs":[],"source":["start = 0\n","end = config[\"epochs\"]\n","best_wer = float(\"inf\")\n","\n","checkpoint_dir = '/content/gdrive/MyDrive/11785-IDLf23/Final_project/6_Checkpoints/'\n","epoch_model_path = os.path.join(checkpoint_dir, 'checkpoint_epoch{epoch}_valid_dist{valid_dist:.4f}.pth')\n","best_model_path = os.path.join(checkpoint_dir, 'best_model.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yulWRLXCUOsC"},"outputs":[],"source":["torch.cuda.empty_cache()\n","gc.collect()\n","\n","for epoch in range(start, config['epochs']):\n","\n","    print(\"\\nEpoch: {}/{}\".format(epoch+1, config['epochs']))\n","\n","    curr_lr = optimizer.param_groups[0]['lr']\n","\n","    # Call train and validate, get attention weights from training\n","    train_loss = train(model, train_loader, criterion, optimizer)\n","    valid_loss, wer_lev, wer_general, accuracy_general, wer_vocab, accuracy_vocab = validate(model, val_loader)\n","\n","    # Print your metrics\n","    print(\"\\tTrain Loss {:.04f}\\t Learning Rate {:.07f}\".format(train_loss, curr_lr))\n","    print(\"\\tVal Loss {:.04f}%\".format(valid_loss))\n","\n","    # Plot attention for a single item in the batch\n","    #plot_attention(attention_plot[0].cpu().detach().numpy())\n","\n","    # Log metrics to Wandb\n","    wandb.log({\"train_loss\": train_loss,\n","               \"valid_loss\": valid_loss,\n","               \"wer_lev\": wer_lev,\n","               \"wer_general\": wer_general,\n","               \"accuracy_general\": accuracy_general,\n","               \"wer_vocab\": wer_vocab,\n","               \"accuracy_vocab\": accuracy_vocab,\n","               \"epoch\": epoch+1,\n","               \"lr\": curr_lr})\n","    save_model(model, optimizer, scheduler, ['wer_vocab', wer_vocab], epoch, epoch_model_path.format(epoch=epoch,\n","                                                                                                     wer_vocab=wer_vocab))\n","\n","    wandb.save(epoch_model_path)\n","    print(\"Saved epoch model\")\n","\n","    # Scheduler\n","    scheduler.step(wer_vocab)\n","\n","    if wer_vocab <= best_wer:\n","      best_lev_dist = wer_vocab\n","      save_model(model, optimizer, scheduler, None, ['wer_vocab', wer_vocab], epoch, best_model_path)\n","      wandb.save(best_model_path)\n","      print(\"Saved best model\")\n","\n","run.finish()"]},{"cell_type":"markdown","source":["# viz"],"metadata":{"id":"EXoEy94Dj3kt"}},{"cell_type":"code","source":[],"metadata":{"id":"q_cyKW1Sj0ol"},"execution_count":null,"outputs":[]}]}